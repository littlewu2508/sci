From e2abee80c809892aa84d237f8986d5c586e0ea65 Mon Sep 17 00:00:00 2001
From: Benjamin Ulmer <benjamin.ulmer@amd.com>
Date: Mon, 4 Oct 2021 13:04:19 -0600
Subject: [PATCH 01/12] Remove winners from BenchmarkProblems and remove missed
 old client dependency in ClientWriter

---
 Tensile/BenchmarkProblems.py | 365 +++++------------------------------
 Tensile/ClientWriter.py      |  23 +--
 2 files changed, 60 insertions(+), 328 deletions(-)

diff --git a/Tensile/BenchmarkProblems.py b/Tensile/BenchmarkProblems.py
index b1f4c15ed..dbe62450a 100644
--- a/Tensile/BenchmarkProblems.py
+++ b/Tensile/BenchmarkProblems.py
@@ -19,7 +19,6 @@
 # CTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 ################################################################################
 
-import collections
 import csv
 import itertools
 import os
@@ -47,7 +46,7 @@
 ############################################################################
 # generateForkedSolutions
 ############################################################################
-def generateForkedSolutions (problemType, hardcodedParameters, benchmarkPermutations, winners=None, initialSolutionParameters=None):
+def generateForkedSolutions (problemType, hardcodedParameters, benchmarkPermutations, initialSolutionParameters=None):
   """this creates a set or solutions based on the forked parameters using
      a set of common parameters from which to fork from
 
@@ -76,12 +75,6 @@ def generateForkedSolutions (problemType, hardcodedParameters, benchmarkPermutat
       solution = {"ProblemType": deepcopy(problemType.state)}
       solution.update(benchmarkPermutation)
       solution.update(hardcodedParamDict)
-      if winners:
-        winningParameters = winners[hardcodedParamDict]
-        if winningParameters == None:
-          # this is a joined parameter that didn't have a winner, that's okay
-          continue
-        solution.update(winningParameters)
 
       # append default parameters where necessary
       if initialSolutionParameters:
@@ -136,7 +129,7 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, \
 
   totalBenchmarkSteps = len(benchmarkProcess)
   resultsFileBaseFinal = None
-  winners = WinningParameterDict()
+
   print1("# NumBenchmarkSteps: %u" % totalBenchmarkSteps)
   print1("")
   print1(HR)
@@ -149,21 +142,11 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, \
   for benchmarkStepIdx in range(0, totalBenchmarkSteps):
 
     benchmarkStep = benchmarkProcess[benchmarkStepIdx]
-    if winners.winners == {}:
-      # perf optimization to skip the initial winners creation
-      # this helps a little here but really helps below with avoiding the super-expensive
-      # removeHardcoded step below - that can use a fast-path to create
-      # winners when needed.
-      print1("# Empty winners - use fast initialization of hardcodedParameters")
-      resultingHardcodedParameterList = benchmarkStep.hardcodedParameters
-    else:
-      resultingHardcodedParameterList = \
-          winners.wpdUpdate( benchmarkStep.hardcodedParameters )
 
-    benchmarkStep.hardcodedParameters = resultingHardcodedParameterList
     numHardcoded = len(benchmarkStep.hardcodedParameters)
     stepName = str(benchmarkStep)
     shortName = benchmarkStep.abbreviation()
+
     print1("\n")
     print1(HR)
     currentTime = time.time()
@@ -171,6 +154,7 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, \
     print1("# BenchmarkStep: %s - %s %.3fs" % (problemSizeGroupName, stepName, elapsedTime))
     print1("# NumProblems: %u" % benchmarkStep.problemSizes.totalProblemSizes)
     print1("# BenchmarkParameters:")
+
     for paramName in benchmarkStep.benchmarkParameters:
       paramValues = benchmarkStep.benchmarkParameters[paramName]
       printStr = "#     %s = { %s" % (paramName, paramValues[0])
@@ -179,17 +163,6 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, \
       printStr += " }"
       print1(printStr)
 
-    if False:
-      print1("# HardcodedParameters | WinningParameters:")
-      paramDictIdx = 0
-      hardcodedMinNaming = \
-          Solution.getMinNaming(benchmarkStep.hardcodedParameters)
-      for paramDict in benchmarkStep.hardcodedParameters:
-        winningParameters = winners[paramDict]
-        print1("#    (%u) %s | %s" % (paramDictIdx, \
-            Solution.getNameMin(paramDict, hardcodedMinNaming), \
-            Solution.getNameFull(winningParameters) ))
-        paramDictIdx += 1
     pushWorkingPath(shortName)
 
     ############################################################################
@@ -228,30 +201,32 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, \
 
     benchmarkPermutations = constructForkPermutations(benchmarkStep.benchmarkParameters)
     maxPossibleSolutions = len(benchmarkPermutations) * numHardcoded
+
     ############################################################################
     # Enumerate Solutions = Hardcoded * Benchmark
     ############################################################################
-    #print1("# Enumerating Solutions")
-    theWinners = None
-    if benchmarkStepIdx > 0:
-      theWinners = winners
+    solutions = generateForkedSolutions(benchmarkProcess.problemType, \
+        benchmarkStep.hardcodedParameters, benchmarkPermutations, \
+        benchmarkStep.initialSolutionParameters)
 
-    solutions = generateForkedSolutions (benchmarkProcess.problemType, benchmarkStep.hardcodedParameters, \
-        benchmarkPermutations, theWinners, benchmarkStep.initialSolutionParameters)
     # remove hardcoded that don't have any valid benchmarks
-    removeHardcoded = list([x for i,x in enumerate(benchmarkStep.hardcodedParameters) if len(solutions[i]) == 0])
-    validHardcoded =  list([x for i,x in enumerate(benchmarkStep.hardcodedParameters) if len(solutions[i]) > 0])
+    removeHardcoded = list([x for i, x in enumerate(benchmarkStep.hardcodedParameters) \
+        if len(solutions[i]) == 0])
+    validHardcoded =  list([x for i, x in enumerate(benchmarkStep.hardcodedParameters) \
+        if len(solutions[i]) > 0])
 
     removesExist = len(removeHardcoded) > 0
-
     benchmarkStep.hardcodedParameters = validHardcoded
 
     # add custom kernels to list of solutions
     customKernelList = problemSizeGroupConfig.get("CustomKernels", [])
     customKernelWildcard = False
     if customKernelList == ["*"]:
-      customKernelList = [fname[:-2] for fname in os.listdir(globalParameters["CustomKernelDirectory"]) if fname.endswith(".s")]
+      customKernelList = \
+          [fname[:-2] for fname in os.listdir(globalParameters["CustomKernelDirectory"]) \
+          if fname.endswith(".s")]
       customKernelWildcard = True
+
     for kernelName in customKernelList:
       print1("# Processing custom kernel {}".format(kernelName))
       customSolution = generateCustomKernelSolution(kernelName)
@@ -260,7 +235,8 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, \
         if not customKernelWildcard:
           missingParams = [p for p in benchmarkProcess.problemType if p not in customSolution["ProblemType"]]
           extraParams   = [p for p in customSolution["ProblemType"] if p not in benchmarkProcess.problemType]
-          msg  = "The problem type in the config file does not match that of the custom kernel, {0}.".format(kernelName)
+          msg  = "The problem type in the config file does not match" \
+              "that of the custom kernel, {0}.".format(kernelName)
           msg += "\nMissing config parameters:\n" + str(missingParams)
           msg += "\nExtra custom kernel parameters:\n" + str(extraParams)
           raise RuntimeError(msg)
@@ -275,22 +251,9 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, \
         elif globalParameters["PrintSolutionRejectionReason"]:
           print1("rejecting solution %s" % str(customSolution))
 
+    numHardcoded = len(benchmarkStep.hardcodedParameters )
     if removesExist:
-      if "CustomKernels" not in problemSizeGroupConfig:
-        print1("# Updating winners since enumeration removed unused hardcoded solutions.  removeHardcoded=%u winners=%u" \
-              %(len(removeHardcoded), len(winners.winners)))
-        winners.wpdUpdate( benchmarkStep.hardcodedParameters )
-      if globalParameters["PrintLevel"] >= 1:
-        print1("")
-      numHardcoded = len(benchmarkStep.hardcodedParameters )
-      # remove from solution 2D list also
       solutions = list([s for s in solutions if len(s) > 0])
-    elif winners.winners=={} and "CustomKernels" not in problemSizeGroupConfig:
-      print1("# Populating initial winners (%u solutions)\n" % len(benchmarkStep.hardcodedParameters))
-      for hcParm in benchmarkStep.hardcodedParameters:
-        winners.winners[FrozenDictionary(hcParm)] = [{},-1]
-    else:
-      numHardcoded = len(benchmarkStep.hardcodedParameters )
 
     print1("# Actual Solutions: %u / %u after SolutionStructs\n" % ( len(solutions), \
         maxPossibleSolutions ))
@@ -301,9 +264,11 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, \
     if len(solutionList) == 0:
         msg = "Your parameters resulted in 0 valid solutions."
         if globalParameters["PrintSolutionRejectionReason"]:
-            msg += "\nExamine reject and backtrace messages above to see why and where solutions were rejected."
+            msg += "\nExamine reject and backtrace messages above to see why" \
+                "and where solutions were rejected."
         else:
-            msg += "\nYou should re-run with \"PrintSolutionRejectionReason: True\" to see why each parameter combination was rejected."
+            msg += "\nYou should re-run with \"PrintSolutionRejectionReason: True\"" \
+                "to see why each parameter combination was rejected."
         printExit(msg)
     if globalParameters["PrintLevel"] >= 1:
       for i,solutionsForHardcoded in enumerate(solutions):
@@ -341,15 +306,12 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, \
       benchmarkStep.hardcodedParameters.remove(hardcodedParam)
 
     if removesExist:
-      if "CustomKernels" not in problemSizeGroupConfig:
-        print1("# Updating winners since kernelwriter removed unused hardcoded solutions.  removeHardcoded=%u winners=%u"
-               %(len(removeHardcoded), len(winners.winners)))
-        winners.wpdUpdate( benchmarkStep.hardcodedParameters )
       numHardcoded = len(benchmarkStep.hardcodedParameters )
       # remove from solution 2D list also
+      prevCount = len(solutions)
       solutions = list([s for s in solutions if len(s) > 0])
-      print1("# Actual Solutions: %u / %u after kernelwriter\n" \
-            % ( len(winners.winners)-len(removeHardcoded), len(winners.winners) ))
+      print1("# Actual Solutions: %u / %u after KernelWriter\n" \
+            % (len(solutions), prevCount ))
 
     popWorkingPath() # source
 
@@ -377,22 +339,6 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, \
     else:
       print1("# Already benchmarked; skipping.")
 
-    ############################################################################
-    # Winners -> Determined Parameters
-    ############################################################################
-    if not enableTileSelection and "CustomKernels" not in problemSizeGroupConfig:
-        results = getResults(resultsFileName, solutions, enableTileSelection, newResultsFileName)
-        currentTime = time.time()
-        elapsedTime = currentTime - startTime
-        print1("# Finish GetResults - %.3fs\n" % (elapsedTime))
-        print2("CSV Results: %s" % results)
-        winners.addResults(benchmarkStep.hardcodedParameters, \
-            benchmarkPermutations, solutions, results)
-        currentTime = time.time()
-        elapsedTime = currentTime - startTime
-        print1("# Finish Adding Results - %.3fs\n" % (elapsedTime))
-
-
     ############################################################################
     # Write Solutions YAML
     ############################################################################
@@ -618,224 +564,17 @@ def writeBenchmarkFiles(stepBaseDir, solutions, problemSizes, stepName, filesToC
       outputPath )
 
 
-################################################################################
-# FrozenDictionary
-################################################################################
-class FrozenDictionary(collections.abc.Mapping):
-  def __init__(self, parameters):
-    self.parameters = deepcopy(parameters)
-    self.stringValue = Solution.getNameFull(self.parameters)
-    self.hashValue = hash(self.stringValue)
-
-  def __len__(self):
-    return len(self.parameters)
-
-  def __iter__(self):
-    return iter(self.parameters)
-
-  def __getitem__(self, key):
-    return self.parameters[key]
-
-  def __hash__(self):
-    return self.hashValue
-
-  def __str__(self):
-    return self.stringValue
-  def __repr__(self):
-    return self.__str__()
-
-
-################################################################################
-# Winning Parameters For Hardcoded Parameters
-###############################################################################
-class WinningParameterDict:
-
-  ##########################################################
-  # Init
-  def __init__(self):
-    # Index with 'hardcodedParameterKey'
-    # Each element in winners contains a 2D array:
-    #  [0] = winningParamters
-    #  [1] = winningScore
-    self.winners = {}
-
-
-  ##########################################################
-  # Add Winning Parameters For Hardcoded Parameters
-  def addResults( self, hardcodedParameterList, benchmarkPermutations, \
-      solutions, results):
-    print1("# Adding Results to Solution Database")
-    for hardcodedIdx,hardcodedResults in Utils.tqdm(enumerate(results)):
-      if not hardcodedResults: continue
-
-      hardcodedParameters = hardcodedParameterList[hardcodedIdx]
-      winningIdx = -1
-      winningScore = -9999 # -1 is score of invalid so use -9999 here
-      # find fastest benchmark parameters for this hardcoded
-      for benchmarkIdx,benchmarkResult in enumerate(hardcodedResults):
-        if not benchmarkResult: continue
-
-        benchmarkScore = max(benchmarkResult) # take fastest regardless of size
-        if benchmarkScore > winningScore:
-          winningScore = benchmarkScore
-          winningIdx = benchmarkIdx
-      winningSolution = solutions[hardcodedIdx][winningIdx]
-      winningParameters = {}
-      for paramName in benchmarkPermutations[0]:
-        winningParameters[paramName] = winningSolution[paramName]
-      #print2("HCP[%u] Winner: idx=%u, gflops=%f, param=%s" \
-      #    % ( hardcodedIdx, winningIdx, winningScore, winningParameters))
-      matches = WinningParameterDict.get(hardcodedParameters, self.winners)
-      if len(matches) != 1:
-        printExit("Didn't find exactly 1 match")
-      hardcodedParametersKey = matches[0][0]
-      #oldWinningParameters = matches[0][1]
-      #oldScore = matches[0][2]
-      self.winners[hardcodedParametersKey][0].update(winningParameters)
-      self.winners[hardcodedParametersKey][1] = winningScore
-
-
-  ##########################################################
-  # Get Winning Parameters For Hardcoded Parameters
-  def __getitem__( self, hardcodedParameters ):
-    #(hardcodedParametersKey, winningParameters, score) = \
-    matches = WinningParameterDict.get(hardcodedParameters, self.winners)
-    if len(matches) == 1:
-      return matches[0][1]
-    elif len(matches) == 0:
-      return None
-    else:
-      printExit("Didn't find exactly 1 match")
-
-
-  ##########################################################
-  # Update Hardcoded Parameters In Winning Parameters
-  # could be forking, joining or adding parameters to same hardcodeds
-  def wpdUpdate(self, newHardcodedParameterList ):
-    # TODO when new list is joining, we need to choose the fastest
-    oldWinners = self.winners
-    self.winners = {}
-
-    # if this is first time, populate with dummies and early exit
-    if len(oldWinners) == 0:
-      for newHardcodedParameters in newHardcodedParameterList:
-        self.winners[FrozenDictionary(newHardcodedParameters)] = [{},-1]
-    else:
-      if globalParameters["PrintLevel"] >= 1:
-        print1("# Updating Solution Database")
-      for newHardcodedParameters in Utils.tqdm(newHardcodedParameterList):
-        #(oldHardcodedParameters, winningParameters, score) = \
-        matches = WinningParameterDict.get(newHardcodedParameters, oldWinners)
-        if len(matches) == 1: # plain update
-          hardcodedFrozen = matches[0][0]
-          winningParameters = matches[0][1]
-          score = matches[0][2]
-          #if winningParameters != None:
-          newHardcodedParameters.update(hardcodedFrozen.parameters)
-          self.winners[FrozenDictionary(newHardcodedParameters)] = \
-              [ winningParameters, score ]
-        elif len(matches) > 1: # join
-          fastestScore = -1
-          fastestHardcodedParameters = {}
-          fastestWinningParameters = {}
-          for matchIdx,match in enumerate(matches):
-            hardcodedFrozen = match[0]
-            winningParameters = match[1]
-            score = match[2]
-            if score > fastestScore:
-              fastestScore = score
-              fastestWinningParameters = winningParameters
-              fastestHardcodedParameters = hardcodedFrozen.parameters
-          newHardcodedParameters.update(fastestHardcodedParameters)
-          self.winners[FrozenDictionary(newHardcodedParameters)] = \
-              [ fastestWinningParameters, fastestScore ]
-
-
-    # return resulting hardcodedParameterList
-    returnHardcodedParameterList = []
-    for hardcodedFrozen in self.winners:
-      returnHardcodedParameterList.append(hardcodedFrozen.parameters)
-    #print "info: after winner-update, returnHardcodedParameterList=", len(returnHardcodedParameterList)
-    return returnHardcodedParameterList
-
-  ##########################################################
-  # Get Winning Parameters For Hardcoded Parameters
-  # For "Updating Solution Database"
-  #  - winners is a hash of all the solutions.  Points to 2D(?) list
-  #       0 : parameters
-  #       1 : score
-  #  - lookupHardcodedParameters is a dict of hard-coded parms, ie "BufferLoad: True"
-  #  - Return a list of matches -
-  # need to match MacroTile also
-  @staticmethod
-  def get( lookupHardcodedParameters, winners ):
-    matches = []
-
-    # only 1 winner, when benchmarking 1 solution
-    if len(winners) == 1:
-      hardcodedFrozen = list(winners.keys())[0]
-      winningParameters = winners[hardcodedFrozen][0]
-      score = winners[hardcodedFrozen][1]
-      matches.append([hardcodedFrozen, winningParameters, score])
-      return matches
-
-    for hardcodedFrozen in winners:
-      winningParameters = winners[hardcodedFrozen][0]
-      score = winners[hardcodedFrozen][1]
-      frozenMatch = True
-      # a match if no key in hardcoded has a different value than lookup
-      for paramName in hardcodedFrozen:
-        if paramName in lookupHardcodedParameters:
-          if lookupHardcodedParameters[paramName] != \
-              hardcodedFrozen[paramName]:
-            frozenMatch = False
-            break
-      if frozenMatch:
-        matchMacroTile = True
-        matchUnion = {}
-        matchUnion.update(hardcodedFrozen.parameters)
-        matchUnion.update(winningParameters)
-        if "MacroTile0" in lookupHardcodedParameters:
-          lookupMacroTile0 = lookupHardcodedParameters["MacroTile0"]
-          lookupMacroTile1 = lookupHardcodedParameters["MacroTile1"]
-          Solution.assignProblemIndependentDerivedParameters(matchUnion)
-          Solution.assignProblemIndependentDerivedParameters(hardcodedFrozen.parameters)
-          if matchUnion["MacroTile0"] != lookupMacroTile0 \
-              or matchUnion["MacroTile1"] != lookupMacroTile1:
-            matchMacroTile = False
-        if matchMacroTile:
-          matches.append([hardcodedFrozen, winningParameters, score])
-      else:
-        pass
-
-    return matches
-
-  ##########################################################
-  # To String
-  def __str__(self):
-    state = ""
-    idx = 0
-    for hardcodedParameters in self.winners:
-      winningParameters = self.winners[hardcodedParameters][0]
-      score = self.winners[hardcodedParameters][1]
-      state += "  %2u: %s -> %s %f GFlop/s\n" % (idx, hardcodedParameters, \
-          Solution.getNameFull(winningParameters), score)
-      idx += 1
-    return state
-  def __repr__(self):
-    return self.__str__()
-
-
 ################################################################################
 # Main
 ################################################################################
-def main( config ):
+def main(config):
+  """Entry point for the "BenchmarkProblems" section of a Tensile config yaml"""
   ClientExecutable.getClientExecutable()
 
-  dataPath = os.path.join(globalParameters["WorkingPath"], \
-      globalParameters["BenchmarkDataPath"])
+  dataPath = os.path.join(globalParameters["WorkingPath"], globalParameters["BenchmarkDataPath"])
   pushWorkingPath(globalParameters["BenchmarkProblemsPath"])
   ensurePath(dataPath)
+
   totalTestFails = 0
   for benchmarkProblemTypeConfig in config:
     problemTypeConfig = benchmarkProblemTypeConfig[0]
@@ -843,45 +582,47 @@ def main( config ):
       problemSizeGroupConfigs = [{}]
     else:
       problemSizeGroupConfigs = benchmarkProblemTypeConfig[1:]
-    for problemSizeGroupIdx,problemSizeGroupConfig in enumerate(problemSizeGroupConfigs):
-      print2("ProblemTypeConfig: %s" % problemTypeConfig)
+
+    for problemSizeGroupIdx, problemSizeGroupConfig in enumerate(problemSizeGroupConfigs):
+      print2("ProblemTypeConfig: {}".format(problemTypeConfig))
       problemTypeObj = ProblemType(problemTypeConfig)
       globalParameters["EnableHalf"] = problemTypeObj["DataType"].isHalf()
 
       # using a suffix to check the csv version (for later addFromCSV())
       csvSuffix = "_CSVWinner" if globalParameters["CSVExportWinner"] else ""
       # results files will be named
-      newResultsFileName = os.path.join(dataPath, "%s_%02u%s.csv" \
-          % (str(problemTypeObj), problemSizeGroupIdx, csvSuffix) )
-      newSolutionsFileName = os.path.join(dataPath, "%s_%02u%s.yaml" \
-          % (str(problemTypeObj), problemSizeGroupIdx, csvSuffix) )
-      newGranularityFileName = os.path.join(dataPath, "%s_%02u%s.gsp" \
-          % (str(problemTypeObj), problemSizeGroupIdx, csvSuffix) )
+      newResultsFileName = os.path.join(dataPath, "{}_{:02d}{}.csv" \
+          .format(str(problemTypeObj), problemSizeGroupIdx, csvSuffix) )
+      newSolutionsFileName = os.path.join(dataPath, "{}_{:02d}{}.yaml" \
+          .format(str(problemTypeObj), problemSizeGroupIdx, csvSuffix) )
+      newGranularityFileName = os.path.join(dataPath, "{}_{:02d}{}.gsp" \
+          .format(str(problemTypeObj), problemSizeGroupIdx, csvSuffix) )
 
       # skip if possible
-      if globalParameters["ForceRedoBenchmarkProblems"] or \
-          not os.path.exists(newResultsFileName):
+      if globalParameters["ForceRedoBenchmarkProblems"] \
+          or not os.path.exists(newResultsFileName):
 
-        # Benchmark Problem Size Group
-        (resultsFileBaseFinal, benchmarkErrors) = benchmarkProblemType(problemTypeConfig, \
-            problemSizeGroupConfig, problemSizeGroupIdx)
+        # benchmark problem size group
+        (resultsFileBaseFinal, benchmarkErrors) = \
+            benchmarkProblemType(problemTypeConfig, problemSizeGroupConfig, problemSizeGroupIdx)
         totalTestFails += benchmarkErrors
 
-        print("clientExit=%u %s for %s" %\
-                (totalTestFails, "(ERROR)" if totalTestFails else "(PASS)", \
-                globalParameters["ConfigPath"]))
+        print("clientExit={} {} for {}" \
+            .format(totalTestFails, "(ERROR)" if totalTestFails else "(PASS)", \
+            globalParameters["ConfigPath"]) )
 
-        # Copy Data
+        # copy data
         resultsFileBase = resultsFileBaseFinal
-        resultsFileName = "%s.csv" % (resultsFileBase)
-        solutionsFileName = "%s.yaml" % (resultsFileBase)
-        granularityFileName = "%s_Granularity.csv" % (resultsFileBase)
+        resultsFileName = resultsFileBase + ".csv"
+        solutionsFileName = resultsFileBase + ".yaml"
+        granularityFileName = resultsFileBase + "_Granularity.csv"
         shutil.copy( resultsFileName, newResultsFileName )
         shutil.copy( solutionsFileName, newSolutionsFileName )
         if os.path.isfile(granularityFileName):
           shutil.copy( granularityFileName, newGranularityFileName )
       else:
-        print1("# %s_%02u already benchmarked; skipping." % (str(problemTypeObj), problemSizeGroupIdx) )
+        print1("# {}_{:02d} already benchmarked; skipping." \
+            .format(str(problemTypeObj), problemSizeGroupIdx) )
 
   popWorkingPath()
 
diff --git a/Tensile/ClientWriter.py b/Tensile/ClientWriter.py
index 03ce7b931..fa1ae2676 100644
--- a/Tensile/ClientWriter.py
+++ b/Tensile/ClientWriter.py
@@ -28,8 +28,7 @@
 import os
 import subprocess
 import shlex
-from shutil import copy as shutil_copy
-from shutil import rmtree
+import shutil
 from enum import Enum
 
 from .Contractions import FreeIndex
@@ -75,29 +74,21 @@ def main( config ):
   ##############################################################################
   pushWorkingPath("source")
   filesToCopy = [
-      "SolutionMapper.h",
-      "Client.cpp",
-      "Client.h",
-      "DeviceStats.h",
-      "ReferenceCPU.h",
-      "TensorUtils.h",
-      "MathTemplates.cpp",
-      "MathTemplates.h",
+      "TensileTypes.h",
+      "tensile_bfloat16.h",
       "KernelHeader.h",
-      "Tools.h",
-      "TensileCreateLibrary.cmake",
       ]
 
   for f in filesToCopy:
-    shutil_copy(
+    shutil.copy(
         os.path.join(globalParameters["SourcePath"], f),
         globalParameters["WorkingPath"] )
   if globalParameters["RuntimeLanguage"] == "OCL":
-    shutil_copy(
+    shutil.copy(
         os.path.join(globalParameters["SourcePath"], "FindOpenCL.cmake"),
         globalParameters["WorkingPath"] )
   else:
-    shutil_copy(
+    shutil.copy(
         os.path.join(globalParameters["SourcePath"], "FindHIP.cmake"),
         globalParameters["WorkingPath"] )
 
@@ -157,7 +148,7 @@ def main( config ):
   ##############################################################################
   # if redo=true, clobber the build directory
   if globalParameters["ForceRedoLibraryClient"]:
-    rmtree(os.path.join(globalParameters["WorkingPath"], "build"), \
+    shutil.rmtree(os.path.join(globalParameters["WorkingPath"], "build"), \
         ignore_errors=True)
 
   forBenchmark = False

From 9ff1a2a6984c95f558b92a4b9d1ca16d46bf6ce6 Mon Sep 17 00:00:00 2001
From: Benjamin Ulmer <benjamin.ulmer@amd.com>
Date: Mon, 4 Oct 2021 16:36:00 -0600
Subject: [PATCH 02/12] Remove unsupported steps from BenchmarkStructs

---
 Tensile/BenchmarkStructs.py | 307 +++---------------------------------
 1 file changed, 23 insertions(+), 284 deletions(-)

diff --git a/Tensile/BenchmarkStructs.py b/Tensile/BenchmarkStructs.py
index 181971cc0..273e276ac 100644
--- a/Tensile/BenchmarkStructs.py
+++ b/Tensile/BenchmarkStructs.py
@@ -1,5 +1,5 @@
 ################################################################################
-# Copyright 2016-2020 Advanced Micro Devices, Inc. All rights reserved.
+# Copyright 2016-2021 Advanced Micro Devices, Inc. All rights reserved.
 #
 # Permission is hereby granted, free of charge, to any person obtaining a copy
 # of this software and associated documentation files (the "Software"), to deal
@@ -150,12 +150,10 @@ class BenchmarkProcess:
   Steps in config need to be expanded and missing elements need to be assigned a default.
   """
 
-  def __init__(self, problemTypeConfig, problemSizeGroupConfig ):
+  def __init__(self, problemTypeConfig, problemSizeGroupConfig):
 
     self.problemType = ProblemType(problemTypeConfig)
-    self.isBatched = True \
-        if "Batched" in problemTypeConfig and problemTypeConfig["Batched"] \
-        else False
+    self.isBatched = "Batched" in problemTypeConfig and problemTypeConfig["Batched"]
     print2("# BenchmarkProcess beginning %s" % str(self.problemType))
 
     # read initial solution parameters
@@ -213,20 +211,18 @@ def fillInMissingStepsWithDefaults(self, isbatched, config):
     ############################################################################
     # (I-0) get 6 phases from config
     configBenchmarkCommonParameters = config["BenchmarkCommonParameters"] \
-        if "BenchmarkCommonParameters" in config \
-        else [{"ProblemSizes": defaultBatchedProblemSizes}] \
-        if isbatched \
-        else [{"ProblemSizes": defaultProblemSizes}]
+        if "BenchmarkCommonParameters" in config else []
     configForkParameters = config["ForkParameters"] \
         if "ForkParameters" in config else []
-    configBenchmarkForkParameters = config["BenchmarkForkParameters"] \
-        if "BenchmarkForkParameters" in config \
-        else []
-    configJoinParameters = config["JoinParameters"] \
-        if "JoinParameters" in config else []
-    configBenchmarkJoinParameters = config["BenchmarkJoinParameters"] \
-        if "BenchmarkJoinParameters" in config \
-        else []
+
+    # TODO cleanup error messages
+    if config.get("BenchmarkForkParameters") is not None:
+      print("no longer supported")
+    if config.get("JoinParameters") is not None:
+      print("no longer supported")
+    if config.get("BenchmarkJoinParameters") is not None:
+      print("no longer supported")
+
     configBenchmarkFinalParameters = config["BenchmarkFinalParameters"] \
         if "BenchmarkFinalParameters" in config and config["BenchmarkFinalParameters"] != None \
         and len(config["BenchmarkFinalParameters"]) > 0 \
@@ -238,8 +234,7 @@ def fillInMissingStepsWithDefaults(self, isbatched, config):
     # Ensure only valid solution parameters were requested
     validParameterNames = set(validParameters.keys())
     for paramDictList in [configBenchmarkCommonParameters, \
-        configForkParameters, configBenchmarkForkParameters, \
-        configBenchmarkJoinParameters]:
+        configForkParameters]:
       if paramDictList != None:
         for paramDict in paramDictList:
           try:
@@ -247,35 +242,15 @@ def fillInMissingStepsWithDefaults(self, isbatched, config):
           except RuntimeError as e:
             printExit(str(e))
 
-
-    ############################################################################
-    # (I-1) get current problem sizes
-    currentProblemSizes = defaultBatchedProblemSizes \
-        if isbatched \
-        else defaultProblemSizes
-    if configBenchmarkCommonParameters != None:
-      if len(configBenchmarkCommonParameters) > 0:
-        if "ProblemSizes" in configBenchmarkCommonParameters[0]:
-          # user specified, so use it, remove it from config and insert later
-          currentProblemSizes = \
-            configBenchmarkCommonParameters[0]["ProblemSizes"]
-          del configBenchmarkCommonParameters[0]
-    # into common we put in all Dcommon that
-    # don't show up in Ccommon/Cfork/CBfork/Cjoin/CBjoin
-    # followed by Ccommon
-    self.benchmarkCommonParameters = [{"ProblemSizes": currentProblemSizes}]
-    # need to use deepcopy to prevent default parameters from being washed-out later
-
-    benchmarkCommonParameters = fillMissingParametersWithDefaults([ configBenchmarkCommonParameters, \
-            configForkParameters, configBenchmarkForkParameters, \
-            configJoinParameters, configBenchmarkJoinParameters], deepcopy(defaultBenchmarkCommonParameters))
+    # TODO warn/guard against BenchmarkCommons that have mor than one value
+    benchmarkCommonParameters = fillMissingParametersWithDefaults( \
+        [configBenchmarkCommonParameters, configForkParameters], \
+        deepcopy(defaultBenchmarkCommonParameters))
     self.benchmarkCommonParameters.extend(benchmarkCommonParameters)
 
     if configBenchmarkCommonParameters != None:
       for paramDict in configBenchmarkCommonParameters:
         self.benchmarkCommonParameters.append(paramDict)
-    else: # make empty
-      self.benchmarkCommonParameters = [{"ProblemSizes": currentProblemSizes}]
 
     ############################################################################
     # (I-2) into fork we put in all Dfork that
@@ -286,8 +261,7 @@ def fillInMissingStepsWithDefaults(self, isbatched, config):
     for paramDict in deepcopy(defaultForkParameters):
       for paramName in paramDict:
         if not hasParam( paramName, [ self.benchmarkCommonParameters, \
-            configForkParameters, configBenchmarkForkParameters, \
-            configJoinParameters, configBenchmarkJoinParameters]) \
+            configForkParameters]) \
             or paramName == "ProblemSizes":
           self.forkParameters.append(paramDict)
     if configForkParameters != None:
@@ -296,87 +270,13 @@ def fillInMissingStepsWithDefaults(self, isbatched, config):
     else: # make empty
       self.forkParameters = []
 
-    ############################################################################
-    # (I-3) get current problem sizes
-    if configBenchmarkForkParameters != None:
-      if len(configBenchmarkForkParameters) > 0:
-        if "ProblemSizes" in configBenchmarkForkParameters[0]:
-          # user specified, so use it, remove it from config and insert later
-          currentProblemSizes = configBenchmarkForkParameters[0]["ProblemSizes"]
-          del configBenchmarkForkParameters[0]
-    # into Bfork we put in all DBfork that
-    # don't show up in Bcommon/Bfork/CBfork/Cjoin/CBjoin
-    # followed by CBforked
-    self.benchmarkForkParameters = [{"ProblemSizes": currentProblemSizes}]
-    # need to use deepcopy to prevent default parameters from being washed-out later
-    for paramDict in deepcopy(defaultBenchmarkForkParameters):
-      for paramName in paramDict:
-        if not hasParam( paramName, [ self.benchmarkCommonParameters, \
-            self.forkParameters, configBenchmarkForkParameters, \
-            configJoinParameters, configBenchmarkJoinParameters]) \
-            or paramName == "ProblemSizes":
-          self.benchmarkForkParameters.append(paramDict)
-    if configBenchmarkForkParameters != None:
-      for paramDict in configBenchmarkForkParameters:
-        self.benchmarkForkParameters.append(paramDict)
-    else: # make empty
-      self.benchmarkForkParameters = [{"ProblemSizes": currentProblemSizes}]
-
-    ############################################################################
-    # (I-4) into join we put in all non-derrived Djoin that
-    # don't show up in Bcommon/Bfork/CBfork/Cjoin/CBjoin
-    # followed by CBforked
-    self.joinParameters = []
-    # need to use deepcopy to prevent default parameters from being washed-out later
-    for paramName in deepcopy(defaultJoinParameters):
-      if not hasParam( paramName, [ self.benchmarkCommonParameters, \
-          self.forkParameters, self.benchmarkForkParameters, \
-          configJoinParameters, configBenchmarkJoinParameters]) \
-          or paramName == "ProblemSizes":
-        if "JoinParameters" not in config \
-            or (paramName != "MacroTile"):
-          self.joinParameters.append(paramName)
-    if configJoinParameters != None:
-      for paramName in configJoinParameters:
-        self.joinParameters.append(paramName)
-    else: # make empty
-        self.joinParameters = []
-
-    ############################################################################
-    # (I-5) benchmark join
-    if configBenchmarkJoinParameters != None:
-      if len(configBenchmarkJoinParameters) > 0:
-        if "ProblemSizes" in configBenchmarkJoinParameters[0]:
-          # user specified, so use it, remove it from config and insert later
-          currentProblemSizes = configBenchmarkJoinParameters[0]["ProblemSizes"]
-          del configBenchmarkJoinParameters[0]
-    # into Bjoin we put in all DBjoin that
-    # don't show up in Bcommon/Bfork/BBfork/Bjoin/CBjoin
-    # followed by CBjoin
-    self.benchmarkJoinParameters = [{"ProblemSizes": currentProblemSizes}]
-    # need to use deepcopy to prevent default parameters from being washed-out later
-    for paramDict in deepcopy(defaultBenchmarkJoinParameters):
-      for paramName in paramDict:
-        if not hasParam( paramName, [ self.benchmarkCommonParameters, \
-            self.forkParameters, self.benchmarkForkParameters, \
-            self.joinParameters, configBenchmarkJoinParameters]) \
-            or paramName == "ProblemSizes":
-          self.benchmarkJoinParameters.append(paramDict)
-    if configBenchmarkJoinParameters != None:
-      for paramDict in configBenchmarkJoinParameters:
-        self.benchmarkJoinParameters.append(paramDict)
-    else: # make empty
-      self.benchmarkJoinParameters = [{"ProblemSizes": currentProblemSizes}]
-
     ############################################################################
     # (I-6) benchmark final sizes
     self.benchmarkFinalParameters = configBenchmarkFinalParameters
     # no other parameters besides problem sizes
 
-
     ############################################################################
     # (I-7) any default param with 1 value will be hardcoded; move to beginning
-
     singleValues = getSingleValues([self.benchmarkCommonParameters, \
         self.forkParameters, self.benchmarkForkParameters, \
         self.benchmarkJoinParameters])
@@ -386,23 +286,6 @@ def fillInMissingStepsWithDefaults(self, isbatched, config):
       self.singleValueParameters[paramName] = [ paramValue ]
       self.initialSolutionParameters[paramName] = paramValue
 
-    ############################################################################
-    # (I-8) if fork and join, but no benchmark fork, append dummy benchmarkFork
-    if len(self.forkParameters) > 0 and len(self.joinParameters) > 0 \
-        and (len(self.benchmarkForkParameters) == 0 \
-        or (len(self.benchmarkForkParameters) == 1 \
-        and hasParam("ProblemSizes", self.benchmarkForkParameters)) ):
-      self.benchmarkForkParameters.append({"BenchmarkFork": [0]})
-
-    ############################################################################
-    # (I-9) if join, but no benchmark join, append dummy benchmarkJoin
-    #if len(self.joinParameters) > 0 \
-    #    and (len(self.benchmarkJoinParameters) == 0 \
-    #    or (len(self.benchmarkJoinParameters) == 1 \
-    #    and hasParam("ProblemSizes", self.benchmarkJoinParameters)) ):
-    #  self.benchmarkJoinParameters.append({"BenchmarkJoin": [0]})
-    # No, this is handles by Final Benchmark
-
     ############################################################################
     # (I-10) Parameter Lists
     # benchmarkCommonParameters
@@ -417,23 +300,6 @@ def fillInMissingStepsWithDefaults(self, isbatched, config):
     print2("ForkParameters:")
     for param in self.forkParameters:
       print2("    %s" % param)
-    # benchmarkForkParameters
-    print2("BenchmarkForkParameters:")
-    for step in self.benchmarkForkParameters:
-      print2("    %s" % step)
-    # joinParameters
-    print2("JoinParameters:")
-    for param in self.joinParameters:
-      print2("    %s" % param)
-    # benchmarkJoinParameters
-    print2("BenchmarkJoinParameters:")
-    for step in self.benchmarkJoinParameters:
-      print2("    %s" % step)
-    # benchmarkFinalParameters
-    print2("BenchmarkFinalParameters:")
-    for step in self.benchmarkFinalParameters:
-      print2("    %s" % step)
-
 
   ##############################################################################
   # (II) convert lists of parameters to benchmark steps
@@ -445,13 +311,6 @@ def convertParametersToSteps(self):
     print2("####################################################################")
     print2("")
 
-    ############################################################################
-    # (II-1) benchmark common parameters
-    print2("")
-    print2("####################################################################")
-    print1("# Benchmark Common Parameters")
-    self.addStepsForParameters( self.benchmarkCommonParameters  )
-
     ############################################################################
     # (II-2) fork parameters
     # calculate permutations of
@@ -463,117 +322,6 @@ def convertParametersToSteps(self):
     if len(forkPermutations) > 0:
       self.forkHardcodedParameters(forkPermutations)
 
-    ############################################################################
-    # (II-3) benchmark fork parameters
-    print2("")
-    print2("####################################################################")
-    print1("# Benchmark Fork Parameters")
-    self.addStepsForParameters( self.benchmarkForkParameters  )
-
-    ############################################################################
-    # (II-4.1) join parameters
-    # answer should go in hard-coded parameters
-    # does it remove the prior forks? Yes.
-    print2("")
-    print2("####################################################################")
-    print1("# Join Parameters")
-    macroTileJoinSet = set()
-    totalPermutations = 1
-    if len(self.joinParameters) > 0:
-      for joinName in self.joinParameters:
-        # joining a parameter with only a single value
-        if hasParam(joinName, self.singleValueParameters):
-          pass
-        elif hasParam(joinName, self.forkParameters):
-          # count permutations
-          for param in self.forkParameters:
-            for name in param: # only 1
-              if name == joinName:
-                values = param[name]
-                localPermutations = len(values)
-                print2("JoinParameter %s has %u possibilities" % (joinName, localPermutations))
-                totalPermutations *= localPermutations
-
-        ##########################################################################
-        # (II-4.2) Join MacroTile
-        elif joinName == "MacroTile":
-          print2("JoinParam: MacroTile")
-          # get possible WorkGroupEdges from forked
-          print2("currentForkParameters = %s" % str(self.forkParameters))
-          threadTileValues = []
-          workGroupValues = []
-          # todo having MacroTile as join parameter causes trouble if
-          # one parameter is benchmarked rather than forked
-          # however, this may still be the right way to do it
-
-          # count permutations
-          for paramList in [self.benchmarkCommonParameters, \
-              self.forkParameters, self.benchmarkForkParameters, \
-              self.benchmarkJoinParameters, self.singleValueParameters ]:
-            if hasParam("ThreadTile", paramList):
-              threadTileValues = getParamValues("ThreadTile", paramList)
-            if hasParam("WorkGroup", paramList):
-              workGroupValues = getParamValues("WorkGroup", paramList)
-          macroTilePermutations = len(workGroupValues) * len(threadTileValues)
-          print2("# Total JoinMacroTile Permutations: %u" % macroTilePermutations)
-
-          # enumerate permutations
-          for i in range(0, macroTilePermutations):
-            pIdx = i
-            workGroupIdx = pIdx % len(workGroupValues)
-            pIdx //= len(workGroupValues)
-            threadTileIdx = pIdx % len(threadTileValues)
-
-            workGroup = workGroupValues[workGroupIdx]
-            threadTile = threadTileValues[threadTileIdx]
-
-            macroTile0 = workGroup[0]*threadTile[0]
-            macroTile1 = workGroup[1]*threadTile[1]
-            macroTileJoinSet.add((macroTile0, macroTile1))
-          totalPermutations *= len(macroTileJoinSet)
-          print2("JoinMacroTileSet(%u): %s" % (len(macroTileJoinSet), macroTileJoinSet) )
-
-        # invalid join parameter
-        else:
-          validJoinNames = ["MacroTile"]
-          for validParam in self.forkParameters:
-            for validName in validParam: # only 1
-              validJoinNames.append(validName)
-          printExit("JoinParameter \"%s\" not in %s" % (joinName, validJoinNames) )
-
-      ############################################################################
-      # (II-4.4) Enumerate Permutations Other * MacroTile * DepthU
-      macroTiles = list(macroTileJoinSet)
-      print2("# TotalJoinPermutations = %u" % ( totalPermutations) )
-      joinPermutations = []
-      for i in range(0, totalPermutations):
-        joinPermutations.append({})
-        pIdx = i
-        for joinName in self.joinParameters:
-          if hasParam(joinName, self.forkParameters):
-            for paramDict in self.forkParameters: # hardcodedPermutations
-              if joinName in paramDict:
-                paramValues = paramDict[joinName]
-                valueIdx = pIdx % len(paramValues)
-                joinPermutations[i][joinName] = paramValues[valueIdx]
-                pIdx //= len(paramValues)
-                break
-          elif joinName == "MacroTile":
-            valueIdx = pIdx % len(macroTiles)
-            pIdx //= len(macroTiles)
-            joinPermutations[i]["MacroTile0"] = macroTiles[valueIdx][0]
-            joinPermutations[i]["MacroTile1"] = macroTiles[valueIdx][1]
-      if len(joinPermutations) > 0:
-        self.joinHardcodedParameters(joinPermutations)
-
-
-    ############################################################################
-    # (II-5) benchmark join parameters
-    print2("")
-    print2("####################################################################")
-    print1("# Benchmark Join Parameters")
-    self.addStepsForParameters( self.benchmarkJoinParameters  )
-
     ############################################################################
     # (II-6) benchmark final
     print2("")
@@ -586,7 +334,8 @@ def convertParametersToSteps(self):
         problemSizes = problemSizesDict["ProblemSizes"]
         self.currentProblemSizes = ProblemSizes(self.problemType, problemSizes)
         currentBenchmarkParameters = {}
-        checkCDBufferAndStrides(self.problemType, self.currentProblemSizes, globalParameters["CEqualD"])
+        checkCDBufferAndStrides(self.problemType, \
+            self.currentProblemSizes, globalParameters["CEqualD"])
         benchmarkStep = BenchmarkStep(
             self.hardcodedParameters,
             currentBenchmarkParameters,
@@ -596,7 +345,6 @@ def convertParametersToSteps(self):
         self.benchmarkSteps.append(benchmarkStep)
         self.benchmarkStepIdx+=1
 
-
   ##############################################################################
   # For list of config parameters convert to steps and append to steps list
   ##############################################################################
@@ -617,7 +365,8 @@ def addStepsForParameters(self, configParameterList):
               % ( paramName, str(self.problemType) ) )
       if len(currentBenchmarkParameters) > 0:
         print2("Adding BenchmarkStep for %s" % str(currentBenchmarkParameters))
-        checkCDBufferAndStrides(self.problemType, self.currentProblemSizes, globalParameters["CEqualD"])
+        checkCDBufferAndStrides(self.problemType, self.currentProblemSizes, \
+            globalParameters["CEqualD"])
         benchmarkStep = BenchmarkStep(
             self.hardcodedParameters,
             currentBenchmarkParameters,
@@ -643,13 +392,6 @@ def forkHardcodedParameters( self, update ):
       #updatedHardcodedParameters.append(permutation)
     self.hardcodedParameters = updatedHardcodedParameters
 
-  ##############################################################################
-  # contract old permutations of hardcoded parameters based on new
-  ##############################################################################
-  def joinHardcodedParameters( self, update ):
-    self.hardcodedParameters = update
-    return
-
   def __len__(self):
     return len(self.benchmarkSteps)
   def __getitem__(self, key):
@@ -717,6 +459,3 @@ def __str__(self):
 
   def __repr__(self):
     return self.__str__()
-
-
-

From ffa83e2990fbec17d62ea39875b7a02741f2961d Mon Sep 17 00:00:00 2001
From: Benjamin Ulmer <benjamin.ulmer@amd.com>
Date: Wed, 6 Oct 2021 11:38:32 -0600
Subject: [PATCH 03/12] Removing unused code and cleaning up BenchmarkStructs
 before refactor

---
 Tensile/BenchmarkStructs.py                   | 212 ++++--------------
 Tensile/Common.py                             |  19 +-
 .../Tests/unit/test_TensileCreateLibrary.py   |  19 --
 3 files changed, 55 insertions(+), 195 deletions(-)

diff --git a/Tensile/BenchmarkStructs.py b/Tensile/BenchmarkStructs.py
index 273e276ac..2c3086653 100644
--- a/Tensile/BenchmarkStructs.py
+++ b/Tensile/BenchmarkStructs.py
@@ -23,19 +23,13 @@
 from .Common import print1, print2, printWarning, defaultSolution, \
     defaultProblemSizes, defaultBenchmarkFinalProblemSizes, \
     defaultBatchedProblemSizes, defaultBatchedBenchmarkFinalProblemSizes, \
-    defaultBenchmarkCommonParameters, hasParam, \
-    defaultBenchmarkJoinParameters, getParamValues, defaultForkParameters, \
-    defaultBenchmarkForkParameters, defaultJoinParameters, printExit, \
+    defaultBenchmarkCommonParameters, hasParam, getParamValues, printExit, \
     validParameters, defaultSolutionSummationSizes, globalParameters
 from .SolutionStructs import Solution, ProblemType, ProblemSizes
 
 
-### modularize benchmark steps construction
-
-##############################################################################
-# forkHardcodedParameters
-##############################################################################
-def forkHardcodedParameters( basePermutations, update ):
+def forkHardcodedParameters(basePermutations, update):
+  """Temp doc"""
   updatedHardcodedParameters = []
   for oldPermutation in basePermutations:
     for newPermutation in update:
@@ -45,17 +39,18 @@ def forkHardcodedParameters( basePermutations, update ):
       updatedHardcodedParameters.append(permutation)
   return updatedHardcodedParameters
 
-def fillMissingParametersWithDefaults(parameterConfigurationList, defaultParameters):
-
+def getDefaultsForMissingParameters(parameterConfigurationList, defaultParameters):
+  """Temp doc"""
   benchmarkParameters = []
   for paramDict in defaultParameters:
     for paramName in paramDict:
-      if not hasParam( paramName, parameterConfigurationList) \
+      if not hasParam(paramName, parameterConfigurationList) \
           or paramName == "ProblemSizes":
         benchmarkParameters.append(paramDict)
   return benchmarkParameters
 
 def checkForValidParameters(params, validParameterNames):
+  """Temp doc"""
   for paramName in params:
     if paramName in ["ProblemSizes"]:
       continue
@@ -71,6 +66,7 @@ def checkForValidParameters(params, validParameterNames):
                         " (only first 32 combos printed)\nRefer to Common.py for more info" if len(validParameters[paramName])>32 else ""))
 
 def constructForkPermutations(forkParametersConfig):
+  """Temp doc"""
   totalPermutations = 1
   for param in forkParametersConfig:
     for name in param: # only 1
@@ -89,7 +85,7 @@ def constructForkPermutations(forkParametersConfig):
   return forkPermutations
 
 def getSingleValues(parameterSetList):
-  ############################################################################
+  """Temp doc"""
   singleVaules = {}
   for stepList in parameterSetList:
     for paramDict in copy(stepList):
@@ -105,37 +101,8 @@ def getSingleValues(parameterSetList):
 
   return singleVaules
 
-##############################################################################
-# assignParameters
-##############################################################################
-def assignParameters(problemTypeConfig, configBenchmarkCommonParameters, configForkParameters):
-
-  problemTypeObj = ProblemType(problemTypeConfig)
-  initialSolutionParameters = { "ProblemType": problemTypeConfig }
-  initialSolutionParameters.update(defaultSolution)
-
-  hardcodedParameters = []
-  benchmarkCommonParameters = fillMissingParametersWithDefaults([configBenchmarkCommonParameters, configForkParameters], defaultBenchmarkCommonParameters)
-  if configBenchmarkCommonParameters != None:
-    for paramDict in configBenchmarkCommonParameters:
-      benchmarkCommonParameters.append(paramDict)
-
-  singleValues = getSingleValues([benchmarkCommonParameters, configForkParameters])
-  for paramName in singleValues:
-    paramValue = singleValues[paramName]
-    initialSolutionParameters[paramName] = paramValue
-
-  forkPermutations = constructForkPermutations(configForkParameters)
-  if len(forkPermutations) > 0:
-    hardcodedParameters = forkHardcodedParameters([initialSolutionParameters], forkPermutations)
-
-  return (problemTypeObj, hardcodedParameters, initialSolutionParameters)
-
-
-##############################################################################
-# check LDD == LDC if CEqualD"
-##############################################################################
 def checkCDBufferAndStrides(problemType, problemSizes, isCEqualD):
+  """Temp doc"""
   if isCEqualD and problemType["OperationType"] == "GEMM":
     for problem in problemSizes.problems:
       ldd = problem.sizes[problemType["IndexAssignmentsLD"][0]]
@@ -146,46 +113,30 @@ def checkCDBufferAndStrides(problemType, problemSizes, isCEqualD):
 
 class BenchmarkProcess:
   """
-  Benchmark Process
   Steps in config need to be expanded and missing elements need to be assigned a default.
   """
 
   def __init__(self, problemTypeConfig, problemSizeGroupConfig):
-
+    """Temp doc"""
     self.problemType = ProblemType(problemTypeConfig)
     self.isBatched = "Batched" in problemTypeConfig and problemTypeConfig["Batched"]
     print2("# BenchmarkProcess beginning %s" % str(self.problemType))
 
-    # read initial solution parameters
+    # create initial solution parameters
     self.initialSolutionParameters = { "ProblemType": problemTypeConfig }
     self.initialSolutionParameters.update(defaultSolution)
-    if "InitialSolutionParameters" not in problemSizeGroupConfig:
-      print2("No InitialSolutionParameters; using defaults.")
-    else:
-      if problemSizeGroupConfig["InitialSolutionParameters"] != None:
-        for paramDict in problemSizeGroupConfig["InitialSolutionParameters"]:
-          for paramName in paramDict:
-            paramValueList = paramDict[paramName]
-            if isinstance(paramValueList, list):
-              if len(paramValueList) != 1:
-                printWarning("InitialSolutionParameters must have length=1: %s:%s" % (paramName, paramValueList))
-              self.initialSolutionParameters[paramName] = paramValueList[0]
-            else:
-              self.initialSolutionParameters[paramName] = paramValueList
-    print2("# InitialSolutionParameters: %s" % str(self.initialSolutionParameters))
 
     # fill in missing steps using defaults
     self.benchmarkCommonParameters = []
     self.forkParameters = []
-    self.benchmarkForkParameters = []
-    self.joinParameters = []
-    self.benchmarkJoinParameters = []
     self.benchmarkFinalParameters = []
     self.benchmarkSteps = []
     self.hardcodedParameters = [{}]
-    self.singleValueParameters = {}
+    self.singleValueParameters = {} # keep
     self.solutionSummationSizes = []
 
+    self.multiValueParameters = {}
+
     # (I)
     self.fillInMissingStepsWithDefaults(self.isBatched, problemSizeGroupConfig)
 
@@ -207,79 +158,46 @@ def fillInMissingStepsWithDefaults(self, isbatched, config):
     print2("####################################################################")
     print2("")
 
-    self.solutionSummationSizes = defaultSolutionSummationSizes
-    ############################################################################
-    # (I-0) get 6 phases from config
-    configBenchmarkCommonParameters = config["BenchmarkCommonParameters"] \
-        if "BenchmarkCommonParameters" in config else []
-    configForkParameters = config["ForkParameters"] \
-        if "ForkParameters" in config else []
-
-    # TODO cleanup error messages
-    if config.get("BenchmarkForkParameters") is not None:
-      print("no longer supported")
-    if config.get("JoinParameters") is not None:
-      print("no longer supported")
-    if config.get("BenchmarkJoinParameters") is not None:
-      print("no longer supported")
-
-    configBenchmarkFinalParameters = config["BenchmarkFinalParameters"] \
-        if "BenchmarkFinalParameters" in config and config["BenchmarkFinalParameters"] != None \
-        and len(config["BenchmarkFinalParameters"]) > 0 \
-        else [{"ProblemSizes": defaultBatchedBenchmarkFinalProblemSizes}] \
-        if isbatched \
+    # check for no longer supported legacy benchmark steps
+    badParams = ["InitialSolutionParameters", "BenchmarkForkParameters", \
+                 "JoinParameters", "BenchmarkJoinParameters"]
+    badsInConfig = []
+
+    for p in badParams:
+      if config.get(p) is not None:
+        badsInConfig.append(p)
+
+    if len(badsInConfig) == 1:
+      printExit("Benchmark step {} is no longer supported".format("'" + badsInConfig[0] + "'"))
+    elif len(badsInConfig) > 1:
+      printExit("Benchmark steps {} are no longer supported".format(badsInConfig))
+
+    # get supported legacy benchmark steps
+    defaultSizes = [{"ProblemSizes": defaultBatchedBenchmarkFinalProblemSizes}] if isbatched \
         else [{"ProblemSizes": defaultBenchmarkFinalProblemSizes}]
 
-    ############################################################################
-    # Ensure only valid solution parameters were requested
-    validParameterNames = set(validParameters.keys())
-    for paramDictList in [configBenchmarkCommonParameters, \
-        configForkParameters]:
-      if paramDictList != None:
-        for paramDict in paramDictList:
-          try:
-            checkForValidParameters(paramDict, validParameterNames)
-          except RuntimeError as e:
-            printExit(str(e))
-
-    # TODO warn/guard against BenchmarkCommons that have mor than one value
-    benchmarkCommonParameters = fillMissingParametersWithDefaults( \
-        [configBenchmarkCommonParameters, configForkParameters], \
-        deepcopy(defaultBenchmarkCommonParameters))
-    self.benchmarkCommonParameters.extend(benchmarkCommonParameters)
-
-    if configBenchmarkCommonParameters != None:
-      for paramDict in configBenchmarkCommonParameters:
-        self.benchmarkCommonParameters.append(paramDict)
+    self.solutionSummationSizes    = defaultSolutionSummationSizes
+    self.benchmarkCommonParameters = config.get("BenchmarkCommonParameters", [])
+    self.forkParameters            = config.get("ForkParameters", [])
+    self.benchmarkFinalParameters  = config.get("BenchmarkFinalParameters", defaultSizes)
 
-    ############################################################################
-    # (I-2) into fork we put in all Dfork that
-    # don't show up in Bcommon/Cfork/CBfork/Cjoin/CBjoin
-    # followed by Cfork
-    self.forkParameters = []
-    # need to use deepcopy to prevent default parameters from being washed-out later
-    for paramDict in deepcopy(defaultForkParameters):
-      for paramName in paramDict:
-        if not hasParam( paramName, [ self.benchmarkCommonParameters, \
-            configForkParameters]) \
-            or paramName == "ProblemSizes":
-          self.forkParameters.append(paramDict)
-    if configForkParameters != None:
-      for paramDict in configForkParameters:
-        self.forkParameters.append(paramDict)
-    else: # make empty
-      self.forkParameters = []
+    configParameters = self.benchmarkCommonParameters + self.forkParameters
 
-    ############################################################################
-    # (I-6) benchmark final sizes
-    self.benchmarkFinalParameters = configBenchmarkFinalParameters
-    # no other parameters besides problem sizes
+    # ensure only valid solution parameters were requested
+    validParameterNames = set(validParameters.keys())
+    for paramDict in configParameters:
+      try:
+        checkForValidParameters(paramDict, validParameterNames)
+      except RuntimeError as e:
+        printExit(str(e))
+
+    missingParameters = getDefaultsForMissingParameters( \
+        configParameters, deepcopy(defaultBenchmarkCommonParameters))
 
     ############################################################################
     # (I-7) any default param with 1 value will be hardcoded; move to beginning
-    singleValues = getSingleValues([self.benchmarkCommonParameters, \
-        self.forkParameters, self.benchmarkForkParameters, \
-        self.benchmarkJoinParameters])
+    singleValues = getSingleValues([missingParameters, self.benchmarkCommonParameters, \
+        self.forkParameters])
     for paramName in singleValues:
       paramValue = singleValues[paramName]
       self.hardcodedParameters[0][paramName] = paramValue
@@ -307,7 +225,7 @@ def fillInMissingStepsWithDefaults(self, isbatched, config):
   def convertParametersToSteps(self):
     print2("")
     print2("####################################################################")
-    print1("# Convert Parameters to Steps")
+    print1("# Convert Parameters to Benchmark Step")
     print2("####################################################################")
     print2("")
 
@@ -345,38 +263,6 @@ def convertParametersToSteps(self):
         self.benchmarkSteps.append(benchmarkStep)
         self.benchmarkStepIdx+=1
 
-  ##############################################################################
-  # For list of config parameters convert to steps and append to steps list
-  ##############################################################################
-  def addStepsForParameters(self, configParameterList):
-    print2("# AddStepsForParameters: %s" % configParameterList)
-    for paramConfig in configParameterList:
-      if isinstance(paramConfig, dict):
-        if "ProblemSizes" in paramConfig:
-          self.currentProblemSizes = ProblemSizes(self.problemType, paramConfig["ProblemSizes"])
-          continue
-      currentBenchmarkParameters = {}
-      for paramName in paramConfig:
-        paramValues = paramConfig[paramName]
-        if isinstance(paramValues, list):
-          currentBenchmarkParameters[paramName] = paramValues
-        else:
-          printExit("Parameter \"%s\" for ProblemType %s must be formatted as a list but isn't" \
-              % ( paramName, str(self.problemType) ) )
-      if len(currentBenchmarkParameters) > 0:
-        print2("Adding BenchmarkStep for %s" % str(currentBenchmarkParameters))
-        checkCDBufferAndStrides(self.problemType, self.currentProblemSizes, \
-            globalParameters["CEqualD"])
-        benchmarkStep = BenchmarkStep(
-            self.hardcodedParameters,
-            currentBenchmarkParameters,
-            self.initialSolutionParameters,
-            self.currentProblemSizes,
-            self.benchmarkStepIdx )
-        self.benchmarkSteps.append(benchmarkStep)
-        self.benchmarkStepIdx+=1
-
-
   ##############################################################################
   # Add new permutations of hardcoded parameters to old permutations of params
   ##############################################################################
diff --git a/Tensile/Common.py b/Tensile/Common.py
index 308eb7df7..d64c997ae 100644
--- a/Tensile/Common.py
+++ b/Tensile/Common.py
@@ -1299,19 +1299,12 @@ def getArchitectureName(gfxName):
     {"GroupLoadStore":            [ False ] },
     {"MIArchVgpr":                [ False ] },
     ]
-# benchmark these solution independently
-defaultForkParameters = []
-defaultBenchmarkForkParameters = []
-defaultJoinParameters = []
-defaultBenchmarkJoinParameters = []
 
-# dictionary of defaults comprised for 1st option for each parameter
+# dictionary of defaults comprised of default option for each parameter
 defaultSolution = {}
-for paramList in [defaultBenchmarkCommonParameters, defaultForkParameters, \
-    defaultBenchmarkForkParameters,defaultBenchmarkJoinParameters]:
-  for paramDict in paramList:
-    for key, value in paramDict.items():
-      defaultSolution[key] = value[0]
+for paramDict in defaultBenchmarkCommonParameters:
+  for key, value in paramDict.items():
+    defaultSolution[key] = value[0]
 # other non-benchmark options for solutions
 
 # valid fields in ConvolutionConfig and explanations:
@@ -1746,7 +1739,7 @@ def detectGlobalCurrentISA():
   Returns returncode if detection failure
   """
   global globalParameters
-  
+
   if globalParameters["CurrentISA"] == (0,0,0) and globalParameters["ROCmAgentEnumeratorPath"]:
     process = subprocess.run([globalParameters["ROCmAgentEnumeratorPath"]], stdout=subprocess.PIPE)
     if os.name == "nt":
@@ -1771,7 +1764,7 @@ def detectGlobalCurrentISA():
       printWarning("%s exited with code %u" % (globalParameters["ROCmAgentEnumeratorPath"], process.returncode))
     return process.returncode
   return 0
-      
+
 def restoreDefaultGlobalParameters():
   """
   Restores `globalParameters` back to defaults.
diff --git a/Tensile/Tests/unit/test_TensileCreateLibrary.py b/Tensile/Tests/unit/test_TensileCreateLibrary.py
index 0b12c807f..0ae8cc1a5 100644
--- a/Tensile/Tests/unit/test_TensileCreateLibrary.py
+++ b/Tensile/Tests/unit/test_TensileCreateLibrary.py
@@ -34,25 +34,6 @@
 
 mylogger = logging.getLogger()
 
-def test_assignParameters():
-    problemTypeConfig = \
-        {"Batched": True, "DataType": "s", "OperationType": "GEMM", "TransposeA": False, "TransposeB": False, "UseBeta": True}
-
-    benchmarkCommonParameters = [{"LoopTail": [True]}, {"KernelLanguage": ["Assembly"]}, \
-        {"EdgeType": ["ShiftPtr"]}, {"GlobalSplitU": [1]}, {"VectorWidth": [-1]}, {"FractionalLoad": [1]}, \
-        {"PrefetchGlobalRead": [True]}]
-
-    configForkParameters = \
-        [{"WorkGroup": [[16, 16, 1]]}, {"ThreadTile": [[4, 4],[8, 8]]}]
-
-    problemTypeObj, hardcodedParameters, initialSolutionParameters = \
-        BenchmarkStructs.assignParameters(problemTypeConfig, benchmarkCommonParameters, configForkParameters)
-
-
-    assert problemTypeObj != None
-    assert hardcodedParameters != None
-    assert initialSolutionParameters != None
-
 def test_generateSolutions(useGlobalParameters):
     with useGlobalParameters():
         scriptDir = os.path.dirname(os.path.realpath(__file__))

From 182c79ddf57a18efe19d8a7e166c4efab6268084 Mon Sep 17 00:00:00 2001
From: Benjamin Ulmer <benjamin.ulmer@amd.com>
Date: Wed, 6 Oct 2021 15:25:48 -0600
Subject: [PATCH 04/12] Simplify benchmark process to work off idea of single
 and multi value parameters

---
 Tensile/BenchmarkProblems.py | 237 ++++++++++-------------------------
 Tensile/BenchmarkStructs.py  | 212 ++++++++++---------------------
 Tensile/LibraryIO.py         |  21 ++--
 3 files changed, 141 insertions(+), 329 deletions(-)

diff --git a/Tensile/BenchmarkProblems.py b/Tensile/BenchmarkProblems.py
index dbe62450a..691ada12f 100644
--- a/Tensile/BenchmarkProblems.py
+++ b/Tensile/BenchmarkProblems.py
@@ -34,8 +34,8 @@
 from . import Utils
 from .BenchmarkStructs import BenchmarkProcess, constructForkPermutations, checkForValidParameters
 from .ClientWriter import runClient, writeClientConfig
-from .Common import globalParameters, HR, pushWorkingPath, popWorkingPath, print1, print2, printExit, printWarning, ensurePath, \
-                    startTime, validParameters
+from .Common import globalParameters, HR, pushWorkingPath, popWorkingPath, print1, print2, \
+    printExit, printWarning, ensurePath, startTime, validParameters
 from .KernelWriterAssembly import KernelWriterAssembly
 from .KernelWriterSource import KernelWriterSource
 from .SolutionStructs import Solution, ProblemType, ProblemSizes
@@ -43,10 +43,8 @@
 from .TensileCreateLibrary import writeSolutionsAndKernels, writeCMake, buildObjectFileNames
 from .CustomKernels import getCustomKernelConfig
 
-############################################################################
-# generateForkedSolutions
-############################################################################
-def generateForkedSolutions (problemType, hardcodedParameters, benchmarkPermutations, initialSolutionParameters=None):
+
+def generateForkedSolutions (problemType, constantParams, benchmarkPermutations):
   """this creates a set or solutions based on the forked parameters using
      a set of common parameters from which to fork from
 
@@ -59,43 +57,26 @@ def generateForkedSolutions (problemType, hardcodedParameters, benchmarkPermutat
 
   Returns:
   list: Soutions list
-
   """
+  print1("# Enumerating Solutions")
 
   solutions = []
-  numHardcoded = len(hardcodedParameters)
-
-  print1("# Enumerating Solutions")
-  solutionSet = set()
-
-  for hardcodedIdx in Utils.tqdm(range(0, numHardcoded), "Enumerating Solutions"):
-    solutions.append([])
-    hardcodedParamDict = hardcodedParameters[hardcodedIdx]
-    for benchmarkPermutation in benchmarkPermutations:
-      solution = {"ProblemType": deepcopy(problemType.state)}
-      solution.update(benchmarkPermutation)
-      solution.update(hardcodedParamDict)
-
-      # append default parameters where necessary
-      if initialSolutionParameters:
-        for initialSolutionParameterName in initialSolutionParameters:
-          if initialSolutionParameterName not in solution:
-            solution[initialSolutionParameterName] = \
-              initialSolutionParameters[initialSolutionParameterName]
-
-      # TODO check if solution matches problem size for exact tile kernels
-      solutionObject = Solution(solution)
-      if solutionObject["Valid"]:
-        if solutionObject not in solutionSet:
-          solutionSet.add(solutionObject)
-          solutions[hardcodedIdx].append(solutionObject)
-      else:
-        if globalParameters["PrintSolutionRejectionReason"]:
-          print1("rejecting solution %s" % str(solutionObject))
+  for benchmarkPermutation in benchmarkPermutations:
+    solution = {"ProblemType": deepcopy(problemType.state)}
+    solution.update(constantParams)
+    solution.update(benchmarkPermutation)
+
+    # TODO check if solution matches problem size for exact tile kernels
+    solutionObject = Solution(solution)
+    if solutionObject["Valid"]:
+      solutions.append(solutionObject)
+    elif globalParameters["PrintSolutionRejectionReason"]:
+      print1("rejecting solution " + str(solutionObject))
 
   return solutions
 
 def generateCustomKernelSolution(kernelName, directory=globalParameters["CustomKernelDirectory"]):
+    """Temp docs"""
     kernelConfig = getCustomKernelConfig(kernelName, directory)
     checkForValidParameters({p: [kernelConfig[p]] for p in kernelConfig if p != "ProblemType"}, set(validParameters.keys()))
     # test if problem type matches with configuration file
@@ -104,12 +85,8 @@ def generateCustomKernelSolution(kernelName, directory=globalParameters["CustomK
 
     return Solution(kernelConfig)
 
-################################################################################
-# Benchmark Problem Type
-################################################################################
-def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, \
-    problemSizeGroupIdx ):
-
+def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, problemSizeGroupIdx):
+  """Temp docs"""
   benchmarkTestFails = 0
 
   # convert config to full benchmark process (resolves defaults)
@@ -118,8 +95,7 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, \
   print1("# Converting Config to BenchmarkProcess Object")
   print1(HR)
   print1("")
-  benchmarkProcess = BenchmarkProcess( problemTypeConfig, \
-      problemSizeGroupConfig )
+  benchmarkProcess = BenchmarkProcess(problemTypeConfig, problemSizeGroupConfig)
 
   enableTileSelection = benchmarkProcess.problemType["TileAwareSelection"]
   problemTypeName = str(benchmarkProcess.problemType)
@@ -130,20 +106,15 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, \
   totalBenchmarkSteps = len(benchmarkProcess)
   resultsFileBaseFinal = None
 
-  print1("# NumBenchmarkSteps: %u" % totalBenchmarkSteps)
+  print1("# NumBenchmarkSteps: {}".format(totalBenchmarkSteps))
   print1("")
   print1(HR)
   print1("# Done Creating BenchmarkProcess Object")
   print1(HR)
 
-  ##############################################################################
-  # For Each Benchmark Step
-  ##############################################################################
-  for benchmarkStepIdx in range(0, totalBenchmarkSteps):
 
+  for benchmarkStepIdx in range(0, totalBenchmarkSteps):
     benchmarkStep = benchmarkProcess[benchmarkStepIdx]
-
-    numHardcoded = len(benchmarkStep.hardcodedParameters)
     stepName = str(benchmarkStep)
     shortName = benchmarkStep.abbreviation()
 
@@ -151,26 +122,17 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, \
     print1(HR)
     currentTime = time.time()
     elapsedTime = currentTime - startTime
-    print1("# BenchmarkStep: %s - %s %.3fs" % (problemSizeGroupName, stepName, elapsedTime))
-    print1("# NumProblems: %u" % benchmarkStep.problemSizes.totalProblemSizes)
-    print1("# BenchmarkParameters:")
-
-    for paramName in benchmarkStep.benchmarkParameters:
-      paramValues = benchmarkStep.benchmarkParameters[paramName]
-      printStr = "#     %s = { %s" % (paramName, paramValues[0])
-      for paramValueIdx in range(1, len(paramValues)):
-        printStr += ", %s" % str(paramValues[paramValueIdx])
-      printStr += " }"
-      print1(printStr)
+    print1("# Benchmark Step: {} - {} {:.3f}s".format(problemSizeGroupName, stepName, elapsedTime))
+    print1("# Num Sizes: {}".format(benchmarkStep.problemSizes.totalProblemSizes))
+    print1("# Fork Parameters:")
+    for k, v in benchmarkStep.forkParams.items():
+      print1("#     {}: {}".format(k, v))
 
     pushWorkingPath(shortName)
 
-    ############################################################################
-    # Copy Files to Benchmark Source Directory
-    ############################################################################
+    # copy files to benchmark source directory
     stepBaseDir = globalParameters["WorkingPath"]
-    sourceDir = \
-      os.path.join(stepBaseDir, "source" )
+    sourceDir = os.path.join(stepBaseDir, "source" )
     ensurePath(sourceDir)
 
     filesToCopy = []
@@ -194,29 +156,11 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, \
           os.path.join(globalParameters["SourcePath"], "FindHIP.cmake"),
           globalParameters["WorkingPath"] )
 
-    ############################################################################
-    # Enumerate Benchmark Permutations
-    ############################################################################
-    solutions = []
-
-    benchmarkPermutations = constructForkPermutations(benchmarkStep.benchmarkParameters)
-    maxPossibleSolutions = len(benchmarkPermutations) * numHardcoded
-
-    ############################################################################
-    # Enumerate Solutions = Hardcoded * Benchmark
-    ############################################################################
+    # enumerate benchmark permutations and create resulting solution objects
+    benchmarkPermutations = constructForkPermutations(benchmarkStep.forkParams)
+    maxPossibleSolutions = len(benchmarkPermutations) #* numHardcoded
     solutions = generateForkedSolutions(benchmarkProcess.problemType, \
-        benchmarkStep.hardcodedParameters, benchmarkPermutations, \
-        benchmarkStep.initialSolutionParameters)
-
-    # remove hardcoded that don't have any valid benchmarks
-    removeHardcoded = list([x for i, x in enumerate(benchmarkStep.hardcodedParameters) \
-        if len(solutions[i]) == 0])
-    validHardcoded =  list([x for i, x in enumerate(benchmarkStep.hardcodedParameters) \
-        if len(solutions[i]) > 0])
-
-    removesExist = len(removeHardcoded) > 0
-    benchmarkStep.hardcodedParameters = validHardcoded
+        benchmarkStep.constantParams, benchmarkPermutations)
 
     # add custom kernels to list of solutions
     customKernelList = problemSizeGroupConfig.get("CustomKernels", [])
@@ -233,10 +177,13 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, \
       if customSolution["ProblemType"] != benchmarkProcess.problemType:
         # Raise error if this kernel was specifically requested and problem type doesn't match
         if not customKernelWildcard:
-          missingParams = [p for p in benchmarkProcess.problemType if p not in customSolution["ProblemType"]]
-          extraParams   = [p for p in customSolution["ProblemType"] if p not in benchmarkProcess.problemType]
+          missingParams = [p for p in benchmarkProcess.problemType \
+              if p not in customSolution["ProblemType"]]
+          extraParams   = [p for p in customSolution["ProblemType"] \
+              if p not in benchmarkProcess.problemType]
+
           msg  = "The problem type in the config file does not match" \
-              "that of the custom kernel, {0}.".format(kernelName)
+                 "that of the custom kernel, {0}.".format(kernelName)
           msg += "\nMissing config parameters:\n" + str(missingParams)
           msg += "\nExtra custom kernel parameters:\n" + str(extraParams)
           raise RuntimeError(msg)
@@ -246,22 +193,15 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, \
         print1("# Added {} to solutions".format(kernelName))
         maxPossibleSolutions += 1
         if customSolution["Valid"]:
-          solutions.append([customSolution])
-          benchmarkStep.hardcodedParameters.append(customSolution._state)
+          solutions.append(customSolution)
         elif globalParameters["PrintSolutionRejectionReason"]:
-          print1("rejecting solution %s" % str(customSolution))
-
-    numHardcoded = len(benchmarkStep.hardcodedParameters )
-    if removesExist:
-      solutions = list([s for s in solutions if len(s) > 0])
-
-    print1("# Actual Solutions: %u / %u after SolutionStructs\n" % ( len(solutions), \
-        maxPossibleSolutions ))
+          print1("rejecting solution " + str(customSolution))
 
-    # create linear list
-    solutionList = list(itertools.chain.from_iterable(solutions))
+    print1("# Actual Solutions: {} / {} after SolutionStructs\n" \
+        .format(len(solutions), maxPossibleSolutions))
 
-    if len(solutionList) == 0:
+    # handle no valid solutions
+    if len(solutions) == 0:
         msg = "Your parameters resulted in 0 valid solutions."
         if globalParameters["PrintSolutionRejectionReason"]:
             msg += "\nExamine reject and backtrace messages above to see why" \
@@ -270,65 +210,32 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, \
             msg += "\nYou should re-run with \"PrintSolutionRejectionReason: True\"" \
                 "to see why each parameter combination was rejected."
         printExit(msg)
+
     if globalParameters["PrintLevel"] >= 1:
-      for i,solutionsForHardcoded in enumerate(solutions):
-        for j, solution in enumerate(solutionsForHardcoded):
-          print2("#    (%u:%u) %s" % (i, j, \
-              Solution.getNameFull(solution) ))
+      for solution in solutions:
+        print2("#    (%u:%u) %s" % (0, 0, Solution.getNameFull(solution) ))
       print2(HR)
 
     # write benchmarkFiles
-    writeBenchmarkFiles(stepBaseDir, solutionList, benchmarkStep.problemSizes, \
+    prevCount = len(solutions)
+    writeBenchmarkFiles(stepBaseDir, solutions, benchmarkStep.problemSizes, \
         shortName, filesToCopy, benchmarkProcess.solutionSummationSizes)
+    # ^ this mutates solutions
 
-    removeSolutions = []
-    for i in range(0, len(solutions)):
-      solutionsForHardcoded = solutions[i]
-      removeSolutions.append([])
-      for j in range(0, len(solutionsForHardcoded)):
-        solution = solutionsForHardcoded[j]
-        if solutionList.count(solution) == 0:
-          removeSolutions[i].append(solution)
-
-    for i in range(0, len(solutions)):
-      solutionsForHardcoded = solutions[i]
-      for j in range(0, len(removeSolutions[i])):
-          solutionsForHardcoded.remove(removeSolutions[i][j])
-
-    # remove hardcoded that don't have any valid benchmarks
-    removeHardcoded = []
-    for hardcodedIdx in range(0, numHardcoded):
-      if len(solutions[hardcodedIdx]) == 0:
-        hardcodedParamDict = benchmarkStep.hardcodedParameters[hardcodedIdx]
-        removeHardcoded.append(hardcodedParamDict)
-    removesExist = len(removeHardcoded) > 0
-    for hardcodedParam in removeHardcoded:
-      benchmarkStep.hardcodedParameters.remove(hardcodedParam)
-
-    if removesExist:
-      numHardcoded = len(benchmarkStep.hardcodedParameters )
-      # remove from solution 2D list also
-      prevCount = len(solutions)
-      solutions = list([s for s in solutions if len(s) > 0])
-      print1("# Actual Solutions: %u / %u after KernelWriter\n" \
-            % (len(solutions), prevCount ))
+    print1("# Actual Solutions: %u / %u after KernelWriter\n" \
+          % (len(solutions), prevCount ))
 
     popWorkingPath() # source
 
-    ############################################################################
-    # Run Benchmark Script
-    ############################################################################
+    # run benchmarking client
     resultsFileBase = os.path.normpath(os.path.join( \
         globalParameters["WorkingPath"], "../Data", shortName))
     if benchmarkStep.isFinal():
       resultsFileBaseFinal = resultsFileBase
     resultsFileName = resultsFileBase + ".csv"
-    newResultsFileName = None # Add another results file to generate diff
     solutionsFileName = resultsFileBase + ".yaml"
-    if not os.path.exists(resultsFileName) or \
-        globalParameters["ForceRedoBenchmarkProblems"]:
-
 
+    if not os.path.exists(resultsFileName) or globalParameters["ForceRedoBenchmarkProblems"]:
       libraryLogicPath = None
       forBenchmark = True
       returncode = runClient(libraryLogicPath, forBenchmark, enableTileSelection)
@@ -339,11 +246,8 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, \
     else:
       print1("# Already benchmarked; skipping.")
 
-    ############################################################################
-    # Write Solutions YAML
-    ############################################################################
-    LibraryIO.writeSolutions(solutionsFileName, benchmarkStep.problemSizes, \
-        solutions )
+    # write solutions YAML
+    LibraryIO.writeSolutions(solutionsFileName, benchmarkStep.problemSizes, solutions)
 
     # End Iteration
     popWorkingPath() # stepName
@@ -357,6 +261,7 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, \
 # End benchmarkProblemType()
 
 def compareResults(old, new, name):
+    """Temp doc"""
     import math
     if name == " WinnerIdx":
       return 0
@@ -383,11 +288,8 @@ def isbad(x):
 
     return abs((old-new)/old)
 
-################################################################################
-# Read GFlop/s from file
-################################################################################
 def getResults(resultsFileName, solutions, enableTileSelection, newResultsFileName=None):
-
+  """Temp docs"""
   print1("# Get Results from CSV")
   try:
     resultsFile = open(resultsFileName, "r")
@@ -459,19 +361,14 @@ def getResults(resultsFileName, solutions, enableTileSelection, newResultsFileNa
     diffFile.close()
   return results
 
-
-################################################################################
-# Write Benchmark Files
-################################################################################
-def writeBenchmarkFiles(stepBaseDir, solutions, problemSizes, stepName, filesToCopy, solutionSummationSizes):
+def writeBenchmarkFiles(stepBaseDir, solutions, problemSizes, \
+    stepName, filesToCopy, solutionSummationSizes):
+  """Temp doc"""
   if not globalParameters["MergeFiles"] or globalParameters["NumMergedFiles"] > 1:
     ensurePath(os.path.join(globalParameters["WorkingPath"], "Solutions"))
     ensurePath(os.path.join(globalParameters["WorkingPath"], "Kernels"))
 
-  ##############################################################################
-  # Min Naming
-  ##############################################################################
-
+  # min Naming
   kernels = []
   kernelHelperOjbs = []
 
@@ -507,6 +404,7 @@ def writeBenchmarkFiles(stepBaseDir, solutions, problemSizes, stepName, filesToC
   codeObjectFiles = writeSolutionsAndKernels( \
       globalParameters["WorkingPath"], globalParameters["CxxCompiler"], [problemType], solutions, kernels, kernelHelperOjbs, \
       solutionWriter, kernelWriterSource, kernelWriterAssembly, errorTolerant=True )
+  # ^ this is where solutions is mutated
 
   newLibraryDir = ensurePath(os.path.join(globalParameters["WorkingPath"], 'library'))
   newLibraryFile = os.path.join(newLibraryDir, "TensileLibrary")
@@ -545,9 +443,7 @@ def writeBenchmarkFiles(stepBaseDir, solutions, problemSizes, stepName, filesToC
   if len(solutions) == 0:
     printExit("write solutions and kernels results 0 valid soultion.")
 
-  ##############################################################################
-  # Write CMake
-  ##############################################################################
+  # write CMake
   outputPath = globalParameters["WorkingPath"]
 
   (solutionFiles,
@@ -564,9 +460,6 @@ def writeBenchmarkFiles(stepBaseDir, solutions, problemSizes, stepName, filesToC
       outputPath )
 
 
-################################################################################
-# Main
-################################################################################
 def main(config):
   """Entry point for the "BenchmarkProblems" section of a Tensile config yaml"""
   ClientExecutable.getClientExecutable()
diff --git a/Tensile/BenchmarkStructs.py b/Tensile/BenchmarkStructs.py
index 2c3086653..ed50610c7 100644
--- a/Tensile/BenchmarkStructs.py
+++ b/Tensile/BenchmarkStructs.py
@@ -21,24 +21,12 @@
 
 from copy import copy, deepcopy
 from .Common import print1, print2, printWarning, defaultSolution, \
-    defaultProblemSizes, defaultBenchmarkFinalProblemSizes, \
-    defaultBatchedProblemSizes, defaultBatchedBenchmarkFinalProblemSizes, \
-    defaultBenchmarkCommonParameters, hasParam, getParamValues, printExit, \
-    validParameters, defaultSolutionSummationSizes, globalParameters
+    defaultBenchmarkFinalProblemSizes, defaultBatchedBenchmarkFinalProblemSizes, \
+    defaultBenchmarkCommonParameters, hasParam, printExit, \
+    validParameters, globalParameters
 from .SolutionStructs import Solution, ProblemType, ProblemSizes
 
 
-def forkHardcodedParameters(basePermutations, update):
-  """Temp doc"""
-  updatedHardcodedParameters = []
-  for oldPermutation in basePermutations:
-    for newPermutation in update:
-      permutation = {}
-      permutation.update(oldPermutation)
-      permutation.update(newPermutation)
-      updatedHardcodedParameters.append(permutation)
-  return updatedHardcodedParameters
-
 def getDefaultsForMissingParameters(parameterConfigurationList, defaultParameters):
   """Temp doc"""
   benchmarkParameters = []
@@ -68,20 +56,17 @@ def checkForValidParameters(params, validParameterNames):
 def constructForkPermutations(forkParametersConfig):
   """Temp doc"""
   totalPermutations = 1
-  for param in forkParametersConfig:
-    for name in param: # only 1
-      values = param[name]
-      totalPermutations *= len(values)
+  for k, v in forkParametersConfig.items():
+    totalPermutations *= len(v)
   forkPermutations = []
   for i in range(0, totalPermutations):
     forkPermutations.append({})
     pIdx = i
-    for param in forkParametersConfig:
-      for name in param:
-        values = deepcopy(param[name])
-        valueIdx = pIdx % len(values)
-        forkPermutations[i][name] = values[valueIdx]
-        pIdx //= len(values)
+    for k, v in forkParametersConfig.items():
+      values = deepcopy(v)
+      valueIdx = pIdx % len(v)
+      forkPermutations[i][k] = values[valueIdx]
+      pIdx //= len(values)
   return forkPermutations
 
 def getSingleValues(parameterSetList):
@@ -114,6 +99,7 @@ def checkCDBufferAndStrides(problemType, problemSizes, isCEqualD):
 class BenchmarkProcess:
   """
   Steps in config need to be expanded and missing elements need to be assigned a default.
+  TODO better docs
   """
 
   def __init__(self, problemTypeConfig, problemSizeGroupConfig):
@@ -126,32 +112,19 @@ def __init__(self, problemTypeConfig, problemSizeGroupConfig):
     self.initialSolutionParameters = { "ProblemType": problemTypeConfig }
     self.initialSolutionParameters.update(defaultSolution)
 
-    # fill in missing steps using defaults
-    self.benchmarkCommonParameters = []
-    self.forkParameters = []
-    self.benchmarkFinalParameters = []
-    self.benchmarkSteps = []
-    self.hardcodedParameters = [{}]
-    self.singleValueParameters = {} # keep
-    self.solutionSummationSizes = []
-
+    # fill parameter values from config
+    self.singleValueParameters = {}
     self.multiValueParameters = {}
+    self.getConfigParameter(self.isBatched, problemSizeGroupConfig)
 
-    # (I)
-    self.fillInMissingStepsWithDefaults(self.isBatched, problemSizeGroupConfig)
-
-    # convert list of parameters to list of steps
+    # convert parameter lists to steps
+    # currently only 1 benchmark step is possible, more may be added back later
     self.currentProblemSizes = []
     self.benchmarkStepIdx = 0
-
-    # (II)
     self.convertParametersToSteps()
 
-
-  ##############################################################################
-  # (I) Create lists of param, filling in missing params from defaults
-  ##############################################################################
-  def fillInMissingStepsWithDefaults(self, isbatched, config):
+  def getConfigParameter(self, isbatched, config):
+    """Temp doc"""
     print2("")
     print2("####################################################################")
     print1("# Filling in Parameters With Defaults")
@@ -176,12 +149,11 @@ def fillInMissingStepsWithDefaults(self, isbatched, config):
     defaultSizes = [{"ProblemSizes": defaultBatchedBenchmarkFinalProblemSizes}] if isbatched \
         else [{"ProblemSizes": defaultBenchmarkFinalProblemSizes}]
 
-    self.solutionSummationSizes    = defaultSolutionSummationSizes
-    self.benchmarkCommonParameters = config.get("BenchmarkCommonParameters", [])
-    self.forkParameters            = config.get("ForkParameters", [])
+    benchmarkCommonParameters      = config.get("BenchmarkCommonParameters", [])
+    forkParameters                 = config.get("ForkParameters", [])
     self.benchmarkFinalParameters  = config.get("BenchmarkFinalParameters", defaultSizes)
 
-    configParameters = self.benchmarkCommonParameters + self.forkParameters
+    configParameters = benchmarkCommonParameters + forkParameters
 
     # ensure only valid solution parameters were requested
     validParameterNames = set(validParameters.keys())
@@ -191,95 +163,55 @@ def fillInMissingStepsWithDefaults(self, isbatched, config):
       except RuntimeError as e:
         printExit(str(e))
 
+    # get defaults for parameters not specified in config file
     missingParameters = getDefaultsForMissingParameters( \
         configParameters, deepcopy(defaultBenchmarkCommonParameters))
 
-    ############################################################################
-    # (I-7) any default param with 1 value will be hardcoded; move to beginning
-    singleValues = getSingleValues([missingParameters, self.benchmarkCommonParameters, \
-        self.forkParameters])
-    for paramName in singleValues:
-      paramValue = singleValues[paramName]
-      self.hardcodedParameters[0][paramName] = paramValue
-      self.singleValueParameters[paramName] = [ paramValue ]
-      self.initialSolutionParameters[paramName] = paramValue
-
-    ############################################################################
-    # (I-10) Parameter Lists
-    # benchmarkCommonParameters
-    print2("HardcodedParameters:")
-    for paramName in self.hardcodedParameters[0]:
-      paramValues = self.hardcodedParameters[0][paramName]
-      print2("    %s: %s" % (paramName, paramValues))
-    print2("BenchmarkCommonParameters:")
-    for step in self.benchmarkCommonParameters:
-      print2("    %s" % step)
-    # forkParameters
-    print2("ForkParameters:")
-    for param in self.forkParameters:
-      print2("    %s" % param)
-
-  ##############################################################################
-  # (II) convert lists of parameters to benchmark steps
-  ##############################################################################
+    # split parameters into single value and multi-value
+    self.singleValueParameters = getSingleValues([missingParameters, configParameters])
+
+    # above function call removes singles
+    self.multiValueParameters = {}
+    for paramDict in configParameters:
+      for param, values in paramDict.items():
+        self.multiValueParameters[param] = values
+
+    # print summary of parameter values
+    print2("Single Value Parameters:")
+    for k, v in self.singleValueParameters.items():
+      print2("    {}: {}".format(k, v))
+    print2("Multi-Value Parameters:")
+    for k, v in self.multiValueParameters.items():
+      print2("    {}: {}".format(k, v))
+
   def convertParametersToSteps(self):
+    """Temp doc"""
     print2("")
     print2("####################################################################")
-    print1("# Convert Parameters to Benchmark Step")
+    print1("# Convert Parameters to Benchmark Step(s)")
     print2("####################################################################")
     print2("")
 
-    ############################################################################
-    # (II-2) fork parameters
-    # calculate permutations of
-    print2("")
-    print2("####################################################################")
-    print1("# Fork Parameters")
-    print2(self.forkParameters)
-    forkPermutations = constructForkPermutations(self.forkParameters)
-    if len(forkPermutations) > 0:
-      self.forkHardcodedParameters(forkPermutations)
-
-    ############################################################################
-    # (II-6) benchmark final
+    # currently only a single step is supported
     print2("")
     print2("####################################################################")
     print1("# Benchmark Final")
     for problemSizesDict in self.benchmarkFinalParameters:
-      if "SolutionSummationSizes" in problemSizesDict:
-        self.solutionSummationSizes = problemSizesDict["SolutionSummationSizes"]
-      else:
         problemSizes = problemSizesDict["ProblemSizes"]
         self.currentProblemSizes = ProblemSizes(self.problemType, problemSizes)
-        currentBenchmarkParameters = {}
         checkCDBufferAndStrides(self.problemType, \
             self.currentProblemSizes, globalParameters["CEqualD"])
-        benchmarkStep = BenchmarkStep(
-            self.hardcodedParameters,
-            currentBenchmarkParameters,
-            self.initialSolutionParameters,
-            self.currentProblemSizes,
+        benchmarkStep = BenchmarkStep( \
+            self.multiValueParameters, \
+            self.singleValueParameters, \
+            self.currentProblemSizes, \
             self.benchmarkStepIdx )
         self.benchmarkSteps.append(benchmarkStep)
         self.benchmarkStepIdx+=1
 
-  ##############################################################################
-  # Add new permutations of hardcoded parameters to old permutations of params
-  ##############################################################################
-  def forkHardcodedParameters( self, update ):
-    #updatedHardcodedParameters = []
-    #for oldPermutation in self.hardcodedParameters:
-      #for newPermutation in update:
-      #  permutation = {}
-      #  permutation.update(oldPermutation)
-      #  permutation.update(newPermutation)
-      #  updatedHardcodedParameters.append(permutation)
-    updatedHardcodedParameters = forkHardcodedParameters( self.hardcodedParameters, update )
-      #updatedHardcodedParameters.append(permutation)
-    self.hardcodedParameters = updatedHardcodedParameters
-
   def __len__(self):
     return len(self.benchmarkSteps)
+
   def __getitem__(self, key):
     return self.benchmarkSteps[key]
 
@@ -288,59 +220,47 @@ def __str__(self):
     for step in self.benchmarkSteps:
       string += str(step)
     return string
+
   def __repr__(self):
     return self.__str__()
 
-################################################################################
-# Benchmark Step
-################################################################################
-class BenchmarkStep:
-
-  def __init__(self, hardcodedParameters, \
-      benchmarkParameters, initialSolutionParameters, problemSizes, idx):
-    # what is my step Idx
-    self.stepIdx = idx
-
-    # what parameters don't need to be benchmarked because hard-coded or forked
-    # it's a list of dictionaries, each element a permutation
-    self.hardcodedParameters = deepcopy(hardcodedParameters)
-    #if len(self.hardcodedParameters) == 0:
-    #  printExit("hardcodedParameters is empty")
 
-    # what parameters will I benchmark
-    self.benchmarkParameters = deepcopy(benchmarkParameters)
-    #if len(self.benchmarkParameters) == 0:
-    #  printExit("benchmarkParameters is empty")
-
-    # what solution parameters do I use for what hasn't been benchmarked
-    self.initialSolutionParameters = initialSolutionParameters
+class BenchmarkStep:
+  """Temp doc"""
 
-    # what problem sizes do I benchmark
+  def __init__(self, forkParams, constantParams, problemSizes, idx):
+    """Temp doc"""
+    #TODO see if deepcopy really needed
+    self.forkParams = deepcopy(forkParams)
+    self.constantParams = deepcopy(constantParams)
     self.problemSizes = deepcopy(problemSizes)
+    self.stepIdx = idx
 
-    print2("# Creating BenchmarkStep [BP]=%u [HCP]=%u [P]=%u" \
-        % ( len(benchmarkParameters), len(hardcodedParameters), \
-        problemSizes.totalProblemSizes))
+    print2("# Creating BenchmarkStep: {} fork params and {} sizes" \
+        .format( len(forkParams), problemSizes.totalProblemSizes))
 
   def isFinal(self):
-    return len(self.benchmarkParameters) == 0
+    """Temp doc"""
+    # currently only one benchmark step is possible
+    return True
 
   def abbreviation(self):
-    string = "%02u" % self.stepIdx
+    """Temp doc"""
+    string = "{:02d}".format(self.stepIdx)
     if self.isFinal():
       string += "_Final"
     else:
       for param in self.benchmarkParameters:
-        string += "_%s" % Solution.getParameterNameAbbreviation(param)
+        string += "_" + Solution.getParameterNameAbbreviation(param)
     return string
 
   def __str__(self):
-    string = "%02u" % self.stepIdx
+    string = "{:02d}".format(self.stepIdx)
     if self.isFinal():
       string += "_Final"
     else:
       for param in self.benchmarkParameters:
-        string += "_%s" % str(param)
+        string += "_" + str(param)
     return string
 
   def __repr__(self):
diff --git a/Tensile/LibraryIO.py b/Tensile/LibraryIO.py
index a3b762bea..6d2760dad 100644
--- a/Tensile/LibraryIO.py
+++ b/Tensile/LibraryIO.py
@@ -71,17 +71,16 @@ def writeSolutions(filename, problemSizes, solutions):
 
     # convert objects to nested dictionaries
     solutionStates = []
-    for hardcoded in solutions:
-        for solution in hardcoded:
-            solutionState = solution.getAttributes()
-            solutionState["ProblemType"] = solutionState["ProblemType"].state
-            solutionState["ProblemType"]["DataType"] = \
-                    solutionState["ProblemType"]["DataType"].value
-            solutionState["ProblemType"]["DestDataType"] = \
-                    solutionState["ProblemType"]["DestDataType"].value
-            solutionState["ProblemType"]["ComputeDataType"] = \
-                    solutionState["ProblemType"]["ComputeDataType"].value
-            solutionStates.append(solutionState)
+    for solution in solutions:
+        solutionState = solution.getAttributes()
+        solutionState["ProblemType"] = solutionState["ProblemType"].state
+        solutionState["ProblemType"]["DataType"] = \
+                solutionState["ProblemType"]["DataType"].value
+        solutionState["ProblemType"]["DestDataType"] = \
+                solutionState["ProblemType"]["DestDataType"].value
+        solutionState["ProblemType"]["ComputeDataType"] = \
+                solutionState["ProblemType"]["ComputeDataType"].value
+        solutionStates.append(solutionState)
     # write dictionaries
     with open(filename, "w") as f:
         f.write("- MinimumRequiredVersion: %s\n" % __version__ )

From 412c76aea83f4ee792f60506da332c91f1fc36ca Mon Sep 17 00:00:00 2001
From: Benjamin Ulmer <benjamin.ulmer@amd.com>
Date: Wed, 6 Oct 2021 15:57:41 -0600
Subject: [PATCH 05/12] Indent with 4 spaces instead of 2

---
 Tensile/BenchmarkProblems.py | 694 +++++++++++++++++------------------
 Tensile/BenchmarkStructs.py  | 446 +++++++++++-----------
 2 files changed, 570 insertions(+), 570 deletions(-)

diff --git a/Tensile/BenchmarkProblems.py b/Tensile/BenchmarkProblems.py
index 691ada12f..6d5db00de 100644
--- a/Tensile/BenchmarkProblems.py
+++ b/Tensile/BenchmarkProblems.py
@@ -35,7 +35,7 @@
 from .BenchmarkStructs import BenchmarkProcess, constructForkPermutations, checkForValidParameters
 from .ClientWriter import runClient, writeClientConfig
 from .Common import globalParameters, HR, pushWorkingPath, popWorkingPath, print1, print2, \
-    printExit, printWarning, ensurePath, startTime, validParameters
+        printExit, printWarning, ensurePath, startTime, validParameters
 from .KernelWriterAssembly import KernelWriterAssembly
 from .KernelWriterSource import KernelWriterSource
 from .SolutionStructs import Solution, ProblemType, ProblemSizes
@@ -45,35 +45,35 @@
 
 
 def generateForkedSolutions (problemType, constantParams, benchmarkPermutations):
-  """this creates a set or solutions based on the forked parameters using
-     a set of common parameters from which to fork from
-
-  Parameters:
-  problemType the problem type
-  hardcodedParameters the set of parameters which overrides the baseline parameters
-  benchmarkPermutations set of baseline parameters from which the the updates are branched form
-  winners previous winning parameters which overrides the derived parameters
-  initialSolutionParameters set of parameters which fills in missing params default parameters
-
-  Returns:
-  list: Soutions list
-  """
-  print1("# Enumerating Solutions")
-
-  solutions = []
-  for benchmarkPermutation in benchmarkPermutations:
-    solution = {"ProblemType": deepcopy(problemType.state)}
-    solution.update(constantParams)
-    solution.update(benchmarkPermutation)
-
-    # TODO check if solution matches problem size for exact tile kernels
-    solutionObject = Solution(solution)
-    if solutionObject["Valid"]:
-      solutions.append(solutionObject)
-    elif globalParameters["PrintSolutionRejectionReason"]:
-      print1("rejecting solution " + str(solutionObject))
-
-  return solutions
+    """this creates a set or solutions based on the forked parameters using
+         a set of common parameters from which to fork from
+
+    Parameters:
+    problemType the problem type
+    hardcodedParameters the set of parameters which overrides the baseline parameters
+    benchmarkPermutations set of baseline parameters from which the the updates are branched form
+    winners previous winning parameters which overrides the derived parameters
+    initialSolutionParameters set of parameters which fills in missing params default parameters
+
+    Returns:
+    list: Soutions list
+    """
+    print1("# Enumerating Solutions")
+
+    solutions = []
+    for benchmarkPermutation in benchmarkPermutations:
+        solution = {"ProblemType": deepcopy(problemType.state)}
+        solution.update(constantParams)
+        solution.update(benchmarkPermutation)
+
+        # TODO check if solution matches problem size for exact tile kernels
+        solutionObject = Solution(solution)
+        if solutionObject["Valid"]:
+            solutions.append(solutionObject)
+        elif globalParameters["PrintSolutionRejectionReason"]:
+            print1("rejecting solution " + str(solutionObject))
+
+    return solutions
 
 def generateCustomKernelSolution(kernelName, directory=globalParameters["CustomKernelDirectory"]):
     """Temp docs"""
@@ -86,280 +86,280 @@ def generateCustomKernelSolution(kernelName, directory=globalParameters["CustomK
     return Solution(kernelConfig)
 
 def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, problemSizeGroupIdx):
-  """Temp docs"""
-  benchmarkTestFails = 0
-
-  # convert config to full benchmark process (resolves defaults)
-  print1("")
-  print1(HR)
-  print1("# Converting Config to BenchmarkProcess Object")
-  print1(HR)
-  print1("")
-  benchmarkProcess = BenchmarkProcess(problemTypeConfig, problemSizeGroupConfig)
-
-  enableTileSelection = benchmarkProcess.problemType["TileAwareSelection"]
-  problemTypeName = str(benchmarkProcess.problemType)
-  problemSizeGroupName = "%s_%02u" % (problemTypeName, problemSizeGroupIdx)
-  pushWorkingPath(problemSizeGroupName)
-  ensurePath(os.path.join(globalParameters["WorkingPath"],"Data"))
-
-  totalBenchmarkSteps = len(benchmarkProcess)
-  resultsFileBaseFinal = None
-
-  print1("# NumBenchmarkSteps: {}".format(totalBenchmarkSteps))
-  print1("")
-  print1(HR)
-  print1("# Done Creating BenchmarkProcess Object")
-  print1(HR)
-
-
-  for benchmarkStepIdx in range(0, totalBenchmarkSteps):
-    benchmarkStep = benchmarkProcess[benchmarkStepIdx]
-    stepName = str(benchmarkStep)
-    shortName = benchmarkStep.abbreviation()
-
-    print1("\n")
+    """Temp docs"""
+    benchmarkTestFails = 0
+
+    # convert config to full benchmark process (resolves defaults)
+    print1("")
     print1(HR)
-    currentTime = time.time()
-    elapsedTime = currentTime - startTime
-    print1("# Benchmark Step: {} - {} {:.3f}s".format(problemSizeGroupName, stepName, elapsedTime))
-    print1("# Num Sizes: {}".format(benchmarkStep.problemSizes.totalProblemSizes))
-    print1("# Fork Parameters:")
-    for k, v in benchmarkStep.forkParams.items():
-      print1("#     {}: {}".format(k, v))
-
-    pushWorkingPath(shortName)
-
-    # copy files to benchmark source directory
-    stepBaseDir = globalParameters["WorkingPath"]
-    sourceDir = os.path.join(stepBaseDir, "source" )
-    ensurePath(sourceDir)
-
-    filesToCopy = []
-    pushWorkingPath("source")
-    filesToCopy = [
-        "TensileTypes.h",
-        "tensile_bfloat16.h",
-        "KernelHeader.h",
-        ]
-
-    for f in filesToCopy:
-      shutil.copy(
-          os.path.join(globalParameters["SourcePath"], f),
-          globalParameters["WorkingPath"] )
-    if globalParameters["RuntimeLanguage"] == "OCL":
-      shutil.copy(
-          os.path.join(globalParameters["SourcePath"], "FindOpenCL.cmake"),
-          globalParameters["WorkingPath"] )
-    else:
-      shutil.copy(
-          os.path.join(globalParameters["SourcePath"], "FindHIP.cmake"),
-          globalParameters["WorkingPath"] )
-
-    # enumerate benchmark permutations and create resulting solution objects
-    benchmarkPermutations = constructForkPermutations(benchmarkStep.forkParams)
-    maxPossibleSolutions = len(benchmarkPermutations) #* numHardcoded
-    solutions = generateForkedSolutions(benchmarkProcess.problemType, \
-        benchmarkStep.constantParams, benchmarkPermutations)
-
-    # add custom kernels to list of solutions
-    customKernelList = problemSizeGroupConfig.get("CustomKernels", [])
-    customKernelWildcard = False
-    if customKernelList == ["*"]:
-      customKernelList = \
-          [fname[:-2] for fname in os.listdir(globalParameters["CustomKernelDirectory"]) \
-          if fname.endswith(".s")]
-      customKernelWildcard = True
-
-    for kernelName in customKernelList:
-      print1("# Processing custom kernel {}".format(kernelName))
-      customSolution = generateCustomKernelSolution(kernelName)
-      if customSolution["ProblemType"] != benchmarkProcess.problemType:
-        # Raise error if this kernel was specifically requested and problem type doesn't match
-        if not customKernelWildcard:
-          missingParams = [p for p in benchmarkProcess.problemType \
-              if p not in customSolution["ProblemType"]]
-          extraParams   = [p for p in customSolution["ProblemType"] \
-              if p not in benchmarkProcess.problemType]
-
-          msg  = "The problem type in the config file does not match" \
-                 "that of the custom kernel, {0}.".format(kernelName)
-          msg += "\nMissing config parameters:\n" + str(missingParams)
-          msg += "\nExtra custom kernel parameters:\n" + str(extraParams)
-          raise RuntimeError(msg)
+    print1("# Converting Config to BenchmarkProcess Object")
+    print1(HR)
+    print1("")
+    benchmarkProcess = BenchmarkProcess(problemTypeConfig, problemSizeGroupConfig)
+
+    enableTileSelection = benchmarkProcess.problemType["TileAwareSelection"]
+    problemTypeName = str(benchmarkProcess.problemType)
+    problemSizeGroupName = "%s_%02u" % (problemTypeName, problemSizeGroupIdx)
+    pushWorkingPath(problemSizeGroupName)
+    ensurePath(os.path.join(globalParameters["WorkingPath"],"Data"))
+
+    totalBenchmarkSteps = len(benchmarkProcess)
+    resultsFileBaseFinal = None
+
+    print1("# NumBenchmarkSteps: {}".format(totalBenchmarkSteps))
+    print1("")
+    print1(HR)
+    print1("# Done Creating BenchmarkProcess Object")
+    print1(HR)
+
+
+    for benchmarkStepIdx in range(0, totalBenchmarkSteps):
+        benchmarkStep = benchmarkProcess[benchmarkStepIdx]
+        stepName = str(benchmarkStep)
+        shortName = benchmarkStep.abbreviation()
+
+        print1("\n")
+        print1(HR)
+        currentTime = time.time()
+        elapsedTime = currentTime - startTime
+        print1("# Benchmark Step: {} - {} {:.3f}s".format(problemSizeGroupName, stepName, elapsedTime))
+        print1("# Num Sizes: {}".format(benchmarkStep.problemSizes.totalProblemSizes))
+        print1("# Fork Parameters:")
+        for k, v in benchmarkStep.forkParams.items():
+            print1("#     {}: {}".format(k, v))
+
+        pushWorkingPath(shortName)
+
+        # copy files to benchmark source directory
+        stepBaseDir = globalParameters["WorkingPath"]
+        sourceDir = os.path.join(stepBaseDir, "source" )
+        ensurePath(sourceDir)
+
+        filesToCopy = []
+        pushWorkingPath("source")
+        filesToCopy = [
+                "TensileTypes.h",
+                "tensile_bfloat16.h",
+                "KernelHeader.h",
+                ]
+
+        for f in filesToCopy:
+            shutil.copy(
+                    os.path.join(globalParameters["SourcePath"], f),
+                    globalParameters["WorkingPath"] )
+        if globalParameters["RuntimeLanguage"] == "OCL":
+            shutil.copy(
+                    os.path.join(globalParameters["SourcePath"], "FindOpenCL.cmake"),
+                    globalParameters["WorkingPath"] )
         else:
-          print1("# Rejected {}: Problem Type doesn't match".format(kernelName))
-      else:
-        print1("# Added {} to solutions".format(kernelName))
-        maxPossibleSolutions += 1
-        if customSolution["Valid"]:
-          solutions.append(customSolution)
-        elif globalParameters["PrintSolutionRejectionReason"]:
-          print1("rejecting solution " + str(customSolution))
+            shutil.copy(
+                    os.path.join(globalParameters["SourcePath"], "FindHIP.cmake"),
+                    globalParameters["WorkingPath"] )
+
+        # enumerate benchmark permutations and create resulting solution objects
+        benchmarkPermutations = constructForkPermutations(benchmarkStep.forkParams)
+        maxPossibleSolutions = len(benchmarkPermutations) #* numHardcoded
+        solutions = generateForkedSolutions(benchmarkProcess.problemType, \
+                benchmarkStep.constantParams, benchmarkPermutations)
+
+        # add custom kernels to list of solutions
+        customKernelList = problemSizeGroupConfig.get("CustomKernels", [])
+        customKernelWildcard = False
+        if customKernelList == ["*"]:
+            customKernelList = \
+                    [fname[:-2] for fname in os.listdir(globalParameters["CustomKernelDirectory"]) \
+                    if fname.endswith(".s")]
+            customKernelWildcard = True
+
+        for kernelName in customKernelList:
+            print1("# Processing custom kernel {}".format(kernelName))
+            customSolution = generateCustomKernelSolution(kernelName)
+            if customSolution["ProblemType"] != benchmarkProcess.problemType:
+                # Raise error if this kernel was specifically requested and problem type doesn't match
+                if not customKernelWildcard:
+                    missingParams = [p for p in benchmarkProcess.problemType \
+                            if p not in customSolution["ProblemType"]]
+                    extraParams   = [p for p in customSolution["ProblemType"] \
+                            if p not in benchmarkProcess.problemType]
+
+                    msg  = "The problem type in the config file does not match" \
+                                 "that of the custom kernel, {0}.".format(kernelName)
+                    msg += "\nMissing config parameters:\n" + str(missingParams)
+                    msg += "\nExtra custom kernel parameters:\n" + str(extraParams)
+                    raise RuntimeError(msg)
+                else:
+                    print1("# Rejected {}: Problem Type doesn't match".format(kernelName))
+            else:
+                print1("# Added {} to solutions".format(kernelName))
+                maxPossibleSolutions += 1
+                if customSolution["Valid"]:
+                    solutions.append(customSolution)
+                elif globalParameters["PrintSolutionRejectionReason"]:
+                    print1("rejecting solution " + str(customSolution))
+
+        print1("# Actual Solutions: {} / {} after SolutionStructs\n" \
+                .format(len(solutions), maxPossibleSolutions))
+
+        # handle no valid solutions
+        if len(solutions) == 0:
+                msg = "Your parameters resulted in 0 valid solutions."
+                if globalParameters["PrintSolutionRejectionReason"]:
+                        msg += "\nExamine reject and backtrace messages above to see why" \
+                                "and where solutions were rejected."
+                else:
+                        msg += "\nYou should re-run with \"PrintSolutionRejectionReason: True\"" \
+                                "to see why each parameter combination was rejected."
+                printExit(msg)
+
+        if globalParameters["PrintLevel"] >= 1:
+            for solution in solutions:
+                print2("#    (%u:%u) %s" % (0, 0, Solution.getNameFull(solution) ))
+            print2(HR)
+
+        # write benchmarkFiles
+        prevCount = len(solutions)
+        writeBenchmarkFiles(stepBaseDir, solutions, benchmarkStep.problemSizes, \
+                shortName, filesToCopy, benchmarkProcess.solutionSummationSizes)
+        # ^ this mutates solutions
+
+        print1("# Actual Solutions: %u / %u after KernelWriter\n" \
+                    % (len(solutions), prevCount ))
+
+        popWorkingPath() # source
+
+        # run benchmarking client
+        resultsFileBase = os.path.normpath(os.path.join( \
+                globalParameters["WorkingPath"], "../Data", shortName))
+        if benchmarkStep.isFinal():
+            resultsFileBaseFinal = resultsFileBase
+        resultsFileName = resultsFileBase + ".csv"
+        solutionsFileName = resultsFileBase + ".yaml"
 
-    print1("# Actual Solutions: {} / {} after SolutionStructs\n" \
-        .format(len(solutions), maxPossibleSolutions))
+        if not os.path.exists(resultsFileName) or globalParameters["ForceRedoBenchmarkProblems"]:
+            libraryLogicPath = None
+            forBenchmark = True
+            returncode = runClient(libraryLogicPath, forBenchmark, enableTileSelection)
 
-    # handle no valid solutions
-    if len(solutions) == 0:
-        msg = "Your parameters resulted in 0 valid solutions."
-        if globalParameters["PrintSolutionRejectionReason"]:
-            msg += "\nExamine reject and backtrace messages above to see why" \
-                "and where solutions were rejected."
+            if returncode:
+                benchmarkTestFails += 1
+                printWarning("BenchmarkProblems: Benchmark Process exited with code %u" % returncode)
         else:
-            msg += "\nYou should re-run with \"PrintSolutionRejectionReason: True\"" \
-                "to see why each parameter combination was rejected."
-        printExit(msg)
-
-    if globalParameters["PrintLevel"] >= 1:
-      for solution in solutions:
-        print2("#    (%u:%u) %s" % (0, 0, Solution.getNameFull(solution) ))
-      print2(HR)
-
-    # write benchmarkFiles
-    prevCount = len(solutions)
-    writeBenchmarkFiles(stepBaseDir, solutions, benchmarkStep.problemSizes, \
-        shortName, filesToCopy, benchmarkProcess.solutionSummationSizes)
-    # ^ this mutates solutions
-
-    print1("# Actual Solutions: %u / %u after KernelWriter\n" \
-          % (len(solutions), prevCount ))
-
-    popWorkingPath() # source
-
-    # run benchmarking client
-    resultsFileBase = os.path.normpath(os.path.join( \
-        globalParameters["WorkingPath"], "../Data", shortName))
-    if benchmarkStep.isFinal():
-      resultsFileBaseFinal = resultsFileBase
-    resultsFileName = resultsFileBase + ".csv"
-    solutionsFileName = resultsFileBase + ".yaml"
-
-    if not os.path.exists(resultsFileName) or globalParameters["ForceRedoBenchmarkProblems"]:
-      libraryLogicPath = None
-      forBenchmark = True
-      returncode = runClient(libraryLogicPath, forBenchmark, enableTileSelection)
-
-      if returncode:
-        benchmarkTestFails += 1
-        printWarning("BenchmarkProblems: Benchmark Process exited with code %u" % returncode)
-    else:
-      print1("# Already benchmarked; skipping.")
+            print1("# Already benchmarked; skipping.")
 
-    # write solutions YAML
-    LibraryIO.writeSolutions(solutionsFileName, benchmarkStep.problemSizes, solutions)
+        # write solutions YAML
+        LibraryIO.writeSolutions(solutionsFileName, benchmarkStep.problemSizes, solutions)
 
-    # End Iteration
-    popWorkingPath() # stepName
-    currentTime = time.time()
-    elapsedTime = currentTime - startTime
-    print1("%s\n# %s\n# %s: End - %.3fs\n%s\n" \
-        % (HR, problemSizeGroupName, shortName, elapsedTime, HR))
+        # End Iteration
+        popWorkingPath() # stepName
+        currentTime = time.time()
+        elapsedTime = currentTime - startTime
+        print1("%s\n# %s\n# %s: End - %.3fs\n%s\n" \
+                % (HR, problemSizeGroupName, shortName, elapsedTime, HR))
 
-  popWorkingPath() # ProblemType
-  return (resultsFileBaseFinal, benchmarkTestFails)
+    popWorkingPath() # ProblemType
+    return (resultsFileBaseFinal, benchmarkTestFails)
 # End benchmarkProblemType()
 
 def compareResults(old, new, name):
     """Temp doc"""
     import math
     if name == " WinnerIdx":
-      return 0
+        return 0
 
     try:
-        old = float(old)
+            old = float(old)
     except (ValueError, TypeError):
-        old = -1
+            old = -1
 
     try:
-        new = float(new)
+            new = float(new)
     except (ValueError, TypeError):
-        new = -1
+            new = -1
 
     def isbad(x):
-        return x <= 0 or math.isnan(x) or math.isinf(x)
+            return x <= 0 or math.isnan(x) or math.isinf(x)
 
     if isbad(old) and isbad(new):
-        return 0
+            return 0
     if isbad(old):
-        return 1
+            return 1
     if isbad(new):
-        raise ValueError("Old is good ({}) and new is bad ({}). Name: {}".format(old, new, name))
+            raise ValueError("Old is good ({}) and new is bad ({}). Name: {}".format(old, new, name))
 
     return abs((old-new)/old)
 
 def getResults(resultsFileName, solutions, enableTileSelection, newResultsFileName=None):
-  """Temp docs"""
-  print1("# Get Results from CSV")
-  try:
-    resultsFile = open(resultsFileName, "r")
-  except IOError:
-    printExit("Can't open \"%s\" to get results" % resultsFileName )
-
-  newCSV = itertools.repeat(None)
-  if newResultsFileName is not None:
-    newFile = open(newResultsFileName, 'r')
-    newCSV = csv.reader(newFile)
-
-    diffFile = open(newResultsFileName+'-diff.csv', 'w')
-    diffCSV = csv.writer(diffFile)
-
-  # setup data structures
-  results = []
-  numSolutions = 0
-  for solutionsForHardcoded in solutions:
-    results.append([])
-    for solution in solutionsForHardcoded:
-      numColForProblemSize = solution["ProblemType"]["TotalIndices"]
-      results[-1].append([])
-      numSolutions += 1
-
-  # read results in gflops
-  csvFile = csv.reader(resultsFile)
-
-  if globalParameters["CSVExportWinner"]:
-    # in both old/new clients, csv files always output "GFlops" ,...., "LDD" "LDC" "LDA" "LDB" "TotalFlops" "WinnerGFlops" "WinnerTimeUS" "WinnerIdx" "WinnerName" columns
-    startIdx = numColForProblemSize + 10
-  else:
-    # in both old/new clients, csv files always output "GFlops" ,...., "LDD" "LDC" "LDA" "LDB"columns
-    # old client, non-GEMM csv files don't contain "LDD" "LDC" "LDA" "LDB", so we output an "N/A" text (in csv only) for alignment purpose (-diff.csv)
-    startIdx = numColForProblemSize + 5
-
-  rowLength = startIdx + numSolutions
-
-  rowIdx = 0
-  for row,newRow in zip(csvFile, newCSV):
-    rowIdx+=1
-    if rowIdx == 1:
-      if newRow is not None:
-        diffCSV.writerow(row)
-        diffCSV.writerow(newRow)
-        headerRow = row
-      continue
+    """Temp docs"""
+    print1("# Get Results from CSV")
+    try:
+        resultsFile = open(resultsFileName, "r")
+    except IOError:
+        printExit("Can't open \"%s\" to get results" % resultsFileName )
+
+    newCSV = itertools.repeat(None)
+    if newResultsFileName is not None:
+        newFile = open(newResultsFileName, 'r')
+        newCSV = csv.reader(newFile)
+
+        diffFile = open(newResultsFileName+'-diff.csv', 'w')
+        diffCSV = csv.writer(diffFile)
+
+    # setup data structures
+    results = []
+    numSolutions = 0
+    for solutionsForHardcoded in solutions:
+        results.append([])
+        for solution in solutionsForHardcoded:
+            numColForProblemSize = solution["ProblemType"]["TotalIndices"]
+            results[-1].append([])
+            numSolutions += 1
+
+    # read results in gflops
+    csvFile = csv.reader(resultsFile)
+
+    if globalParameters["CSVExportWinner"]:
+        # in both old/new clients, csv files always output "GFlops" ,...., "LDD" "LDC" "LDA" "LDB" "TotalFlops" "WinnerGFlops" "WinnerTimeUS" "WinnerIdx" "WinnerName" columns
+        startIdx = numColForProblemSize + 10
     else:
-      if len(row) < rowLength:
-        printWarning("CSV File %s row %u doesn't have %u elements; ignoring remainer of file." \
-            % (resultsFileName, rowIdx, rowLength) )
-        break
-      if newRow is not None:
-        diffCSV.writerow([compareResults(old,new,name) for old,new,name in itertools.zip_longest(row, newRow, headerRow)])
-
-      idx = startIdx
-      for i,solutionsForHardcoded in enumerate(solutions):
-        for j,solution in enumerate(solutionsForHardcoded):
-          gflops = float(row[idx])
-
-          results[i][j].append(gflops)
-          idx += 1
-  if rowIdx < 2 and not enableTileSelection:
-    printExit("CSV File %s only has %u row(s); prior benchmark must not have run long enough to produce data." \
-        % (resultsFileName, rowIdx) )
-
-  resultsFile.close()
-  if newResultsFileName is not None:
-    newFile.close()
-    diffFile.close()
-  return results
+        # in both old/new clients, csv files always output "GFlops" ,...., "LDD" "LDC" "LDA" "LDB"columns
+        # old client, non-GEMM csv files don't contain "LDD" "LDC" "LDA" "LDB", so we output an "N/A" text (in csv only) for alignment purpose (-diff.csv)
+        startIdx = numColForProblemSize + 5
+
+    rowLength = startIdx + numSolutions
+
+    rowIdx = 0
+    for row,newRow in zip(csvFile, newCSV):
+        rowIdx+=1
+        if rowIdx == 1:
+            if newRow is not None:
+                diffCSV.writerow(row)
+                diffCSV.writerow(newRow)
+                headerRow = row
+            continue
+        else:
+            if len(row) < rowLength:
+                printWarning("CSV File %s row %u doesn't have %u elements; ignoring remainer of file." \
+                        % (resultsFileName, rowIdx, rowLength) )
+                break
+            if newRow is not None:
+                diffCSV.writerow([compareResults(old,new,name) for old,new,name in itertools.zip_longest(row, newRow, headerRow)])
+
+            idx = startIdx
+            for i,solutionsForHardcoded in enumerate(solutions):
+                for j,solution in enumerate(solutionsForHardcoded):
+                    gflops = float(row[idx])
+
+                    results[i][j].append(gflops)
+                    idx += 1
+    if rowIdx < 2 and not enableTileSelection:
+        printExit("CSV File %s only has %u row(s); prior benchmark must not have run long enough to produce data." \
+                % (resultsFileName, rowIdx) )
+
+    resultsFile.close()
+    if newResultsFileName is not None:
+        newFile.close()
+        diffFile.close()
+    return results
 
 def writeBenchmarkFiles(stepBaseDir, solutions, problemSizes, \
     stepName, filesToCopy, solutionSummationSizes):
@@ -440,84 +440,84 @@ def writeBenchmarkFiles(stepBaseDir, solutions, problemSizes, \
     idealProblemSizes = ProblemSizes(problemType, idealSizes)
     writeClientConfig(True, solutions, idealProblemSizes, stepName, stepBaseDir, newLibrary, codeObjectFiles, True)
 
-  if len(solutions) == 0:
-    printExit("write solutions and kernels results 0 valid soultion.")
+    if len(solutions) == 0:
+        printExit("write solutions and kernels results 0 valid soultion.")
 
-  # write CMake
-  outputPath = globalParameters["WorkingPath"]
+    # write CMake
+    outputPath = globalParameters["WorkingPath"]
 
-  (solutionFiles,
-   sourceKernelFiles,
-   asmKernelFiles,
-   sourceLibFiles,
-   asmLibFiles) = buildObjectFileNames(solutionWriter, kernelWriterSource, \
-    kernelWriterAssembly, solutions, kernels, kernelHelperOjbs)
+    (solutionFiles,
+     sourceKernelFiles,
+     asmKernelFiles,
+     sourceLibFiles,
+     asmLibFiles) = buildObjectFileNames(solutionWriter, kernelWriterSource, \
+        kernelWriterAssembly, solutions, kernels, kernelHelperOjbs)
 
-  writeCMake(outputPath, solutionFiles, sourceKernelFiles, filesToCopy)
+    writeCMake(outputPath, solutionFiles, sourceKernelFiles, filesToCopy)
 
-  for fileName in filesToCopy:
-    shutil.copy( os.path.join(globalParameters["SourcePath"], fileName), \
-      outputPath )
+    for fileName in filesToCopy:
+        shutil.copy( os.path.join(globalParameters["SourcePath"], fileName), \
+            outputPath )
 
 
 def main(config):
-  """Entry point for the "BenchmarkProblems" section of a Tensile config yaml"""
-  ClientExecutable.getClientExecutable()
-
-  dataPath = os.path.join(globalParameters["WorkingPath"], globalParameters["BenchmarkDataPath"])
-  pushWorkingPath(globalParameters["BenchmarkProblemsPath"])
-  ensurePath(dataPath)
-
-  totalTestFails = 0
-  for benchmarkProblemTypeConfig in config:
-    problemTypeConfig = benchmarkProblemTypeConfig[0]
-    if len(benchmarkProblemTypeConfig) < 2:
-      problemSizeGroupConfigs = [{}]
-    else:
-      problemSizeGroupConfigs = benchmarkProblemTypeConfig[1:]
-
-    for problemSizeGroupIdx, problemSizeGroupConfig in enumerate(problemSizeGroupConfigs):
-      print2("ProblemTypeConfig: {}".format(problemTypeConfig))
-      problemTypeObj = ProblemType(problemTypeConfig)
-      globalParameters["EnableHalf"] = problemTypeObj["DataType"].isHalf()
-
-      # using a suffix to check the csv version (for later addFromCSV())
-      csvSuffix = "_CSVWinner" if globalParameters["CSVExportWinner"] else ""
-      # results files will be named
-      newResultsFileName = os.path.join(dataPath, "{}_{:02d}{}.csv" \
-          .format(str(problemTypeObj), problemSizeGroupIdx, csvSuffix) )
-      newSolutionsFileName = os.path.join(dataPath, "{}_{:02d}{}.yaml" \
-          .format(str(problemTypeObj), problemSizeGroupIdx, csvSuffix) )
-      newGranularityFileName = os.path.join(dataPath, "{}_{:02d}{}.gsp" \
-          .format(str(problemTypeObj), problemSizeGroupIdx, csvSuffix) )
-
-      # skip if possible
-      if globalParameters["ForceRedoBenchmarkProblems"] \
-          or not os.path.exists(newResultsFileName):
-
-        # benchmark problem size group
-        (resultsFileBaseFinal, benchmarkErrors) = \
-            benchmarkProblemType(problemTypeConfig, problemSizeGroupConfig, problemSizeGroupIdx)
-        totalTestFails += benchmarkErrors
-
-        print("clientExit={} {} for {}" \
-            .format(totalTestFails, "(ERROR)" if totalTestFails else "(PASS)", \
-            globalParameters["ConfigPath"]) )
-
-        # copy data
-        resultsFileBase = resultsFileBaseFinal
-        resultsFileName = resultsFileBase + ".csv"
-        solutionsFileName = resultsFileBase + ".yaml"
-        granularityFileName = resultsFileBase + "_Granularity.csv"
-        shutil.copy( resultsFileName, newResultsFileName )
-        shutil.copy( solutionsFileName, newSolutionsFileName )
-        if os.path.isfile(granularityFileName):
-          shutil.copy( granularityFileName, newGranularityFileName )
-      else:
-        print1("# {}_{:02d} already benchmarked; skipping." \
-            .format(str(problemTypeObj), problemSizeGroupIdx) )
-
-  popWorkingPath()
-
-  if globalParameters["ExitOnFails"] and totalTestFails:
-    sys.exit(1)
+    """Entry point for the "BenchmarkProblems" section of a Tensile config yaml"""
+    ClientExecutable.getClientExecutable()
+
+    dataPath = os.path.join(globalParameters["WorkingPath"], globalParameters["BenchmarkDataPath"])
+    pushWorkingPath(globalParameters["BenchmarkProblemsPath"])
+    ensurePath(dataPath)
+
+    totalTestFails = 0
+    for benchmarkProblemTypeConfig in config:
+        problemTypeConfig = benchmarkProblemTypeConfig[0]
+        if len(benchmarkProblemTypeConfig) < 2:
+            problemSizeGroupConfigs = [{}]
+        else:
+            problemSizeGroupConfigs = benchmarkProblemTypeConfig[1:]
+
+        for problemSizeGroupIdx, problemSizeGroupConfig in enumerate(problemSizeGroupConfigs):
+            print2("ProblemTypeConfig: {}".format(problemTypeConfig))
+            problemTypeObj = ProblemType(problemTypeConfig)
+            globalParameters["EnableHalf"] = problemTypeObj["DataType"].isHalf()
+
+            # using a suffix to check the csv version (for later addFromCSV())
+            csvSuffix = "_CSVWinner" if globalParameters["CSVExportWinner"] else ""
+            # results files will be named
+            newResultsFileName = os.path.join(dataPath, "{}_{:02d}{}.csv" \
+                    .format(str(problemTypeObj), problemSizeGroupIdx, csvSuffix) )
+            newSolutionsFileName = os.path.join(dataPath, "{}_{:02d}{}.yaml" \
+                    .format(str(problemTypeObj), problemSizeGroupIdx, csvSuffix) )
+            newGranularityFileName = os.path.join(dataPath, "{}_{:02d}{}.gsp" \
+                    .format(str(problemTypeObj), problemSizeGroupIdx, csvSuffix) )
+
+            # skip if possible
+            if globalParameters["ForceRedoBenchmarkProblems"] \
+                    or not os.path.exists(newResultsFileName):
+
+                # benchmark problem size group
+                (resultsFileBaseFinal, benchmarkErrors) = \
+                        benchmarkProblemType(problemTypeConfig, problemSizeGroupConfig, problemSizeGroupIdx)
+                totalTestFails += benchmarkErrors
+
+                print("clientExit={} {} for {}" \
+                        .format(totalTestFails, "(ERROR)" if totalTestFails else "(PASS)", \
+                        globalParameters["ConfigPath"]) )
+
+                # copy data
+                resultsFileBase = resultsFileBaseFinal
+                resultsFileName = resultsFileBase + ".csv"
+                solutionsFileName = resultsFileBase + ".yaml"
+                granularityFileName = resultsFileBase + "_Granularity.csv"
+                shutil.copy( resultsFileName, newResultsFileName )
+                shutil.copy( solutionsFileName, newSolutionsFileName )
+                if os.path.isfile(granularityFileName):
+                    shutil.copy( granularityFileName, newGranularityFileName )
+            else:
+                print1("# {}_{:02d} already benchmarked; skipping." \
+                        .format(str(problemTypeObj), problemSizeGroupIdx) )
+
+    popWorkingPath()
+
+    if globalParameters["ExitOnFails"] and totalTestFails:
+        sys.exit(1)
diff --git a/Tensile/BenchmarkStructs.py b/Tensile/BenchmarkStructs.py
index ed50610c7..9fd64d620 100644
--- a/Tensile/BenchmarkStructs.py
+++ b/Tensile/BenchmarkStructs.py
@@ -21,247 +21,247 @@
 
 from copy import copy, deepcopy
 from .Common import print1, print2, printWarning, defaultSolution, \
-    defaultBenchmarkFinalProblemSizes, defaultBatchedBenchmarkFinalProblemSizes, \
-    defaultBenchmarkCommonParameters, hasParam, printExit, \
-    validParameters, globalParameters
+        defaultBenchmarkFinalProblemSizes, defaultBatchedBenchmarkFinalProblemSizes, \
+        defaultBenchmarkCommonParameters, hasParam, printExit, \
+        validParameters, globalParameters
 from .SolutionStructs import Solution, ProblemType, ProblemSizes
 
 
 def getDefaultsForMissingParameters(parameterConfigurationList, defaultParameters):
-  """Temp doc"""
-  benchmarkParameters = []
-  for paramDict in defaultParameters:
-    for paramName in paramDict:
-      if not hasParam(paramName, parameterConfigurationList) \
-          or paramName == "ProblemSizes":
-        benchmarkParameters.append(paramDict)
-  return benchmarkParameters
+    """Temp doc"""
+    benchmarkParameters = []
+    for paramDict in defaultParameters:
+        for paramName in paramDict:
+            if not hasParam(paramName, parameterConfigurationList) \
+                    or paramName == "ProblemSizes":
+                benchmarkParameters.append(paramDict)
+    return benchmarkParameters
 
 def checkForValidParameters(params, validParameterNames):
-  """Temp doc"""
-  for paramName in params:
-    if paramName in ["ProblemSizes"]:
-      continue
-    else:
-      if paramName not in validParameterNames:
-        raise RuntimeError("Invalid parameter name: %s\nValid parameters are %s." \
-            % (paramName, sorted(validParameterNames)))
-      paramValues = params[paramName]
-      for paramValue in paramValues:
-        if validParameters[paramName] != -1 and paramValue not in validParameters[paramName]:
-          raise RuntimeError("Invalid parameter value: %s = %s\nValid values for %s are %s%s." \
-                    % (paramName, paramValue, paramName, validParameters[paramName][:32],
-                        " (only first 32 combos printed)\nRefer to Common.py for more info" if len(validParameters[paramName])>32 else ""))
+    """Temp doc"""
+    for paramName in params:
+        if paramName in ["ProblemSizes"]:
+            continue
+        else:
+            if paramName not in validParameterNames:
+                raise RuntimeError("Invalid parameter name: %s\nValid parameters are %s." \
+                        % (paramName, sorted(validParameterNames)))
+            paramValues = params[paramName]
+            for paramValue in paramValues:
+                if validParameters[paramName] != -1 and paramValue not in validParameters[paramName]:
+                    raise RuntimeError("Invalid parameter value: %s = %s\nValid values for %s are %s%s." \
+                                        % (paramName, paramValue, paramName, validParameters[paramName][:32],
+                                                " (only first 32 combos printed)\nRefer to Common.py for more info" if len(validParameters[paramName])>32 else ""))
 
 def constructForkPermutations(forkParametersConfig):
-  """Temp doc"""
-  totalPermutations = 1
-  for k, v in forkParametersConfig.items():
-    totalPermutations *= len(v)
-  forkPermutations = []
-  for i in range(0, totalPermutations):
-    forkPermutations.append({})
-    pIdx = i
+    """Temp doc"""
+    totalPermutations = 1
     for k, v in forkParametersConfig.items():
-      values = deepcopy(v)
-      valueIdx = pIdx % len(v)
-      forkPermutations[i][k] = values[valueIdx]
-      pIdx //= len(values)
-  return forkPermutations
+        totalPermutations *= len(v)
+    forkPermutations = []
+    for i in range(0, totalPermutations):
+        forkPermutations.append({})
+        pIdx = i
+        for k, v in forkParametersConfig.items():
+            values = deepcopy(v)
+            valueIdx = pIdx % len(v)
+            forkPermutations[i][k] = values[valueIdx]
+            pIdx //= len(values)
+    return forkPermutations
 
 def getSingleValues(parameterSetList):
-  """Temp doc"""
-  singleVaules = {}
-  for stepList in parameterSetList:
-    for paramDict in copy(stepList):
-      for paramName in copy(paramDict):
-        paramValues = paramDict[paramName]
-        if paramValues == None:
-          printExit("You must specify value for parameters \"%s\"" % paramName )
-        if len(paramValues) < 2 and paramName != "ProblemSizes":
-          paramDict.pop(paramName)
-          singleVaules[paramName] = paramValues[0]
-          if len(paramDict) == 0:
-            stepList.remove(paramDict)
-
-  return singleVaules
+    """Temp doc"""
+    singleVaules = {}
+    for stepList in parameterSetList:
+        for paramDict in copy(stepList):
+            for paramName in copy(paramDict):
+                paramValues = paramDict[paramName]
+                if paramValues == None:
+                    printExit("You must specify value for parameters \"%s\"" % paramName )
+                if len(paramValues) < 2 and paramName != "ProblemSizes":
+                    paramDict.pop(paramName)
+                    singleVaules[paramName] = paramValues[0]
+                    if len(paramDict) == 0:
+                        stepList.remove(paramDict)
+
+    return singleVaules
 
 def checkCDBufferAndStrides(problemType, problemSizes, isCEqualD):
-  """Temp doc"""
-  if isCEqualD and problemType["OperationType"] == "GEMM":
-    for problem in problemSizes.problems:
-      ldd = problem.sizes[problemType["IndexAssignmentsLD"][0]]
-      ldc = problem.sizes[problemType["IndexAssignmentsLD"][1]]
-      if ldd != ldc:
-        printExit("LDD(%d) != LDC(%d) causes unpredictable result when CEqualD(True)" % (ldd, ldc))
+    """Temp doc"""
+    if isCEqualD and problemType["OperationType"] == "GEMM":
+        for problem in problemSizes.problems:
+            ldd = problem.sizes[problemType["IndexAssignmentsLD"][0]]
+            ldc = problem.sizes[problemType["IndexAssignmentsLD"][1]]
+            if ldd != ldc:
+                printExit("LDD(%d) != LDC(%d) causes unpredictable result when CEqualD(True)" % (ldd, ldc))
 
 
 class BenchmarkProcess:
-  """
-  Steps in config need to be expanded and missing elements need to be assigned a default.
-  TODO better docs
-  """
-
-  def __init__(self, problemTypeConfig, problemSizeGroupConfig):
-    """Temp doc"""
-    self.problemType = ProblemType(problemTypeConfig)
-    self.isBatched = "Batched" in problemTypeConfig and problemTypeConfig["Batched"]
-    print2("# BenchmarkProcess beginning %s" % str(self.problemType))
-
-    # create initial solution parameters
-    self.initialSolutionParameters = { "ProblemType": problemTypeConfig }
-    self.initialSolutionParameters.update(defaultSolution)
-
-    # fill parameter values from config
-    self.singleValueParameters = {}
-    self.multiValueParameters = {}
-    self.getConfigParameter(self.isBatched, problemSizeGroupConfig)
-
-    # convert parameter lists to steps
-    # currently only 1 benchmark step is possible, more may be added back later
-    self.currentProblemSizes = []
-    self.benchmarkStepIdx = 0
-    self.convertParametersToSteps()
-
-  def getConfigParameter(self, isbatched, config):
-    """Temp doc"""
-    print2("")
-    print2("####################################################################")
-    print1("# Filling in Parameters With Defaults")
-    print2("####################################################################")
-    print2("")
-
-    # check for no longer supported legacy benchmark steps
-    badParams = ["InitialSolutionParameters", "BenchmarkForkParameters", \
-                 "JoinParameters", "BenchmarkJoinParameters"]
-    badsInConfig = []
-
-    for p in badParams:
-      if config.get(p) is not None:
-        badsInConfig.append(p)
-
-    if len(badsInConfig) == 1:
-      printExit("Benchmark step {} is no longer supported".format("'" + badsInConfig[0] + "'"))
-    elif len(badsInConfig) > 1:
-      printExit("Benchmark steps {} are no longer supported".format(badsInConfig))
-
-    # get supported legacy benchmark steps
-    defaultSizes = [{"ProblemSizes": defaultBatchedBenchmarkFinalProblemSizes}] if isbatched \
-        else [{"ProblemSizes": defaultBenchmarkFinalProblemSizes}]
-
-    benchmarkCommonParameters      = config.get("BenchmarkCommonParameters", [])
-    forkParameters                 = config.get("ForkParameters", [])
-    self.benchmarkFinalParameters  = config.get("BenchmarkFinalParameters", defaultSizes)
-
-    configParameters = benchmarkCommonParameters + forkParameters
-
-    # ensure only valid solution parameters were requested
-    validParameterNames = set(validParameters.keys())
-    for paramDict in configParameters:
-      try:
-        checkForValidParameters(paramDict, validParameterNames)
-      except RuntimeError as e:
-        printExit(str(e))
-
-    # get defaults for parameters not specified in config file
-    missingParameters = getDefaultsForMissingParameters( \
-        configParameters, deepcopy(defaultBenchmarkCommonParameters))
-
-    # split parameters into single value and multi-value
-    self.singleValueParameters = getSingleValues([missingParameters, configParameters])
-
-    # above function call removes singles
-    self.multiValueParameters = {}
-    for paramDict in configParameters:
-      for param, values in paramDict.items():
-        self.multiValueParameters[param] = values
-
-    # print summary of parameter values
-    print2("Single Value Parameters:")
-    for k, v in self.singleValueParameters.items():
-      print2("    {}: {}".format(k, v))
-    print2("Multi-Value Parameters:")
-    for k, v in self.multiValueParameters.items():
-      print2("    {}: {}".format(k, v))
-
-  def convertParametersToSteps(self):
-    """Temp doc"""
-    print2("")
-    print2("####################################################################")
-    print1("# Convert Parameters to Benchmark Step(s)")
-    print2("####################################################################")
-    print2("")
-
-    # currently only a single step is supported
-    print2("")
-    print2("####################################################################")
-    print1("# Benchmark Final")
-    for problemSizesDict in self.benchmarkFinalParameters:
-        problemSizes = problemSizesDict["ProblemSizes"]
-        self.currentProblemSizes = ProblemSizes(self.problemType, problemSizes)
-        checkCDBufferAndStrides(self.problemType, \
-            self.currentProblemSizes, globalParameters["CEqualD"])
-        benchmarkStep = BenchmarkStep( \
-            self.multiValueParameters, \
-            self.singleValueParameters, \
-            self.currentProblemSizes, \
-            self.benchmarkStepIdx )
-        self.benchmarkSteps.append(benchmarkStep)
-        self.benchmarkStepIdx+=1
-
-  def __len__(self):
-    return len(self.benchmarkSteps)
-
-  def __getitem__(self, key):
-    return self.benchmarkSteps[key]
-
-  def __str__(self):
-    string = "BenchmarkProcess:\n"
-    for step in self.benchmarkSteps:
-      string += str(step)
-    return string
-
-  def __repr__(self):
-    return self.__str__()
+    """
+    Steps in config need to be expanded and missing elements need to be assigned a default.
+    TODO better docs
+    """
+
+    def __init__(self, problemTypeConfig, problemSizeGroupConfig):
+        """Temp doc"""
+        self.problemType = ProblemType(problemTypeConfig)
+        self.isBatched = "Batched" in problemTypeConfig and problemTypeConfig["Batched"]
+        print2("# BenchmarkProcess beginning %s" % str(self.problemType))
+
+        # create initial solution parameters
+        self.initialSolutionParameters = { "ProblemType": problemTypeConfig }
+        self.initialSolutionParameters.update(defaultSolution)
+
+        # fill parameter values from config
+        self.singleValueParameters = {}
+        self.multiValueParameters = {}
+        self.getConfigParameter(self.isBatched, problemSizeGroupConfig)
+
+        # convert parameter lists to steps
+        # currently only 1 benchmark step is possible, more may be added back later
+        self.currentProblemSizes = []
+        self.benchmarkStepIdx = 0
+        self.convertParametersToSteps()
+
+    def getConfigParameter(self, isbatched, config):
+        """Temp doc"""
+        print2("")
+        print2("####################################################################")
+        print1("# Filling in Parameters With Defaults")
+        print2("####################################################################")
+        print2("")
+
+        # check for no longer supported legacy benchmark steps
+        badParams = ["InitialSolutionParameters", "BenchmarkForkParameters", \
+                                 "JoinParameters", "BenchmarkJoinParameters"]
+        badsInConfig = []
+
+        for p in badParams:
+            if config.get(p) is not None:
+                badsInConfig.append(p)
+
+        if len(badsInConfig) == 1:
+            printExit("Benchmark step {} is no longer supported".format("'" + badsInConfig[0] + "'"))
+        elif len(badsInConfig) > 1:
+            printExit("Benchmark steps {} are no longer supported".format(badsInConfig))
+
+        # get supported legacy benchmark steps
+        defaultSizes = [{"ProblemSizes": defaultBatchedBenchmarkFinalProblemSizes}] if isbatched \
+                else [{"ProblemSizes": defaultBenchmarkFinalProblemSizes}]
+
+        benchmarkCommonParameters      = config.get("BenchmarkCommonParameters", [])
+        forkParameters                 = config.get("ForkParameters", [])
+        self.benchmarkFinalParameters  = config.get("BenchmarkFinalParameters", defaultSizes)
+
+        configParameters = benchmarkCommonParameters + forkParameters
+
+        # ensure only valid solution parameters were requested
+        validParameterNames = set(validParameters.keys())
+        for paramDict in configParameters:
+            try:
+                checkForValidParameters(paramDict, validParameterNames)
+            except RuntimeError as e:
+                printExit(str(e))
+
+        # get defaults for parameters not specified in config file
+        missingParameters = getDefaultsForMissingParameters( \
+                configParameters, deepcopy(defaultBenchmarkCommonParameters))
+
+        # split parameters into single value and multi-value
+        self.singleValueParameters = getSingleValues([missingParameters, configParameters])
+
+        # above function call removes singles
+        self.multiValueParameters = {}
+        for paramDict in configParameters:
+            for param, values in paramDict.items():
+                self.multiValueParameters[param] = values
+
+        # print summary of parameter values
+        print2("Single Value Parameters:")
+        for k, v in self.singleValueParameters.items():
+            print2("    {}: {}".format(k, v))
+        print2("Multi-Value Parameters:")
+        for k, v in self.multiValueParameters.items():
+            print2("    {}: {}".format(k, v))
+
+    def convertParametersToSteps(self):
+        """Temp doc"""
+        print2("")
+        print2("####################################################################")
+        print1("# Convert Parameters to Benchmark Step(s)")
+        print2("####################################################################")
+        print2("")
+
+        # currently only a single step is supported
+        print2("")
+        print2("####################################################################")
+        print1("# Benchmark Final")
+        for problemSizesDict in self.benchmarkFinalParameters:
+                problemSizes = problemSizesDict["ProblemSizes"]
+                self.currentProblemSizes = ProblemSizes(self.problemType, problemSizes)
+                checkCDBufferAndStrides(self.problemType, \
+                        self.currentProblemSizes, globalParameters["CEqualD"])
+                benchmarkStep = BenchmarkStep( \
+                        self.multiValueParameters, \
+                        self.singleValueParameters, \
+                        self.currentProblemSizes, \
+                        self.benchmarkStepIdx )
+                self.benchmarkSteps.append(benchmarkStep)
+                self.benchmarkStepIdx+=1
+
+    def __len__(self):
+        return len(self.benchmarkSteps)
+
+    def __getitem__(self, key):
+        return self.benchmarkSteps[key]
+
+    def __str__(self):
+        string = "BenchmarkProcess:\n"
+        for step in self.benchmarkSteps:
+            string += str(step)
+        return string
+
+    def __repr__(self):
+        return self.__str__()
 
 
 class BenchmarkStep:
-  """Temp doc"""
-
-  def __init__(self, forkParams, constantParams, problemSizes, idx):
     """Temp doc"""
-    #TODO see if deepcopy really needed
-    self.forkParams = deepcopy(forkParams)
-    self.constantParams = deepcopy(constantParams)
-    self.problemSizes = deepcopy(problemSizes)
-    self.stepIdx = idx
-
-    print2("# Creating BenchmarkStep: {} fork params and {} sizes" \
-        .format( len(forkParams), problemSizes.totalProblemSizes))
 
-  def isFinal(self):
-    """Temp doc"""
-    # currently only one benchmark step is possible
-    return True
-
-  def abbreviation(self):
-    """Temp doc"""
-    string = "{:02d}".format(self.stepIdx)
-    if self.isFinal():
-      string += "_Final"
-    else:
-      for param in self.benchmarkParameters:
-        string += "_" + Solution.getParameterNameAbbreviation(param)
-    return string
-
-  def __str__(self):
-    string = "{:02d}".format(self.stepIdx)
-    if self.isFinal():
-      string += "_Final"
-    else:
-      for param in self.benchmarkParameters:
-        string += "_" + str(param)
-    return string
-
-  def __repr__(self):
-    return self.__str__()
+    def __init__(self, forkParams, constantParams, problemSizes, idx):
+        """Temp doc"""
+        #TODO see if deepcopy really needed
+        self.forkParams = deepcopy(forkParams)
+        self.constantParams = deepcopy(constantParams)
+        self.problemSizes = deepcopy(problemSizes)
+        self.stepIdx = idx
+
+        print2("# Creating BenchmarkStep: {} fork params and {} sizes" \
+                .format( len(forkParams), problemSizes.totalProblemSizes))
+
+    def isFinal(self):
+        """Temp doc"""
+        # currently only one benchmark step is possible
+        return True
+
+    def abbreviation(self):
+        """Temp doc"""
+        string = "{:02d}".format(self.stepIdx)
+        if self.isFinal():
+            string += "_Final"
+        else:
+            for param in self.benchmarkParameters:
+                string += "_" + Solution.getParameterNameAbbreviation(param)
+        return string
+
+    def __str__(self):
+        string = "{:02d}".format(self.stepIdx)
+        if self.isFinal():
+            string += "_Final"
+        else:
+            for param in self.benchmarkParameters:
+                string += "_" + str(param)
+        return string
+
+    def __repr__(self):
+        return self.__str__()

From 5fe26c7c04864e47cda181aefe32feeb242b5ce6 Mon Sep 17 00:00:00 2001
From: Benjamin Ulmer <benjamin.ulmer@amd.com>
Date: Thu, 7 Oct 2021 11:28:56 -0600
Subject: [PATCH 06/12] Docs and final refactor/clean of BenchmarkStructs.py

---
 Tensile/BenchmarkProblems.py |  33 ++---
 Tensile/BenchmarkStructs.py  | 257 ++++++++++++++++-------------------
 Tensile/CustomKernels.py     |   7 +-
 3 files changed, 137 insertions(+), 160 deletions(-)

diff --git a/Tensile/BenchmarkProblems.py b/Tensile/BenchmarkProblems.py
index 6d5db00de..1c0f31624 100644
--- a/Tensile/BenchmarkProblems.py
+++ b/Tensile/BenchmarkProblems.py
@@ -32,7 +32,7 @@
 from . import SolutionLibrary
 from . import LibraryIO
 from . import Utils
-from .BenchmarkStructs import BenchmarkProcess, constructForkPermutations, checkForValidParameters
+from .BenchmarkStructs import BenchmarkProcess, checkParametersAreValid, constructForkPermutations
 from .ClientWriter import runClient, writeClientConfig
 from .Common import globalParameters, HR, pushWorkingPath, popWorkingPath, print1, print2, \
         printExit, printWarning, ensurePath, startTime, validParameters
@@ -41,7 +41,7 @@
 from .SolutionStructs import Solution, ProblemType, ProblemSizes
 from .SolutionWriter import SolutionWriter
 from .TensileCreateLibrary import writeSolutionsAndKernels, writeCMake, buildObjectFileNames
-from .CustomKernels import getCustomKernelConfig
+from .CustomKernels import getAllCustomKernelNames, getCustomKernelConfig
 
 
 def generateForkedSolutions (problemType, constantParams, benchmarkPermutations):
@@ -78,7 +78,7 @@ def generateForkedSolutions (problemType, constantParams, benchmarkPermutations)
 def generateCustomKernelSolution(kernelName, directory=globalParameters["CustomKernelDirectory"]):
     """Temp docs"""
     kernelConfig = getCustomKernelConfig(kernelName, directory)
-    checkForValidParameters({p: [kernelConfig[p]] for p in kernelConfig if p != "ProblemType"}, set(validParameters.keys()))
+    checkParametersAreValid({p: [kernelConfig[p]] for p in kernelConfig if p != "ProblemType"}, validParameters)
     # test if problem type matches with configuration file
     kernelConfig["KernelLanguage"] = "Assembly"   # replacement kernels are always assembly kernels?
     kernelConfig["CustomKernelName"] = kernelName
@@ -116,7 +116,7 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, problemSize
     for benchmarkStepIdx in range(0, totalBenchmarkSteps):
         benchmarkStep = benchmarkProcess[benchmarkStepIdx]
         stepName = str(benchmarkStep)
-        shortName = benchmarkStep.abbreviation()
+        shortName = stepName
 
         print1("\n")
         print1(HR)
@@ -157,32 +157,23 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, problemSize
                     globalParameters["WorkingPath"] )
 
         # enumerate benchmark permutations and create resulting solution objects
-        benchmarkPermutations = constructForkPermutations(benchmarkStep.forkParams)
-        maxPossibleSolutions = len(benchmarkPermutations) #* numHardcoded
+        forkPermutations = constructForkPermutations(benchmarkStep.forkParams)
+        maxPossibleSolutions = len(forkPermutations) #* numHardcoded
         solutions = generateForkedSolutions(benchmarkProcess.problemType, \
-                benchmarkStep.constantParams, benchmarkPermutations)
-
-        # add custom kernels to list of solutions
-        customKernelList = problemSizeGroupConfig.get("CustomKernels", [])
-        customKernelWildcard = False
-        if customKernelList == ["*"]:
-            customKernelList = \
-                    [fname[:-2] for fname in os.listdir(globalParameters["CustomKernelDirectory"]) \
-                    if fname.endswith(".s")]
-            customKernelWildcard = True
-
-        for kernelName in customKernelList:
+                benchmarkStep.constantParams, forkPermutations)
+
+        for kernelName in benchmarkStep.customKernels:
             print1("# Processing custom kernel {}".format(kernelName))
             customSolution = generateCustomKernelSolution(kernelName)
             if customSolution["ProblemType"] != benchmarkProcess.problemType:
                 # Raise error if this kernel was specifically requested and problem type doesn't match
-                if not customKernelWildcard:
+                if not benchmarkStep.customKernelWildcard:
                     missingParams = [p for p in benchmarkProcess.problemType \
                             if p not in customSolution["ProblemType"]]
                     extraParams   = [p for p in customSolution["ProblemType"] \
                             if p not in benchmarkProcess.problemType]
 
-                    msg  = "The problem type in the config file does not match" \
+                    msg  = "The problem type in the config file does not match " \
                                  "that of the custom kernel, {0}.".format(kernelName)
                     msg += "\nMissing config parameters:\n" + str(missingParams)
                     msg += "\nExtra custom kernel parameters:\n" + str(extraParams)
@@ -219,7 +210,7 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, problemSize
         # write benchmarkFiles
         prevCount = len(solutions)
         writeBenchmarkFiles(stepBaseDir, solutions, benchmarkStep.problemSizes, \
-                shortName, filesToCopy, benchmarkProcess.solutionSummationSizes)
+                shortName, filesToCopy, [])
         # ^ this mutates solutions
 
         print1("# Actual Solutions: %u / %u after KernelWriter\n" \
diff --git a/Tensile/BenchmarkStructs.py b/Tensile/BenchmarkStructs.py
index 9fd64d620..36e50307a 100644
--- a/Tensile/BenchmarkStructs.py
+++ b/Tensile/BenchmarkStructs.py
@@ -19,112 +19,93 @@
 # CTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 ################################################################################
 
-from copy import copy, deepcopy
-from .Common import print1, print2, printWarning, defaultSolution, \
+from copy import deepcopy
+from .Common import print1, print2, hasParam, printExit, \
         defaultBenchmarkFinalProblemSizes, defaultBatchedBenchmarkFinalProblemSizes, \
-        defaultBenchmarkCommonParameters, hasParam, printExit, \
-        validParameters, globalParameters
+        defaultBenchmarkCommonParameters, validParameters, globalParameters
+from .CustomKernels import getAllCustomKernelNames
 from .SolutionStructs import Solution, ProblemType, ProblemSizes
+from Tensile import CustomKernels
 
 
-def getDefaultsForMissingParameters(parameterConfigurationList, defaultParameters):
-    """Temp doc"""
-    benchmarkParameters = []
-    for paramDict in defaultParameters:
+def getDefaultsForMissingParameters(paramList, defaultParams):
+    """Returns all parameters (with values) in defaultParams not present in paramList"""
+    benchmarkParams = []
+    for paramDict in defaultParams:
         for paramName in paramDict:
-            if not hasParam(paramName, parameterConfigurationList) \
+            if not hasParam(paramName, paramList) \
                     or paramName == "ProblemSizes":
-                benchmarkParameters.append(paramDict)
-    return benchmarkParameters
+                benchmarkParams.append(paramDict)
+    return benchmarkParams
 
-def checkForValidParameters(params, validParameterNames):
-    """Temp doc"""
-    for paramName in params:
-        if paramName in ["ProblemSizes"]:
+def checkParametersAreValid(params, validParams):
+    """Ensures paramaters in params exist and have valid values as specified by validParames"""
+    for name, values in params.items():
+        if name in ["ProblemSizes"]:
             continue
-        else:
-            if paramName not in validParameterNames:
-                raise RuntimeError("Invalid parameter name: %s\nValid parameters are %s." \
-                        % (paramName, sorted(validParameterNames)))
-            paramValues = params[paramName]
-            for paramValue in paramValues:
-                if validParameters[paramName] != -1 and paramValue not in validParameters[paramName]:
-                    raise RuntimeError("Invalid parameter value: %s = %s\nValid values for %s are %s%s." \
-                                        % (paramName, paramValue, paramName, validParameters[paramName][:32],
-                                                " (only first 32 combos printed)\nRefer to Common.py for more info" if len(validParameters[paramName])>32 else ""))
-
-def constructForkPermutations(forkParametersConfig):
-    """Temp doc"""
-    totalPermutations = 1
-    for k, v in forkParametersConfig.items():
-        totalPermutations *= len(v)
-    forkPermutations = []
-    for i in range(0, totalPermutations):
-        forkPermutations.append({})
-        pIdx = i
-        for k, v in forkParametersConfig.items():
-            values = deepcopy(v)
-            valueIdx = pIdx % len(v)
-            forkPermutations[i][k] = values[valueIdx]
-            pIdx //= len(values)
-    return forkPermutations
 
-def getSingleValues(parameterSetList):
-    """Temp doc"""
+        if name not in validParams:
+            printExit("Invalid parameter name: {}\nValid parameters are {}." \
+                    .format(name, sorted(validParameters.keys())))
+
+        for value in values:
+            if validParams[name] != -1 and value not in validParams[name]:
+                msgBase = "Invalid parameter value: {} = {}\nValid values for {} are {}{}."
+                msgExt = " (only first 32 combos printed)\nRefer to Common.py for more info" \
+                        if len(validParams[name])>32 else ""
+                printExit(msgBase.format(name, value, name, validParams[name][:32], msgExt))
+
+def separateParameters(paramSetList):
+    """Separates paramSetList into parameters with single and multiple values"""
     singleVaules = {}
-    for stepList in parameterSetList:
-        for paramDict in copy(stepList):
-            for paramName in copy(paramDict):
-                paramValues = paramDict[paramName]
-                if paramValues == None:
-                    printExit("You must specify value for parameters \"%s\"" % paramName )
-                if len(paramValues) < 2 and paramName != "ProblemSizes":
-                    paramDict.pop(paramName)
-                    singleVaules[paramName] = paramValues[0]
-                    if len(paramDict) == 0:
-                        stepList.remove(paramDict)
-
-    return singleVaules
+    multiValues = {}
+    for paramDict in paramSetList:
+        for name, values in paramDict.items():
+            if values == None:
+                printExit("You must specify value(s) for parameter \"{}\"".format(name))
+            if len(values) == 1 and name != "ProblemSizes":
+                singleVaules[name] = values[0]
+            elif len(values) > 1 and name != "ProblemSizes":
+                multiValues[name] = values
+
+    return singleVaules, multiValues
 
 def checkCDBufferAndStrides(problemType, problemSizes, isCEqualD):
-    """Temp doc"""
+    """Ensures ldd == ldc when CEqualD"""
     if isCEqualD and problemType["OperationType"] == "GEMM":
         for problem in problemSizes.problems:
             ldd = problem.sizes[problemType["IndexAssignmentsLD"][0]]
             ldc = problem.sizes[problemType["IndexAssignmentsLD"][1]]
             if ldd != ldc:
-                printExit("LDD(%d) != LDC(%d) causes unpredictable result when CEqualD(True)" % (ldd, ldc))
+                printExit("LDD({}) != LDC({}) causes unpredictable result when CEqualD(True)" \
+                        .format(ldd, ldc))
 
 
 class BenchmarkProcess:
-    """
-    Steps in config need to be expanded and missing elements need to be assigned a default.
-    TODO better docs
-    """
+    """Representation of benchmarking parameters and resulting steps"""
 
     def __init__(self, problemTypeConfig, problemSizeGroupConfig):
-        """Temp doc"""
+        """Create from the two sections of a config for a BenchmarkProblem"""
         self.problemType = ProblemType(problemTypeConfig)
         self.isBatched = "Batched" in problemTypeConfig and problemTypeConfig["Batched"]
         print2("# BenchmarkProcess beginning %s" % str(self.problemType))
 
-        # create initial solution parameters
-        self.initialSolutionParameters = { "ProblemType": problemTypeConfig }
-        self.initialSolutionParameters.update(defaultSolution)
-
         # fill parameter values from config
-        self.singleValueParameters = {}
-        self.multiValueParameters = {}
-        self.getConfigParameter(self.isBatched, problemSizeGroupConfig)
+        self.singleValueParams = {}
+        self.multiValueParams = {}
+        self.customKernels = []
+        self.sizes = None
+        self.getConfigParameters(self.isBatched, problemSizeGroupConfig)
 
         # convert parameter lists to steps
-        # currently only 1 benchmark step is possible, more may be added back later
-        self.currentProblemSizes = []
+        # previously, multiple benchmark steps were possible
+        # currently only 1 benchmark step is possible; more may be added back later
+        self.benchmarkSteps = []
         self.benchmarkStepIdx = 0
         self.convertParametersToSteps()
 
-    def getConfigParameter(self, isbatched, config):
-        """Temp doc"""
+    def getConfigParameters(self, isbatched, config):
+        """Parse and validate benchmarking parameters in config"""
         print2("")
         print2("####################################################################")
         print1("# Filling in Parameters With Defaults")
@@ -133,7 +114,7 @@ def getConfigParameter(self, isbatched, config):
 
         # check for no longer supported legacy benchmark steps
         badParams = ["InitialSolutionParameters", "BenchmarkForkParameters", \
-                                 "JoinParameters", "BenchmarkJoinParameters"]
+                     "JoinParameters", "BenchmarkJoinParameters"]
         badsInConfig = []
 
         for p in badParams:
@@ -146,46 +127,42 @@ def getConfigParameter(self, isbatched, config):
             printExit("Benchmark steps {} are no longer supported".format(badsInConfig))
 
         # get supported legacy benchmark steps
-        defaultSizes = [{"ProblemSizes": defaultBatchedBenchmarkFinalProblemSizes}] if isbatched \
-                else [{"ProblemSizes": defaultBenchmarkFinalProblemSizes}]
-
-        benchmarkCommonParameters      = config.get("BenchmarkCommonParameters", [])
-        forkParameters                 = config.get("ForkParameters", [])
-        self.benchmarkFinalParameters  = config.get("BenchmarkFinalParameters", defaultSizes)
+        benchmarkCommonParams = config.get("BenchmarkCommonParameters", [])
+        forkParams            = config.get("ForkParameters", [])
+        self.customKernels    = config.get("CustomKernels", [])
 
-        configParameters = benchmarkCommonParameters + forkParameters
+        if "BenchmarkFinalParameters" in config:
+            sizes = config["BenchmarkFinalParameters"][0]["ProblemSizes"]
+        else:
+            sizes = defaultBatchedBenchmarkFinalProblemSizes if isbatched \
+                else defaultBenchmarkFinalProblemSizes
+        self.problemSizes = ProblemSizes(self.problemType, sizes)
+        checkCDBufferAndStrides(self.problemType, \
+                self.problemSizes, globalParameters["CEqualD"])
 
-        # ensure only valid solution parameters were requested
-        validParameterNames = set(validParameters.keys())
-        for paramDict in configParameters:
-            try:
-                checkForValidParameters(paramDict, validParameterNames)
-            except RuntimeError as e:
-                printExit(str(e))
+        configParams = benchmarkCommonParams + forkParams
 
-        # get defaults for parameters not specified in config file
-        missingParameters = getDefaultsForMissingParameters( \
-                configParameters, deepcopy(defaultBenchmarkCommonParameters))
+        # validate and parse raw parameters into more usable forms
+        for paramDict in configParams:
+            checkParametersAreValid(paramDict, validParameters)
 
-        # split parameters into single value and multi-value
-        self.singleValueParameters = getSingleValues([missingParameters, configParameters])
+        missingParams = getDefaultsForMissingParameters( \
+                configParams, deepcopy(defaultBenchmarkCommonParameters))
 
-        # above function call removes singles
-        self.multiValueParameters = {}
-        for paramDict in configParameters:
-            for param, values in paramDict.items():
-                self.multiValueParameters[param] = values
+        self.singleValueParams, self.multiValueParams \
+                = separateParameters(missingParams + configParams)
 
         # print summary of parameter values
         print2("Single Value Parameters:")
-        for k, v in self.singleValueParameters.items():
+        for k, v in self.singleValueParams.items():
             print2("    {}: {}".format(k, v))
+
         print2("Multi-Value Parameters:")
-        for k, v in self.multiValueParameters.items():
+        for k, v in self.multiValueParams.items():
             print2("    {}: {}".format(k, v))
 
     def convertParametersToSteps(self):
-        """Temp doc"""
+        """Create benchmark steps based on parsed parameters"""
         print2("")
         print2("####################################################################")
         print1("# Convert Parameters to Benchmark Step(s)")
@@ -196,18 +173,14 @@ def convertParametersToSteps(self):
         print2("")
         print2("####################################################################")
         print1("# Benchmark Final")
-        for problemSizesDict in self.benchmarkFinalParameters:
-                problemSizes = problemSizesDict["ProblemSizes"]
-                self.currentProblemSizes = ProblemSizes(self.problemType, problemSizes)
-                checkCDBufferAndStrides(self.problemType, \
-                        self.currentProblemSizes, globalParameters["CEqualD"])
-                benchmarkStep = BenchmarkStep( \
-                        self.multiValueParameters, \
-                        self.singleValueParameters, \
-                        self.currentProblemSizes, \
-                        self.benchmarkStepIdx )
-                self.benchmarkSteps.append(benchmarkStep)
-                self.benchmarkStepIdx+=1
+        benchmarkStep = BenchmarkStep( \
+                self.multiValueParams, \
+                self.singleValueParams, \
+                self.customKernels, \
+                self.problemSizes, \
+                self.benchmarkStepIdx)
+        self.benchmarkSteps.append(benchmarkStep)
+        self.benchmarkStepIdx += 1
 
     def __len__(self):
         return len(self.benchmarkSteps)
@@ -225,42 +198,52 @@ def __repr__(self):
         return self.__str__()
 
 
+def constructForkPermutations(forkParams):
+    """Constructs cartesian product of parameter values in forkParams"""
+    totalPermutations = 1
+    for _, values in forkParams.items():
+        totalPermutations *= len(values)
+
+    forkPermutations = []
+    for i in range(0, totalPermutations):
+        permutation = {}
+        pIdx = i
+        for name, v in forkParams.items():
+            values = deepcopy(v)
+            valueIdx = pIdx % len(v)
+            permutation[name] = values[valueIdx]
+            pIdx //= len(values)
+        forkPermutations.append(permutation)
+    return forkPermutations
+
+
 class BenchmarkStep:
-    """Temp doc"""
-
-    def __init__(self, forkParams, constantParams, problemSizes, idx):
-        """Temp doc"""
-        #TODO see if deepcopy really needed
-        self.forkParams = deepcopy(forkParams)
-        self.constantParams = deepcopy(constantParams)
-        self.problemSizes = deepcopy(problemSizes)
+    """A single benchmark step which consists of constant and fork parameters and a set of sizes"""
+
+    def __init__(self, forkParams, constantParams, customKernels, problemSizes, idx):
+        """Basic constructor storing each argument"""
+        self.forkParams = forkParams
+        self.constantParams = constantParams
+        self.customKernels = customKernels
+        self.problemSizes = problemSizes
         self.stepIdx = idx
 
+        self.customKernelWildcard = False
+        if self.customKernels == ["*"]:
+            self.customKernels = getAllCustomKernelNames()
+            self.customKernelWildcard = True
+
         print2("# Creating BenchmarkStep: {} fork params and {} sizes" \
                 .format( len(forkParams), problemSizes.totalProblemSizes))
 
     def isFinal(self):
-        """Temp doc"""
-        # currently only one benchmark step is possible
+        """Legacy. Currently always returns true since only one benchmark step is possible"""
         return True
 
-    def abbreviation(self):
-        """Temp doc"""
-        string = "{:02d}".format(self.stepIdx)
-        if self.isFinal():
-            string += "_Final"
-        else:
-            for param in self.benchmarkParameters:
-                string += "_" + Solution.getParameterNameAbbreviation(param)
-        return string
-
     def __str__(self):
         string = "{:02d}".format(self.stepIdx)
         if self.isFinal():
             string += "_Final"
-        else:
-            for param in self.benchmarkParameters:
-                string += "_" + str(param)
         return string
 
     def __repr__(self):
diff --git a/Tensile/CustomKernels.py b/Tensile/CustomKernels.py
index 5032312c7..6ebbf30f6 100644
--- a/Tensile/CustomKernels.py
+++ b/Tensile/CustomKernels.py
@@ -31,6 +31,9 @@ def isCustomKernelConfig(config):
 def getCustomKernelFilepath(name, directory=globalParameters["CustomKernelDirectory"]):
     return os.path.join(directory, (name + ".s"))
 
+def getAllCustomKernelNames(directory=globalParameters["CustomKernelDirectory"]):
+    return [fname[:-2] for fname in os.listdir(directory) if fname.endswith(".s")]
+
 def getCustomKernelContents(name, directory=globalParameters["CustomKernelDirectory"]):
     try:
         with open(getCustomKernelFilepath(name, directory)) as f:
@@ -41,7 +44,7 @@ def getCustomKernelContents(name, directory=globalParameters["CustomKernelDirect
 def getCustomKernelConfigAndAssembly(name, directory=globalParameters["CustomKernelDirectory"]):
     contents  = getCustomKernelContents(name, directory)
     config = "\n"    #Yaml configuration properties
-    assembly = "" 
+    assembly = ""
     inConfig = False
     for line in contents.splitlines():
         if   line == "---": inConfig = True                          #Beginning of yaml section
@@ -49,7 +52,7 @@ def getCustomKernelConfigAndAssembly(name, directory=globalParameters["CustomKer
         elif      inConfig: config   += line + "\n"
         else              : assembly += line + "\n"; config += "\n"  #Second statement to keep line numbers consistent for yaml errors
 
-    return (config, assembly)  
+    return (config, assembly)
 
 def getCustomKernelConfig(name, directory=globalParameters["CustomKernelDirectory"]):
     rawConfig, _ = getCustomKernelConfigAndAssembly(name, directory)

From 9b35985c68ea346a20126d5b430dbecf7c8fb377 Mon Sep 17 00:00:00 2001
From: Benjamin Ulmer <benjamin.ulmer@amd.com>
Date: Fri, 8 Oct 2021 12:14:44 -0600
Subject: [PATCH 07/12] Docs and final refactor/clean of BenchmarkProblems.py

---
 Tensile/BenchmarkProblems.py    | 481 ++++++++++++--------------------
 Tensile/BenchmarkStructs.py     |  23 +-
 Tensile/ClientWriter.py         |  27 +-
 Tensile/LibraryIO.py            |   8 +-
 Tensile/TensileCreateLibrary.py |  35 +--
 5 files changed, 208 insertions(+), 366 deletions(-)

diff --git a/Tensile/BenchmarkProblems.py b/Tensile/BenchmarkProblems.py
index 1c0f31624..121a52b71 100644
--- a/Tensile/BenchmarkProblems.py
+++ b/Tensile/BenchmarkProblems.py
@@ -19,8 +19,6 @@
 # CTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 ################################################################################
 
-import csv
-import itertools
 import os
 import shutil
 import sys
@@ -40,31 +38,19 @@
 from .KernelWriterSource import KernelWriterSource
 from .SolutionStructs import Solution, ProblemType, ProblemSizes
 from .SolutionWriter import SolutionWriter
-from .TensileCreateLibrary import writeSolutionsAndKernels, writeCMake, buildObjectFileNames
-from .CustomKernels import getAllCustomKernelNames, getCustomKernelConfig
+from .TensileCreateLibrary import copyStaticFiles, writeSolutionsAndKernels
+from .CustomKernels import getCustomKernelConfig
 
 
-def generateForkedSolutions (problemType, constantParams, benchmarkPermutations):
-    """this creates a set or solutions based on the forked parameters using
-         a set of common parameters from which to fork from
-
-    Parameters:
-    problemType the problem type
-    hardcodedParameters the set of parameters which overrides the baseline parameters
-    benchmarkPermutations set of baseline parameters from which the the updates are branched form
-    winners previous winning parameters which overrides the derived parameters
-    initialSolutionParameters set of parameters which fills in missing params default parameters
-
-    Returns:
-    list: Soutions list
-    """
+def generateForkedSolutions(problemType, constantParams, forkPermutations):
+    """Creates a list with a Solution object for each parameter combination in forkPermutations"""
     print1("# Enumerating Solutions")
 
     solutions = []
-    for benchmarkPermutation in benchmarkPermutations:
+    for perm in forkPermutations:
         solution = {"ProblemType": deepcopy(problemType.state)}
         solution.update(constantParams)
-        solution.update(benchmarkPermutation)
+        solution.update(perm)
 
         # TODO check if solution matches problem size for exact tile kernels
         solutionObject = Solution(solution)
@@ -75,21 +61,141 @@ def generateForkedSolutions (problemType, constantParams, benchmarkPermutations)
 
     return solutions
 
-def generateCustomKernelSolution(kernelName, directory=globalParameters["CustomKernelDirectory"]):
-    """Temp docs"""
+def getCustomKernelSolutionObj(kernelName, directory=globalParameters["CustomKernelDirectory"]):
+    """Creates the Solution object for a custom kernel"""
     kernelConfig = getCustomKernelConfig(kernelName, directory)
-    checkParametersAreValid({p: [kernelConfig[p]] for p in kernelConfig if p != "ProblemType"}, validParameters)
-    # test if problem type matches with configuration file
-    kernelConfig["KernelLanguage"] = "Assembly"   # replacement kernels are always assembly kernels?
+    checkParametersAreValid({p: [kernelConfig[p]] for p in kernelConfig \
+            if p != "ProblemType"}, validParameters)
+    kernelConfig["KernelLanguage"] = "Assembly"
     kernelConfig["CustomKernelName"] = kernelName
 
     return Solution(kernelConfig)
 
-def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, problemSizeGroupIdx):
-    """Temp docs"""
+def generateCustomKernelSolutions(problemType, customKernels, failOnMismatch):
+    """Creates a list with a Solution object for each name in customKernel"""
+    solutions = []
+    for kernelName in customKernels:
+        print1("# Processing custom kernel {}".format(kernelName))
+        solution = getCustomKernelSolutionObj(kernelName)
+        if solution["ProblemType"] != problemType:
+            # Raise error if this kernel was specifically requested and problem type doesn't match
+            if failOnMismatch:
+                foo = [(k,tuple(v)) if type(v) is list else (k,v) \
+                        for k,v in problemType.items()]
+                bar = [(k,tuple(v)) if type(v) is list else (k,v) \
+                        for k,v in solution["ProblemType"].items()]
+
+                benchmarkSet = set(foo)
+                customSet    = set(bar)
+                msg = "The problem type in the config file does not match " \
+                        "that of the custom kernel, {}.".format(kernelName) \
+                        + "\nDifferent benchmark parameters:\n" \
+                        + str(benchmarkSet - (customSet & benchmarkSet)) \
+                        + "\nDifferent custom kernel parameters:\n" \
+                        +  str(customSet - (customSet & benchmarkSet))
+                raise RuntimeError(msg)
+            else:
+                print1("# Rejected {}: Problem Type doesn't match".format(kernelName))
+        else:
+            print1("# Added {} to solutions".format(kernelName))
+            if solution["Valid"]:
+                solutions.append(solution)
+            elif globalParameters["PrintSolutionRejectionReason"]:
+                print1("rejecting solution " + str(solution))
+
+    return solutions
+
+def writeBenchmarkFiles(stepBaseDir, solutions, problemSizes, \
+        stepName, solutionSummationSizes):
+    """Write all the files needed for a given benchmarking step"""
+    if not globalParameters["MergeFiles"]:
+        ensurePath(os.path.join(globalParameters["WorkingPath"], "Solutions"))
+        ensurePath(os.path.join(globalParameters["WorkingPath"], "Kernels"))
+
+    copyStaticFiles()
+
+    kernels           = []
+    kernelHelperOjbs  = []
+    kernelNames       = set()
+    kernelHelperNames = set()
+
+    # get unique kernels and kernel helpers
+    for solution in Utils.tqdm(solutions, "Finding unique solutions"):
+        solutionKernels = solution.getKernels()
+        for kernel in solutionKernels:
+            kName = Solution.getNameFull(kernel)
+            if kName not in kernelNames:
+                kernels.append(kernel)
+                kernelNames.add(kName)
+
+        solutionHelperKernels = solution.getHelperKernelObjects()
+        for ko in solutionHelperKernels:
+            kname = ko.getKernelName()
+            if kname not in kernelHelperNames:
+                kernelHelperOjbs.append(ko)
+                kernelHelperNames.add(kname)
+
+    solutionSerialNaming = Solution.getSerialNaming(solutions)
+    kernelSerialNaming   = Solution.getSerialNaming(kernels)
+    solutionMinNaming    = Solution.getMinNaming(solutions)
+    kernelMinNaming      = Solution.getMinNaming(kernels)
+    solutionWriter       = SolutionWriter(solutionMinNaming, \
+            solutionSerialNaming, kernelMinNaming, kernelSerialNaming)
+    kernelWriterSource   = KernelWriterSource(kernelMinNaming, kernelSerialNaming)
+    kernelWriterAssembly = KernelWriterAssembly(kernelMinNaming, kernelSerialNaming)
+
+    # write solution, kernels and CMake
+    problemType = solutions[0]["ProblemType"]
+    codeObjectFiles = writeSolutionsAndKernels( \
+            globalParameters["WorkingPath"], globalParameters["CxxCompiler"], \
+            [problemType], solutions, kernels, kernelHelperOjbs, solutionWriter, \
+            kernelWriterSource, kernelWriterAssembly, errorTolerant=True )
+    # ^ this is where solutions is mutated
+
+    newLibraryDir = ensurePath(os.path.join(globalParameters["WorkingPath"], 'library'))
+    newLibraryFile = os.path.join(newLibraryDir, "TensileLibrary")
+    newLibrary = SolutionLibrary.MasterSolutionLibrary.BenchmarkingLibrary(solutions)
+    newLibrary.applyNaming(kernelMinNaming)
+    LibraryIO.write(newLibraryFile, Utils.state(newLibrary), globalParameters["LibraryFormat"])
+
+    codeObjectFiles = [os.path.relpath(f, globalParameters["WorkingPath"]) \
+            for f in codeObjectFiles]
+
+    if "TileAwareSelection" in problemType and problemType["TileAwareSelection"]:
+        maxMacroTile0 = 0
+        maxMacroTile1 = 0
+        for solution in solutions:
+            macroTile0 = solution["MacroTile0"]
+            macroTile1 = solution["MacroTile1"]
+            if macroTile0 > maxMacroTile0:
+                maxMacroTile0 = macroTile0
+            if macroTile1 > maxMacroTile1:
+                maxMacroTile1 = macroTile1
+        idealM = 36 * maxMacroTile0
+        idealN = 36 * maxMacroTile1
+        idealSizes = []
+        if problemType["Batched"]:
+                for idealK in solutionSummationSizes:
+                    idealSize = {"Exact": [idealM, idealN, 1, idealK]}
+                    idealSizes.append(idealSize)
+        else:
+                for idealK in solutionSummationSizes:
+                    idealSize = {"Exact": [idealM, idealN, idealK]}
+                    idealSizes.append(idealSize)
+        idealProblemSizes = ProblemSizes(problemType, idealSizes)
+        writeClientConfig(True, solutions, idealProblemSizes, stepName, stepBaseDir, \
+            newLibrary, codeObjectFiles, True)
+    else:
+        writeClientConfig(True, solutions, problemSizes, stepName, stepBaseDir, \
+            newLibrary, codeObjectFiles, False)
+
+    if len(solutions) == 0:
+        printExit("write solutions and kernels results 0 valid soultion.")
+
+def benchmarkProblemType(problemTypeConfig, problemSizeGroupConfig, problemSizeGroupIdx):
+    """Run the benchmarking for a single entry in the BenchmarkProblems of a Tensile config"""
     benchmarkTestFails = 0
 
-    # convert config to full benchmark process (resolves defaults)
     print1("")
     print1(HR)
     print1("# Converting Config to BenchmarkProcess Object")
@@ -98,10 +204,9 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, problemSize
     benchmarkProcess = BenchmarkProcess(problemTypeConfig, problemSizeGroupConfig)
 
     enableTileSelection = benchmarkProcess.problemType["TileAwareSelection"]
-    problemTypeName = str(benchmarkProcess.problemType)
-    problemSizeGroupName = "%s_%02u" % (problemTypeName, problemSizeGroupIdx)
-    pushWorkingPath(problemSizeGroupName)
-    ensurePath(os.path.join(globalParameters["WorkingPath"],"Data"))
+    groupName = "{}_{:02d}".format(str(benchmarkProcess.problemType), problemSizeGroupIdx)
+    pushWorkingPath(groupName)
+    ensurePath(os.path.join(globalParameters["WorkingPath"], "Data"))
 
     totalBenchmarkSteps = len(benchmarkProcess)
     resultsFileBaseFinal = None
@@ -112,7 +217,6 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, problemSize
     print1("# Done Creating BenchmarkProcess Object")
     print1(HR)
 
-
     for benchmarkStepIdx in range(0, totalBenchmarkSteps):
         benchmarkStep = benchmarkProcess[benchmarkStepIdx]
         stepName = str(benchmarkStep)
@@ -122,99 +226,55 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, problemSize
         print1(HR)
         currentTime = time.time()
         elapsedTime = currentTime - startTime
-        print1("# Benchmark Step: {} - {} {:.3f}s".format(problemSizeGroupName, stepName, elapsedTime))
+        print1("# Benchmark Step: {} - {} {:.3f}s".format(groupName, stepName, elapsedTime))
         print1("# Num Sizes: {}".format(benchmarkStep.problemSizes.totalProblemSizes))
         print1("# Fork Parameters:")
         for k, v in benchmarkStep.forkParams.items():
             print1("#     {}: {}".format(k, v))
 
         pushWorkingPath(shortName)
-
-        # copy files to benchmark source directory
         stepBaseDir = globalParameters["WorkingPath"]
-        sourceDir = os.path.join(stepBaseDir, "source" )
-        ensurePath(sourceDir)
-
-        filesToCopy = []
         pushWorkingPath("source")
-        filesToCopy = [
-                "TensileTypes.h",
-                "tensile_bfloat16.h",
-                "KernelHeader.h",
-                ]
-
-        for f in filesToCopy:
-            shutil.copy(
-                    os.path.join(globalParameters["SourcePath"], f),
-                    globalParameters["WorkingPath"] )
-        if globalParameters["RuntimeLanguage"] == "OCL":
-            shutil.copy(
-                    os.path.join(globalParameters["SourcePath"], "FindOpenCL.cmake"),
-                    globalParameters["WorkingPath"] )
-        else:
-            shutil.copy(
-                    os.path.join(globalParameters["SourcePath"], "FindHIP.cmake"),
-                    globalParameters["WorkingPath"] )
 
         # enumerate benchmark permutations and create resulting solution objects
         forkPermutations = constructForkPermutations(benchmarkStep.forkParams)
-        maxPossibleSolutions = len(forkPermutations) #* numHardcoded
-        solutions = generateForkedSolutions(benchmarkProcess.problemType, \
+        maxPossibleSolutions = len(forkPermutations)
+
+        regSolutions = generateForkedSolutions(benchmarkProcess.problemType, \
                 benchmarkStep.constantParams, forkPermutations)
+        kcSolutions = generateCustomKernelSolutions(benchmarkProcess.problemType, \
+                benchmarkStep.customKernels, not benchmarkStep.customKernelWildcard)
 
-        for kernelName in benchmarkStep.customKernels:
-            print1("# Processing custom kernel {}".format(kernelName))
-            customSolution = generateCustomKernelSolution(kernelName)
-            if customSolution["ProblemType"] != benchmarkProcess.problemType:
-                # Raise error if this kernel was specifically requested and problem type doesn't match
-                if not benchmarkStep.customKernelWildcard:
-                    missingParams = [p for p in benchmarkProcess.problemType \
-                            if p not in customSolution["ProblemType"]]
-                    extraParams   = [p for p in customSolution["ProblemType"] \
-                            if p not in benchmarkProcess.problemType]
-
-                    msg  = "The problem type in the config file does not match " \
-                                 "that of the custom kernel, {0}.".format(kernelName)
-                    msg += "\nMissing config parameters:\n" + str(missingParams)
-                    msg += "\nExtra custom kernel parameters:\n" + str(extraParams)
-                    raise RuntimeError(msg)
-                else:
-                    print1("# Rejected {}: Problem Type doesn't match".format(kernelName))
-            else:
-                print1("# Added {} to solutions".format(kernelName))
-                maxPossibleSolutions += 1
-                if customSolution["Valid"]:
-                    solutions.append(customSolution)
-                elif globalParameters["PrintSolutionRejectionReason"]:
-                    print1("rejecting solution " + str(customSolution))
+        maxPossibleSolutions += len(kcSolutions)
+        solutions = regSolutions + kcSolutions
 
         print1("# Actual Solutions: {} / {} after SolutionStructs\n" \
-                .format(len(solutions), maxPossibleSolutions))
+            .format(len(solutions), maxPossibleSolutions))
 
         # handle no valid solutions
         if len(solutions) == 0:
-                msg = "Your parameters resulted in 0 valid solutions."
-                if globalParameters["PrintSolutionRejectionReason"]:
-                        msg += "\nExamine reject and backtrace messages above to see why" \
-                                "and where solutions were rejected."
-                else:
-                        msg += "\nYou should re-run with \"PrintSolutionRejectionReason: True\"" \
-                                "to see why each parameter combination was rejected."
-                printExit(msg)
+            msg = "Your parameters resulted in 0 valid solutions."
+            if globalParameters["PrintSolutionRejectionReason"]:
+                msg += "\nExamine reject and backtrace messages above to see why" \
+                        "and where solutions were rejected."
+            else:
+                msg += "\nYou should re-run with \"PrintSolutionRejectionReason: True\"" \
+                        "to see why each parameter combination was rejected."
+            printExit(msg)
 
         if globalParameters["PrintLevel"] >= 1:
             for solution in solutions:
-                print2("#    (%u:%u) %s" % (0, 0, Solution.getNameFull(solution) ))
+                print2("#    ({}:{}) {}".format(0, 0, Solution.getNameFull(solution)) )
             print2(HR)
 
         # write benchmarkFiles
         prevCount = len(solutions)
         writeBenchmarkFiles(stepBaseDir, solutions, benchmarkStep.problemSizes, \
-                shortName, filesToCopy, [])
+                shortName, [])
         # ^ this mutates solutions
 
-        print1("# Actual Solutions: %u / %u after KernelWriter\n" \
-                    % (len(solutions), prevCount ))
+        print1("# Actual Solutions: {} / {} after KernelWriter\n" \
+                .format(len(solutions), prevCount ))
 
         popWorkingPath() # source
 
@@ -233,7 +293,8 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, problemSize
 
             if returncode:
                 benchmarkTestFails += 1
-                printWarning("BenchmarkProblems: Benchmark Process exited with code %u" % returncode)
+                printWarning("BenchmarkProblems: Benchmark Process exited with code {}" \
+                        .format(returncode))
         else:
             print1("# Already benchmarked; skipping.")
 
@@ -244,211 +305,11 @@ def benchmarkProblemType( problemTypeConfig, problemSizeGroupConfig, problemSize
         popWorkingPath() # stepName
         currentTime = time.time()
         elapsedTime = currentTime - startTime
-        print1("%s\n# %s\n# %s: End - %.3fs\n%s\n" \
-                % (HR, problemSizeGroupName, shortName, elapsedTime, HR))
+        print1("{}\n# {}\n# {}: End - {:.3f}s\n{}\n" \
+                .format(HR, groupName, shortName, elapsedTime, HR))
 
     popWorkingPath() # ProblemType
     return (resultsFileBaseFinal, benchmarkTestFails)
-# End benchmarkProblemType()
-
-def compareResults(old, new, name):
-    """Temp doc"""
-    import math
-    if name == " WinnerIdx":
-        return 0
-
-    try:
-            old = float(old)
-    except (ValueError, TypeError):
-            old = -1
-
-    try:
-            new = float(new)
-    except (ValueError, TypeError):
-            new = -1
-
-    def isbad(x):
-            return x <= 0 or math.isnan(x) or math.isinf(x)
-
-    if isbad(old) and isbad(new):
-            return 0
-    if isbad(old):
-            return 1
-    if isbad(new):
-            raise ValueError("Old is good ({}) and new is bad ({}). Name: {}".format(old, new, name))
-
-    return abs((old-new)/old)
-
-def getResults(resultsFileName, solutions, enableTileSelection, newResultsFileName=None):
-    """Temp docs"""
-    print1("# Get Results from CSV")
-    try:
-        resultsFile = open(resultsFileName, "r")
-    except IOError:
-        printExit("Can't open \"%s\" to get results" % resultsFileName )
-
-    newCSV = itertools.repeat(None)
-    if newResultsFileName is not None:
-        newFile = open(newResultsFileName, 'r')
-        newCSV = csv.reader(newFile)
-
-        diffFile = open(newResultsFileName+'-diff.csv', 'w')
-        diffCSV = csv.writer(diffFile)
-
-    # setup data structures
-    results = []
-    numSolutions = 0
-    for solutionsForHardcoded in solutions:
-        results.append([])
-        for solution in solutionsForHardcoded:
-            numColForProblemSize = solution["ProblemType"]["TotalIndices"]
-            results[-1].append([])
-            numSolutions += 1
-
-    # read results in gflops
-    csvFile = csv.reader(resultsFile)
-
-    if globalParameters["CSVExportWinner"]:
-        # in both old/new clients, csv files always output "GFlops" ,...., "LDD" "LDC" "LDA" "LDB" "TotalFlops" "WinnerGFlops" "WinnerTimeUS" "WinnerIdx" "WinnerName" columns
-        startIdx = numColForProblemSize + 10
-    else:
-        # in both old/new clients, csv files always output "GFlops" ,...., "LDD" "LDC" "LDA" "LDB"columns
-        # old client, non-GEMM csv files don't contain "LDD" "LDC" "LDA" "LDB", so we output an "N/A" text (in csv only) for alignment purpose (-diff.csv)
-        startIdx = numColForProblemSize + 5
-
-    rowLength = startIdx + numSolutions
-
-    rowIdx = 0
-    for row,newRow in zip(csvFile, newCSV):
-        rowIdx+=1
-        if rowIdx == 1:
-            if newRow is not None:
-                diffCSV.writerow(row)
-                diffCSV.writerow(newRow)
-                headerRow = row
-            continue
-        else:
-            if len(row) < rowLength:
-                printWarning("CSV File %s row %u doesn't have %u elements; ignoring remainer of file." \
-                        % (resultsFileName, rowIdx, rowLength) )
-                break
-            if newRow is not None:
-                diffCSV.writerow([compareResults(old,new,name) for old,new,name in itertools.zip_longest(row, newRow, headerRow)])
-
-            idx = startIdx
-            for i,solutionsForHardcoded in enumerate(solutions):
-                for j,solution in enumerate(solutionsForHardcoded):
-                    gflops = float(row[idx])
-
-                    results[i][j].append(gflops)
-                    idx += 1
-    if rowIdx < 2 and not enableTileSelection:
-        printExit("CSV File %s only has %u row(s); prior benchmark must not have run long enough to produce data." \
-                % (resultsFileName, rowIdx) )
-
-    resultsFile.close()
-    if newResultsFileName is not None:
-        newFile.close()
-        diffFile.close()
-    return results
-
-def writeBenchmarkFiles(stepBaseDir, solutions, problemSizes, \
-    stepName, filesToCopy, solutionSummationSizes):
-  """Temp doc"""
-  if not globalParameters["MergeFiles"] or globalParameters["NumMergedFiles"] > 1:
-    ensurePath(os.path.join(globalParameters["WorkingPath"], "Solutions"))
-    ensurePath(os.path.join(globalParameters["WorkingPath"], "Kernels"))
-
-  # min Naming
-  kernels = []
-  kernelHelperOjbs = []
-
-  kernelNames = set()
-  kernelHelperNames = set()
-
-  for solution in Utils.tqdm(solutions, "Finding unique solutions"):
-    solutionKernels = solution.getKernels()
-    for kernel in solutionKernels:
-      kName = Solution.getNameFull(kernel)
-      if kName not in kernelNames:
-        kernels.append(kernel)
-        kernelNames.add(kName)
-
-    solutionHelperKernels = solution.getHelperKernelObjects()
-    for ko in solutionHelperKernels:
-      kname = ko.getKernelName()
-      if kname not in kernelHelperNames:
-        kernelHelperOjbs.append(ko)
-        kernelHelperNames.add(kname)
-
-
-  solutionSerialNaming = Solution.getSerialNaming(solutions)
-  kernelSerialNaming   = Solution.getSerialNaming(kernels)
-  solutionMinNaming    = Solution.getMinNaming(solutions)
-  kernelMinNaming      = Solution.getMinNaming(kernels)
-  solutionWriter       = SolutionWriter(solutionMinNaming, solutionSerialNaming, kernelMinNaming, kernelSerialNaming)
-  kernelWriterSource   = KernelWriterSource(kernelMinNaming, kernelSerialNaming)
-  kernelWriterAssembly = KernelWriterAssembly(kernelMinNaming, kernelSerialNaming)
-
-  # write solution, kernels and CMake
-  problemType = solutions[0]["ProblemType"]
-  codeObjectFiles = writeSolutionsAndKernels( \
-      globalParameters["WorkingPath"], globalParameters["CxxCompiler"], [problemType], solutions, kernels, kernelHelperOjbs, \
-      solutionWriter, kernelWriterSource, kernelWriterAssembly, errorTolerant=True )
-  # ^ this is where solutions is mutated
-
-  newLibraryDir = ensurePath(os.path.join(globalParameters["WorkingPath"], 'library'))
-  newLibraryFile = os.path.join(newLibraryDir, "TensileLibrary")
-  newLibrary = SolutionLibrary.MasterSolutionLibrary.BenchmarkingLibrary(solutions)
-  newLibrary.applyNaming(kernelMinNaming)
-  LibraryIO.write(newLibraryFile, Utils.state(newLibrary), globalParameters["LibraryFormat"])
-
-  codeObjectFiles = [os.path.relpath(f, globalParameters["WorkingPath"]) for f in codeObjectFiles]
-
-  writeClientConfig(True, solutions, problemSizes, stepName, stepBaseDir, newLibrary, codeObjectFiles, False)
-
-  if "TileAwareSelection" in problemType and problemType["TileAwareSelection"]:
-    maxMacroTile0 = 0
-    maxMacroTile1 = 0
-    for solution in solutions:
-      macroTile0 = solution["MacroTile0"]
-      macroTile1 = solution["MacroTile1"]
-      if macroTile0 > maxMacroTile0:
-        maxMacroTile0 = macroTile0
-      if macroTile1 > maxMacroTile1:
-        maxMacroTile1 = macroTile1
-    idealM = 36 * maxMacroTile0
-    idealN = 36 * maxMacroTile1
-    idealSizes = []
-    if problemType["Batched"]:
-        for idealK in solutionSummationSizes:
-          idealSize = {"Exact": [idealM, idealN, 1, idealK]}
-          idealSizes.append(idealSize)
-    else:
-        for idealK in solutionSummationSizes:
-          idealSize = {"Exact": [idealM, idealN, idealK]}
-          idealSizes.append(idealSize)
-    idealProblemSizes = ProblemSizes(problemType, idealSizes)
-    writeClientConfig(True, solutions, idealProblemSizes, stepName, stepBaseDir, newLibrary, codeObjectFiles, True)
-
-    if len(solutions) == 0:
-        printExit("write solutions and kernels results 0 valid soultion.")
-
-    # write CMake
-    outputPath = globalParameters["WorkingPath"]
-
-    (solutionFiles,
-     sourceKernelFiles,
-     asmKernelFiles,
-     sourceLibFiles,
-     asmLibFiles) = buildObjectFileNames(solutionWriter, kernelWriterSource, \
-        kernelWriterAssembly, solutions, kernels, kernelHelperOjbs)
-
-    writeCMake(outputPath, solutionFiles, sourceKernelFiles, filesToCopy)
-
-    for fileName in filesToCopy:
-        shutil.copy( os.path.join(globalParameters["SourcePath"], fileName), \
-            outputPath )
 
 
 def main(config):
@@ -467,7 +328,7 @@ def main(config):
         else:
             problemSizeGroupConfigs = benchmarkProblemTypeConfig[1:]
 
-        for problemSizeGroupIdx, problemSizeGroupConfig in enumerate(problemSizeGroupConfigs):
+        for idx, problemSizeGroupConfig in enumerate(problemSizeGroupConfigs):
             print2("ProblemTypeConfig: {}".format(problemTypeConfig))
             problemTypeObj = ProblemType(problemTypeConfig)
             globalParameters["EnableHalf"] = problemTypeObj["DataType"].isHalf()
@@ -476,11 +337,11 @@ def main(config):
             csvSuffix = "_CSVWinner" if globalParameters["CSVExportWinner"] else ""
             # results files will be named
             newResultsFileName = os.path.join(dataPath, "{}_{:02d}{}.csv" \
-                    .format(str(problemTypeObj), problemSizeGroupIdx, csvSuffix) )
+                    .format(str(problemTypeObj), idx, csvSuffix) )
             newSolutionsFileName = os.path.join(dataPath, "{}_{:02d}{}.yaml" \
-                    .format(str(problemTypeObj), problemSizeGroupIdx, csvSuffix) )
+                    .format(str(problemTypeObj), idx, csvSuffix) )
             newGranularityFileName = os.path.join(dataPath, "{}_{:02d}{}.gsp" \
-                    .format(str(problemTypeObj), problemSizeGroupIdx, csvSuffix) )
+                    .format(str(problemTypeObj), idx, csvSuffix) )
 
             # skip if possible
             if globalParameters["ForceRedoBenchmarkProblems"] \
@@ -488,7 +349,7 @@ def main(config):
 
                 # benchmark problem size group
                 (resultsFileBaseFinal, benchmarkErrors) = \
-                        benchmarkProblemType(problemTypeConfig, problemSizeGroupConfig, problemSizeGroupIdx)
+                        benchmarkProblemType(problemTypeConfig, problemSizeGroupConfig, idx)
                 totalTestFails += benchmarkErrors
 
                 print("clientExit={} {} for {}" \
@@ -496,9 +357,9 @@ def main(config):
                         globalParameters["ConfigPath"]) )
 
                 # copy data
-                resultsFileBase = resultsFileBaseFinal
-                resultsFileName = resultsFileBase + ".csv"
-                solutionsFileName = resultsFileBase + ".yaml"
+                resultsFileBase     = resultsFileBaseFinal
+                resultsFileName     = resultsFileBase + ".csv"
+                solutionsFileName   = resultsFileBase + ".yaml"
                 granularityFileName = resultsFileBase + "_Granularity.csv"
                 shutil.copy( resultsFileName, newResultsFileName )
                 shutil.copy( solutionsFileName, newSolutionsFileName )
@@ -506,7 +367,7 @@ def main(config):
                     shutil.copy( granularityFileName, newGranularityFileName )
             else:
                 print1("# {}_{:02d} already benchmarked; skipping." \
-                        .format(str(problemTypeObj), problemSizeGroupIdx) )
+                        .format(str(problemTypeObj), idx) )
 
     popWorkingPath()
 
diff --git a/Tensile/BenchmarkStructs.py b/Tensile/BenchmarkStructs.py
index 36e50307a..7236b395a 100644
--- a/Tensile/BenchmarkStructs.py
+++ b/Tensile/BenchmarkStructs.py
@@ -24,8 +24,7 @@
         defaultBenchmarkFinalProblemSizes, defaultBatchedBenchmarkFinalProblemSizes, \
         defaultBenchmarkCommonParameters, validParameters, globalParameters
 from .CustomKernels import getAllCustomKernelNames
-from .SolutionStructs import Solution, ProblemType, ProblemSizes
-from Tensile import CustomKernels
+from .SolutionStructs import ProblemType, ProblemSizes
 
 
 def getDefaultsForMissingParameters(paramList, defaultParams):
@@ -41,7 +40,7 @@ def getDefaultsForMissingParameters(paramList, defaultParams):
 def checkParametersAreValid(params, validParams):
     """Ensures paramaters in params exist and have valid values as specified by validParames"""
     for name, values in params.items():
-        if name in ["ProblemSizes"]:
+        if name == "ProblemSizes":
             continue
 
         if name not in validParams:
@@ -88,19 +87,19 @@ def __init__(self, problemTypeConfig, problemSizeGroupConfig):
         """Create from the two sections of a config for a BenchmarkProblem"""
         self.problemType = ProblemType(problemTypeConfig)
         self.isBatched = "Batched" in problemTypeConfig and problemTypeConfig["Batched"]
-        print2("# BenchmarkProcess beginning %s" % str(self.problemType))
+        print2("# BenchmarkProcess beginning {}".format(self.problemType))
 
         # fill parameter values from config
         self.singleValueParams = {}
-        self.multiValueParams = {}
-        self.customKernels = []
-        self.sizes = None
+        self.multiValueParams  = {}
+        self.customKernels     = []
+        self.sizes             = None
         self.getConfigParameters(self.isBatched, problemSizeGroupConfig)
 
         # convert parameter lists to steps
         # previously, multiple benchmark steps were possible
         # currently only 1 benchmark step is possible; more may be added back later
-        self.benchmarkSteps = []
+        self.benchmarkSteps   = []
         self.benchmarkStepIdx = 0
         self.convertParametersToSteps()
 
@@ -222,11 +221,11 @@ class BenchmarkStep:
 
     def __init__(self, forkParams, constantParams, customKernels, problemSizes, idx):
         """Basic constructor storing each argument"""
-        self.forkParams = forkParams
+        self.forkParams     = forkParams
         self.constantParams = constantParams
-        self.customKernels = customKernels
-        self.problemSizes = problemSizes
-        self.stepIdx = idx
+        self.customKernels  = customKernels
+        self.problemSizes   = problemSizes
+        self.stepIdx        = idx
 
         self.customKernelWildcard = False
         if self.customKernels == ["*"]:
diff --git a/Tensile/ClientWriter.py b/Tensile/ClientWriter.py
index fa1ae2676..8deaedb8f 100644
--- a/Tensile/ClientWriter.py
+++ b/Tensile/ClientWriter.py
@@ -19,11 +19,12 @@
 # CTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 ################################################################################
 
-from .Common import globalParameters, pushWorkingPath, popWorkingPath, print1, printExit, CHeader, printWarning, listToInitializer, ClientExecutionLock
 from . import ClientExecutable
 from . import Common
 from . import LibraryIO
+from .Common import globalParameters, pushWorkingPath, popWorkingPath, print1, printExit, CHeader, printWarning, listToInitializer, ClientExecutionLock
 from .SolutionStructs import ProblemType, ProblemSizesMock
+from .TensileCreateLibrary import copyStaticFiles
 
 import os
 import subprocess
@@ -60,6 +61,7 @@ class ClientLogLevel(Enum):
   Verbose = 2
   Debug = 3
 
+
 ################################################################################
 # Main
 ################################################################################
@@ -68,29 +70,8 @@ def main( config ):
       globalParameters["LibraryLogicPath"])
   stepBaseDir = pushWorkingPath(globalParameters["LibraryClientPath"])
 
-
-  ##############################################################################
-  # Copy Source Files
-  ##############################################################################
   pushWorkingPath("source")
-  filesToCopy = [
-      "TensileTypes.h",
-      "tensile_bfloat16.h",
-      "KernelHeader.h",
-      ]
-
-  for f in filesToCopy:
-    shutil.copy(
-        os.path.join(globalParameters["SourcePath"], f),
-        globalParameters["WorkingPath"] )
-  if globalParameters["RuntimeLanguage"] == "OCL":
-    shutil.copy(
-        os.path.join(globalParameters["SourcePath"], "FindOpenCL.cmake"),
-        globalParameters["WorkingPath"] )
-  else:
-    shutil.copy(
-        os.path.join(globalParameters["SourcePath"], "FindHIP.cmake"),
-        globalParameters["WorkingPath"] )
+  copyStaticFiles()
 
   ##############################################################################
   # Read Logic Files
diff --git a/Tensile/LibraryIO.py b/Tensile/LibraryIO.py
index 6d2760dad..b1fbef007 100644
--- a/Tensile/LibraryIO.py
+++ b/Tensile/LibraryIO.py
@@ -110,9 +110,9 @@ def parseSolutionsFile(filename):
 
 def parseSolutionsData(data, srcFile="?"):
     """Parses problem sizes and solutions from the data of a solutions file."""
-
     if len(data) < 3:
-        printExit("Solution file {} is missing required fields (len = {} < 3".format(srcFile, len(data)))
+        printExit("Solution file {} is missing required fields (len = {} < 3" \
+                .format(srcFile, len(data)))
 
     versionString = data[0]["MinimumRequiredVersion"]
     if not versionIsCompatible(versionString):
@@ -142,9 +142,9 @@ def parseLibraryLogicFile(filename):
 
 def parseLibraryLogicData(data, srcFile="?"):
     """Parses the data of a library logic file."""
-
     if len(data) < 9:
-        printExit("Library logic file {} is missing required fields (len = {} < 9)".format(srcFile, len(data)))
+        printExit("Library logic file {} is missing required fields (len = {} < 9)" \
+                .format(srcFile, len(data)))
 
     versionString     = data[0]["MinimumRequiredVersion"]
     scheduleName      = data[1]
diff --git a/Tensile/TensileCreateLibrary.py b/Tensile/TensileCreateLibrary.py
index 05e99498a..dc2b52143 100644
--- a/Tensile/TensileCreateLibrary.py
+++ b/Tensile/TensileCreateLibrary.py
@@ -91,7 +91,7 @@ def getAssemblyCodeObjectFiles(kernels, kernelWriterAssembly, outputPath):
           coFile = os.path.join(os.path.normcase(destArchDir), 'TensileLibrary_{}.co'.format(archName))
 
         if os.name == "nt":
-          # On Windows, the objectFiles list command line (including spaces) 
+          # On Windows, the objectFiles list command line (including spaces)
           # exceeds the limit of 8191 characters, so using response file
 
           responseArgs = objectFiles
@@ -109,7 +109,7 @@ def getAssemblyCodeObjectFiles(kernels, kernelWriterAssembly, outputPath):
         coFiles.append(coFile)
       else:
         # no mergefiles
-        
+
         assemblyKernelNames = [kernelWriterAssembly.getKernelFileBase(k) for k in archKernels]
         origCOFiles = [os.path.join(asmDir,  k + '.co') for k in assemblyKernelNames]
         newCOFiles  = []
@@ -319,8 +319,8 @@ def buildKernelSourceAndHeaderFiles(results, outputPath, kernelsWithBuildErrs, \
     kernelSourceFile:     File to write source data to
     kernelHeaderFile:     File to write header data to
 
-  Returns: 
-    sourceFilenames:      Array containing source kernel filenames 
+  Returns:
+    sourceFilenames:      Array containing source kernel filenames
   """
   sourceFilenames = []
 
@@ -336,19 +336,19 @@ def buildKernelSourceAndHeaderFiles(results, outputPath, kernelsWithBuildErrs, \
     if len(src.strip()) == 0:
       continue
     kernelsToWrite.append((err, src, header, kernelName))
-  
+
   # Write kernel data to files
   if globalParameters["MergeFiles"]:
 
     # Merge all kernels into one file
-    if globalParameters["NumMergedFiles"] == 1: 
+    if globalParameters["NumMergedFiles"] == 1:
       sourceFilenames.append(os.path.join(os.path.normcase(outputPath), "Kernels.cpp"))
       for (err,src,header,kernelName) in kernelsToWrite:
         kernelSourceFile.write(src)
         kernelHeaderFile.write(header)
 
     # Merge kernels into n seperate files
-    else: 
+    else:
 
       # Calculate num of kernels per file
       numValidKernels = len(kernelsToWrite)
@@ -358,7 +358,7 @@ def buildKernelSourceAndHeaderFiles(results, outputPath, kernelsWithBuildErrs, \
 
       # Number of kernels per file (except the last file which gets this plus any remainder)
       kernelsPerFile = numValidKernels // numMergedFiles
-    
+
       # Group kernel sources and headers into a list of lists for writing
       kernelSourceList = []
       kernelHeaderList = []
@@ -399,7 +399,7 @@ def buildKernelSourceAndHeaderFiles(results, outputPath, kernelsWithBuildErrs, \
             kernelHeaderFile.write(header)
 
   # Write kernels into seperate files
-  else: 
+  else:
     for (err,src,header,kernelName) in kernelsToWrite:
 
       # write kernel.cpp
@@ -417,7 +417,7 @@ def buildKernelSourceAndHeaderFiles(results, outputPath, kernelsWithBuildErrs, \
       kernelHeaderFile.write(CHeader)
       kernelHeaderFile.write(header)
       kernelHeaderFile.close()
-  
+
   return sourceFilenames
 
 ################################################################################
@@ -987,7 +987,9 @@ def getSolutionAndKernelWriters(solutions, kernels):
 ################################################################################
 # copy static cpp files and headers
 ################################################################################
-def copyStaticFiles(outputPath):
+def copyStaticFiles(outputPath=None):
+  if outputPath is None:
+    outputPath = globalParameters["WorkingPath"]
   libraryStaticFiles = [
     "TensileTypes.h",
     "tensile_bfloat16.h",
@@ -1454,7 +1456,7 @@ def splitExtraParameters(par):
   if globalParameters["GenerateManifestAndExit"] == True:
     return
 
-  # generate cmake for the source kernels
+  # generate cmake for the source kernels,
   if not arguments["GenerateSourcesAndExit"]:
     writeCMake(outputPath, solutionFiles, sourceKernelFiles, staticFiles)
 
@@ -1468,13 +1470,13 @@ def splitExtraParameters(par):
 
   codeObjectFiles = writeSolutionsAndKernels(outputPath, CxxCompiler, problemTypes, solutions,
                                              kernels, kernelHelperObjs, solutionWriter, kernelWriterSource, kernelWriterAssembly)
-  
+
   bothLibSet = set(sourceLibPaths + asmLibPaths)
-  setA = set( map( os.path.normcase, set(codeObjectFiles) ) ) 
+  setA = set( map( os.path.normcase, set(codeObjectFiles) ) )
   setB = set( map( os.path.normcase, bothLibSet ) )
 
-  sanityCheck0 = setA - setB 
-  sanityCheck1 = setB - setA 
+  sanityCheck0 = setA - setB
+  sanityCheck1 = setB - setA
 
   if globalParameters["PrintCodeCommands"]:
     print("codeObjectFiles:", codeObjectFiles)
@@ -1519,4 +1521,3 @@ def splitExtraParameters(par):
   print1("# Tensile Library Writer DONE")
   print1(HR)
   print1("")
-  
\ No newline at end of file

From 9acd49d19b395afdd7d43d9f6461623582c0c194 Mon Sep 17 00:00:00 2001
From: Benjamin Ulmer <benjamin.ulmer@amd.com>
Date: Fri, 8 Oct 2021 14:23:01 -0600
Subject: [PATCH 08/12] Clean custom kernel error message and remove dup code
 from TensileRetuneLibrary

---
 Tensile/BenchmarkProblems.py    | 21 ++++++++++-----------
 Tensile/TensileRetuneLibrary.py | 25 +------------------------
 2 files changed, 11 insertions(+), 35 deletions(-)

diff --git a/Tensile/BenchmarkProblems.py b/Tensile/BenchmarkProblems.py
index 121a52b71..d7f2a43ae 100644
--- a/Tensile/BenchmarkProblems.py
+++ b/Tensile/BenchmarkProblems.py
@@ -80,20 +80,19 @@ def generateCustomKernelSolutions(problemType, customKernels, failOnMismatch):
         if solution["ProblemType"] != problemType:
             # Raise error if this kernel was specifically requested and problem type doesn't match
             if failOnMismatch:
-                foo = [(k,tuple(v)) if type(v) is list else (k,v) \
-                        for k,v in problemType.items()]
-                bar = [(k,tuple(v)) if type(v) is list else (k,v) \
-                        for k,v in solution["ProblemType"].items()]
+                benchmarkSet = set([(k,tuple(v)) if type(v) is list else (k,v) \
+                        for k,v in problemType.items()])
+                customSet = set([(k,tuple(v)) if type(v) is list else (k,v) \
+                        for k,v in solution["ProblemType"].items()])
 
-                benchmarkSet = set(foo)
-                customSet    = set(bar)
                 msg = "The problem type in the config file does not match " \
                         "that of the custom kernel, {}.".format(kernelName) \
-                        + "\nDifferent benchmark parameters:\n" \
-                        + str(benchmarkSet - (customSet & benchmarkSet)) \
-                        + "\nDifferent custom kernel parameters:\n" \
-                        +  str(customSet - (customSet & benchmarkSet))
-                raise RuntimeError(msg)
+                        + "\nDiffering parameters:\n" \
+                        + "\tConfig values:\n\t" \
+                        + str(sorted(benchmarkSet - (customSet & benchmarkSet))) \
+                        + "\n\tCustom kernel values:\n\t" \
+                        +  str(sorted(customSet - (customSet & benchmarkSet)))
+                printExit(msg)
             else:
                 print1("# Rejected {}: Problem Type doesn't match".format(kernelName))
         else:
diff --git a/Tensile/TensileRetuneLibrary.py b/Tensile/TensileRetuneLibrary.py
index 4bab7e0a3..4b02938d5 100644
--- a/Tensile/TensileRetuneLibrary.py
+++ b/Tensile/TensileRetuneLibrary.py
@@ -88,32 +88,9 @@ def runBenchmarking(solutions, problemSizes, outPath, update):
     if update:
         Common.globalParameters["LibraryUpdateFile"] = os.path.join(resultsDir, "update.yaml")
 
-    # legacy
     pushWorkingPath(shortName)
     pushWorkingPath("source")
-
-    filesToCopy = [
-        "TensileTypes.h",
-        "tensile_bfloat16.h",
-        "KernelHeader.h",
-        ]
-
-    for f in filesToCopy:
-        shutil.copy(
-            os.path.join(globalParameters["SourcePath"], f),
-            globalParameters["WorkingPath"] )
-    if globalParameters["RuntimeLanguage"] == "OCL":
-        shutil.copy(
-            os.path.join(globalParameters["SourcePath"], "FindOpenCL.cmake"),
-            globalParameters["WorkingPath"] )
-    else:
-        shutil.copy(
-            os.path.join(globalParameters["SourcePath"], "FindHIP.cmake"),
-            globalParameters["WorkingPath"] )
-    # end legacy
-
-    BenchmarkProblems.writeBenchmarkFiles(benchmarkDir, solutions, problemSizes, shortName, filesToCopy, [])
-
+    BenchmarkProblems.writeBenchmarkFiles(benchmarkDir, solutions, problemSizes, shortName, [])
     popWorkingPath() # source
 
     libraryLogicPath = None

From d732ea8fc464e2e8d9edd33b413c4c25e0a0e2d2 Mon Sep 17 00:00:00 2001
From: Benjamin Ulmer <benjamin.ulmer@amd.com>
Date: Fri, 8 Oct 2021 17:08:24 -0600
Subject: [PATCH 09/12] Fix getting None values from config

---
 Tensile/BenchmarkStructs.py | 14 ++++++++++----
 1 file changed, 10 insertions(+), 4 deletions(-)

diff --git a/Tensile/BenchmarkStructs.py b/Tensile/BenchmarkStructs.py
index 7236b395a..5298d65ad 100644
--- a/Tensile/BenchmarkStructs.py
+++ b/Tensile/BenchmarkStructs.py
@@ -125,10 +125,16 @@ def getConfigParameters(self, isbatched, config):
         elif len(badsInConfig) > 1:
             printExit("Benchmark steps {} are no longer supported".format(badsInConfig))
 
-        # get supported legacy benchmark steps
-        benchmarkCommonParams = config.get("BenchmarkCommonParameters", [])
-        forkParams            = config.get("ForkParameters", [])
-        self.customKernels    = config.get("CustomKernels", [])
+        # get supported benchmark steps
+        def getNonNoneFromConfig(key, default):
+            if config.get(key) is not None:
+                return config[key]
+            else:
+                return default
+
+        benchmarkCommonParams = getNonNoneFromConfig("BenchmarkCommonParameters", [])
+        forkParams            = getNonNoneFromConfig("ForkParameters", [])
+        self.customKernels    = getNonNoneFromConfig("CustomKernels", [])
 
         if "BenchmarkFinalParameters" in config:
             sizes = config["BenchmarkFinalParameters"][0]["ProblemSizes"]

From e69a257270a19c3b69e0259845558482c3ed54b2 Mon Sep 17 00:00:00 2001
From: Benjamin Ulmer <benjamin.ulmer@amd.com>
Date: Tue, 12 Oct 2021 10:35:31 -0600
Subject: [PATCH 10/12] Fix test and remove no longer needed test

---
 Tensile/Tests/unit/test_CustomKernels.py      |  8 ++++----
 .../Tests/unit/test_TensileCreateLibrary.py   | 19 -------------------
 2 files changed, 4 insertions(+), 23 deletions(-)

diff --git a/Tensile/Tests/unit/test_CustomKernels.py b/Tensile/Tests/unit/test_CustomKernels.py
index f852f11b6..32c88eb47 100644
--- a/Tensile/Tests/unit/test_CustomKernels.py
+++ b/Tensile/Tests/unit/test_CustomKernels.py
@@ -22,7 +22,7 @@
 import pytest
 import os
 from Tensile.CustomKernels import getCustomKernelConfig, getCustomKernelContents
-from Tensile.BenchmarkProblems import generateCustomKernelSolution
+from Tensile.BenchmarkProblems import getCustomKernelSolutionObj
 from Tensile.Common import assignGlobalParameters
 import yaml
 
@@ -38,7 +38,7 @@ def test_FindCustomKernel(objs):
         assert False
 
 configResult = yaml.safe_load(
-"""  
+"""
 ProblemType:
     OperationType: GEMM
     DataType: s
@@ -62,7 +62,7 @@ def test_ReadCustomKernelConfig(objs):
     try:
         name, directory, result = objs
         config = getCustomKernelConfig(name, directory)
-        config["custom.config"] = result  
+        config["custom.config"] = result
     except:
         assert False
 
@@ -72,7 +72,7 @@ def test_CreateSolutionFromCustomKernel(objs):
         assignGlobalParameters({})
 
         name, directory = objs
-        solution = generateCustomKernelSolution(name, directory)
+        solution = getCustomKernelSolutionObj(name, directory)
         assert solution["Valid"]
     except:
         assert False
diff --git a/Tensile/Tests/unit/test_TensileCreateLibrary.py b/Tensile/Tests/unit/test_TensileCreateLibrary.py
index 0ae8cc1a5..7c3ec7a7b 100644
--- a/Tensile/Tests/unit/test_TensileCreateLibrary.py
+++ b/Tensile/Tests/unit/test_TensileCreateLibrary.py
@@ -28,29 +28,10 @@
 import Tensile.Common as Common
 import Tensile.ClientWriter as ClientWriter
 import Tensile.SolutionStructs as SolutionStructs
-import Tensile.BenchmarkProblems as BenchmarkProblems
-import Tensile.BenchmarkStructs as BenchmarkStructs
 import yaml
 
 mylogger = logging.getLogger()
 
-def test_generateSolutions(useGlobalParameters):
-    with useGlobalParameters():
-        scriptDir = os.path.dirname(os.path.realpath(__file__))
-        dataDir = os.path.realpath(os.path.join(scriptDir, "..", "test_data", "unit"))
-        problemTypeFilePath = os.path.join(dataDir, "library_data", "problemType.yaml")
-        hardcodedParametersFilePath = os.path.join(dataDir, "library_data", "hardcodedParameters.yaml")
-        initialSolutionParametersFilePath = os.path.join(dataDir, "library_data", "initialSolutionParameters.yaml")
-
-        problemType = LibraryIO.readYAML(problemTypeFilePath)["ProblemType"]
-        problemTypeObject = SolutionStructs.ProblemType(problemType)
-        hardcodedParameters = LibraryIO.readYAML(hardcodedParametersFilePath)
-        initialSolutionParameters = LibraryIO.readYAML(initialSolutionParametersFilePath)
-
-        solutionList = BenchmarkProblems.generateForkedSolutions (problemTypeObject, hardcodedParameters, [initialSolutionParameters])
-
-        assert len(solutionList) == 2
-
 def test_loadSolutions(caplog, useGlobalParameters):
     with useGlobalParameters():
         mylogger.debug("this is a test of debug log")

From cd669335038646ec37f8e1920d641e0b479729b5 Mon Sep 17 00:00:00 2001
From: Benjamin Ulmer <benjamin.ulmer@amd.com>
Date: Wed, 13 Oct 2021 17:13:13 -0600
Subject: [PATCH 11/12] Rebase on develop and update old test configs

---
 .../Tests/extended/stagger_u/big_skinny_A_NN.yaml    | 12 ------------
 .../Tests/extended/stagger_u/big_skinny_A_NT.yaml    | 12 ------------
 .../Tests/extended/stagger_u/big_skinny_A_TN.yaml    | 12 ------------
 .../Tests/extended/stagger_u/big_skinny_A_TT.yaml    | 12 ------------
 .../Tests/extended/stagger_u/big_skinny_B_NN.yaml    | 12 ------------
 .../Tests/extended/stagger_u/big_skinny_B_NT.yaml    | 12 ------------
 .../Tests/extended/stagger_u/big_skinny_B_TN.yaml    | 12 ------------
 .../Tests/extended/stagger_u/big_skinny_B_TT.yaml    | 12 ------------
 8 files changed, 96 deletions(-)

diff --git a/Tensile/Tests/extended/stagger_u/big_skinny_A_NN.yaml b/Tensile/Tests/extended/stagger_u/big_skinny_A_NN.yaml
index c1d14d029..3b875571f 100644
--- a/Tensile/Tests/extended/stagger_u/big_skinny_A_NN.yaml
+++ b/Tensile/Tests/extended/stagger_u/big_skinny_A_NN.yaml
@@ -28,16 +28,6 @@ BenchmarkProblems:
 
     - # BenchmarkProblemSizeGroup - Standard
       InitialSolutionParameters:
-        - AssertMinApproxSize: [0]
-        - AssertStrideBEqual: [ { 0: 1 } ]
-        - BufferLoad: [True] #
-        - GlobalReadVectorWidth: [1]
-        - UseSgprForGRO: [1]
-        - VectorAtomicWidth: [1]
-        - DirectToLds: True
-        - EnableMatrixInstruction: True
-        - VectorWidth: [1]
-        - LoopTail: True
       BenchmarkCommonParameters:
         - AssertMinApproxSize: [0]
         - AssertStrideBEqual: [ { 0: 1 } ]
@@ -58,8 +48,6 @@ BenchmarkProblems:
         - VectorWidth: [1]
       BenchmarkForkParameters:
       JoinParameters:
-        - MacroTile
-        - GlobalSplitU
       BenchmarkJoinParameters:
       BenchmarkFinalParameters:
         - ProblemSizes:
diff --git a/Tensile/Tests/extended/stagger_u/big_skinny_A_NT.yaml b/Tensile/Tests/extended/stagger_u/big_skinny_A_NT.yaml
index 2080ba845..3b21e9c2c 100644
--- a/Tensile/Tests/extended/stagger_u/big_skinny_A_NT.yaml
+++ b/Tensile/Tests/extended/stagger_u/big_skinny_A_NT.yaml
@@ -28,16 +28,6 @@ BenchmarkProblems:
 
     - # BenchmarkProblemSizeGroup - Standard
       InitialSolutionParameters:
-        - AssertMinApproxSize: [0]
-        - AssertStrideBEqual: [ { 0: 1 } ]
-        - BufferLoad: [True] #
-        - GlobalReadVectorWidth: [1]
-        - UseSgprForGRO: [1]
-        - VectorAtomicWidth: [1]
-        - DirectToLds: True
-        - EnableMatrixInstruction: True
-        - VectorWidth: [1]
-        - LoopTail: True
       BenchmarkCommonParameters:
         - AssertMinApproxSize: [0]
         - AssertStrideBEqual: [ { 0: 1 } ]
@@ -58,8 +48,6 @@ BenchmarkProblems:
         - VectorWidth: [1]
       BenchmarkForkParameters:
       JoinParameters:
-        - MacroTile
-        - GlobalSplitU
       BenchmarkJoinParameters:
       BenchmarkFinalParameters:
         - ProblemSizes:
diff --git a/Tensile/Tests/extended/stagger_u/big_skinny_A_TN.yaml b/Tensile/Tests/extended/stagger_u/big_skinny_A_TN.yaml
index 94db19378..adb5ed4c2 100644
--- a/Tensile/Tests/extended/stagger_u/big_skinny_A_TN.yaml
+++ b/Tensile/Tests/extended/stagger_u/big_skinny_A_TN.yaml
@@ -28,16 +28,6 @@ BenchmarkProblems:
 
     - # BenchmarkProblemSizeGroup - Standard
       InitialSolutionParameters:
-        - AssertMinApproxSize: [0]
-        - AssertStrideBEqual: [ { 0: 1 } ]
-        - BufferLoad: [True] #
-        - GlobalReadVectorWidth: [1]
-        - UseSgprForGRO: [1]
-        - VectorAtomicWidth: [1]
-        - DirectToLds: True
-        - EnableMatrixInstruction: True
-        - VectorWidth: [1]
-        - LoopTail: True
       BenchmarkCommonParameters:
         - AssertMinApproxSize: [0]
         - AssertStrideBEqual: [ { 0: 1 } ]
@@ -58,8 +48,6 @@ BenchmarkProblems:
         - VectorWidth: [1]
       BenchmarkForkParameters:
       JoinParameters:
-        - MacroTile
-        - GlobalSplitU
       BenchmarkJoinParameters:
       BenchmarkFinalParameters:
         - ProblemSizes:
diff --git a/Tensile/Tests/extended/stagger_u/big_skinny_A_TT.yaml b/Tensile/Tests/extended/stagger_u/big_skinny_A_TT.yaml
index 31d6c655a..9e8e74514 100644
--- a/Tensile/Tests/extended/stagger_u/big_skinny_A_TT.yaml
+++ b/Tensile/Tests/extended/stagger_u/big_skinny_A_TT.yaml
@@ -28,16 +28,6 @@ BenchmarkProblems:
 
     - # BenchmarkProblemSizeGroup - Standard
       InitialSolutionParameters:
-        - AssertMinApproxSize: [0]
-        - AssertStrideBEqual: [ { 0: 1 } ]
-        - BufferLoad: [True] #
-        - GlobalReadVectorWidth: [1]
-        - UseSgprForGRO: [1]
-        - VectorAtomicWidth: [1]
-        - DirectToLds: True
-        - EnableMatrixInstruction: True
-        - VectorWidth: [1]
-        - LoopTail: True
       BenchmarkCommonParameters:
         - AssertMinApproxSize: [0]
         - AssertStrideBEqual: [ { 0: 1 } ]
@@ -58,8 +48,6 @@ BenchmarkProblems:
         - VectorWidth: [1]
       BenchmarkForkParameters:
       JoinParameters:
-        - MacroTile
-        - GlobalSplitU
       BenchmarkJoinParameters:
       BenchmarkFinalParameters:
         - ProblemSizes:
diff --git a/Tensile/Tests/extended/stagger_u/big_skinny_B_NN.yaml b/Tensile/Tests/extended/stagger_u/big_skinny_B_NN.yaml
index 02b0c7f9e..74c071116 100644
--- a/Tensile/Tests/extended/stagger_u/big_skinny_B_NN.yaml
+++ b/Tensile/Tests/extended/stagger_u/big_skinny_B_NN.yaml
@@ -28,16 +28,6 @@ BenchmarkProblems:
 
     - # BenchmarkProblemSizeGroup - Standard
       InitialSolutionParameters:
-        - AssertMinApproxSize: [0]
-        - AssertStrideBEqual: [ { 0: 1 } ]
-        - BufferLoad: [True] #
-        - GlobalReadVectorWidth: [1]
-        - UseSgprForGRO: [1]
-        - VectorAtomicWidth: [1]
-        - DirectToLds: True
-        - EnableMatrixInstruction: True
-        - VectorWidth: [1]
-        - LoopTail: True
       BenchmarkCommonParameters:
         - AssertMinApproxSize: [0]
         - AssertStrideBEqual: [ { 0: 1 } ]
@@ -58,8 +48,6 @@ BenchmarkProblems:
         - VectorWidth: [1]
       BenchmarkForkParameters:
       JoinParameters:
-        - MacroTile
-        - GlobalSplitU
       BenchmarkJoinParameters:
       BenchmarkFinalParameters:
         - ProblemSizes:
diff --git a/Tensile/Tests/extended/stagger_u/big_skinny_B_NT.yaml b/Tensile/Tests/extended/stagger_u/big_skinny_B_NT.yaml
index a87206e53..055499eb9 100644
--- a/Tensile/Tests/extended/stagger_u/big_skinny_B_NT.yaml
+++ b/Tensile/Tests/extended/stagger_u/big_skinny_B_NT.yaml
@@ -28,16 +28,6 @@ BenchmarkProblems:
 
     - # BenchmarkProblemSizeGroup - Standard
       InitialSolutionParameters:
-        - AssertMinApproxSize: [0]
-        - AssertStrideBEqual: [ { 0: 1 } ]
-        - BufferLoad: [True] #
-        - GlobalReadVectorWidth: [1]
-        - UseSgprForGRO: [1]
-        - VectorAtomicWidth: [1]
-        - DirectToLds: True
-        - EnableMatrixInstruction: True
-        - VectorWidth: [1]
-        - LoopTail: True
       BenchmarkCommonParameters:
         - AssertMinApproxSize: [0]
         - AssertStrideBEqual: [ { 0: 1 } ]
@@ -58,8 +48,6 @@ BenchmarkProblems:
         - VectorWidth: [1]
       BenchmarkForkParameters:
       JoinParameters:
-        - MacroTile
-        - GlobalSplitU
       BenchmarkJoinParameters:
       BenchmarkFinalParameters:
         - ProblemSizes:
diff --git a/Tensile/Tests/extended/stagger_u/big_skinny_B_TN.yaml b/Tensile/Tests/extended/stagger_u/big_skinny_B_TN.yaml
index aa96d2bfe..05f51c3bc 100644
--- a/Tensile/Tests/extended/stagger_u/big_skinny_B_TN.yaml
+++ b/Tensile/Tests/extended/stagger_u/big_skinny_B_TN.yaml
@@ -28,16 +28,6 @@ BenchmarkProblems:
 
     - # BenchmarkProblemSizeGroup - Standard
       InitialSolutionParameters:
-        - AssertMinApproxSize: [0]
-        - AssertStrideBEqual: [ { 0: 1 } ]
-        - BufferLoad: [True] #
-        - GlobalReadVectorWidth: [1]
-        - UseSgprForGRO: [1]
-        - VectorAtomicWidth: [1]
-        - DirectToLds: True
-        - EnableMatrixInstruction: True
-        - VectorWidth: [1]
-        - LoopTail: True
       BenchmarkCommonParameters:
         - AssertMinApproxSize: [0]
         - AssertStrideBEqual: [ { 0: 1 } ]
@@ -58,8 +48,6 @@ BenchmarkProblems:
         - VectorWidth: [1]
       BenchmarkForkParameters:
       JoinParameters:
-        - MacroTile
-        - GlobalSplitU
       BenchmarkJoinParameters:
       BenchmarkFinalParameters:
         - ProblemSizes:
diff --git a/Tensile/Tests/extended/stagger_u/big_skinny_B_TT.yaml b/Tensile/Tests/extended/stagger_u/big_skinny_B_TT.yaml
index 135006ce1..4ac4f8384 100644
--- a/Tensile/Tests/extended/stagger_u/big_skinny_B_TT.yaml
+++ b/Tensile/Tests/extended/stagger_u/big_skinny_B_TT.yaml
@@ -28,16 +28,6 @@ BenchmarkProblems:
 
     - # BenchmarkProblemSizeGroup - Standard
       InitialSolutionParameters:
-        - AssertMinApproxSize: [0]
-        - AssertStrideBEqual: [ { 0: 1 } ]
-        - BufferLoad: [True] #
-        - GlobalReadVectorWidth: [1]
-        - UseSgprForGRO: [1]
-        - VectorAtomicWidth: [1]
-        - DirectToLds: True
-        - EnableMatrixInstruction: True
-        - VectorWidth: [1]
-        - LoopTail: True
       BenchmarkCommonParameters:
         - AssertMinApproxSize: [0]
         - AssertStrideBEqual: [ { 0: 1 } ]
@@ -58,8 +48,6 @@ BenchmarkProblems:
         - VectorWidth: [1]
       BenchmarkForkParameters:
       JoinParameters:
-        - MacroTile
-        - GlobalSplitU
       BenchmarkJoinParameters:
       BenchmarkFinalParameters:
         - ProblemSizes:

From 375dfb39b52d1333471093e9335d81e3c69f36b3 Mon Sep 17 00:00:00 2001
From: Benjamin Ulmer <benjamin.ulmer@amd.com>
Date: Tue, 19 Oct 2021 14:51:41 -0600
Subject: [PATCH 12/12] Fixed duplicate solutions being forked

---
 Tensile/BenchmarkProblems.py | 5 ++++-
 1 file changed, 4 insertions(+), 1 deletion(-)

diff --git a/Tensile/BenchmarkProblems.py b/Tensile/BenchmarkProblems.py
index d7f2a43ae..03d3fb963 100644
--- a/Tensile/BenchmarkProblems.py
+++ b/Tensile/BenchmarkProblems.py
@@ -47,6 +47,7 @@ def generateForkedSolutions(problemType, constantParams, forkPermutations):
     print1("# Enumerating Solutions")
 
     solutions = []
+    solutionSet = set()
     for perm in forkPermutations:
         solution = {"ProblemType": deepcopy(problemType.state)}
         solution.update(constantParams)
@@ -55,7 +56,9 @@ def generateForkedSolutions(problemType, constantParams, forkPermutations):
         # TODO check if solution matches problem size for exact tile kernels
         solutionObject = Solution(solution)
         if solutionObject["Valid"]:
-            solutions.append(solutionObject)
+            if solutionObject not in solutionSet:
+                solutionSet.add(solutionObject)
+                solutions.append(solutionObject)
         elif globalParameters["PrintSolutionRejectionReason"]:
             print1("rejecting solution " + str(solutionObject))
 
